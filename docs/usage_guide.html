

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>iPython Notebooks &mdash; Model Tuner 0.0.21a0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=f712845e" />

  
    <link rel="canonical" href="https://uclamii.github.io/model_tuner/usage_guide.html" />
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=c82a19bc"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=f281be69"></script>
      <script src="_static/custom.js?v=59429b38"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Zero Variance Columns" href="caveats.html" />
    <link rel="prev" title="Welcome to Model Tuner’s Documentation!" href="getting_started.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Model Tuner
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Welcome to Model Tuner’s Documentation!</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Usage Guide</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">iPython Notebooks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#binary-classification-examples">Binary Classification Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="#regression-example">Regression Example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#key-methods-and-functionalities">Key Methods and Functionalities</a></li>
<li class="toctree-l1"><a class="reference internal" href="#helper-functions">Helper Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="#input-parameters">Input Parameters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Model"><code class="docutils literal notranslate"><span class="pre">Model</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#pipeline-management">Pipeline Management</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#specifying-pipeline-steps">Specifying Pipeline Steps</a></li>
<li class="toctree-l2"><a class="reference internal" href="#helper-methods-for-pipeline-extraction">Helper Methods for Pipeline Extraction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#get_preprocessing_and_feature_selection_pipeline"><code class="docutils literal notranslate"><span class="pre">get_preprocessing_and_feature_selection_pipeline()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#get_feature_selection_pipeline"><code class="docutils literal notranslate"><span class="pre">get_feature_selection_pipeline()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#get_preprocessing_pipeline"><code class="docutils literal notranslate"><span class="pre">get_preprocessing_pipeline()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#binary-classification">Binary Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#aids-clinical-trials-group-study">AIDS Clinical Trials Group Study</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-import-necessary-libraries">Step 1: Import Necessary Libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-load-the-dataset-define-x-y">Step 2: Load the dataset, define X, y</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-check-for-zero-variance-columns-and-drop-accordingly">Step 3: Check for zero-variance columns and drop accordingly</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-4-create-an-instance-of-the-xgbclassifier">Step 4: Create an Instance of the XGBClassifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-5-define-hyperparameters-for-xgboost">Step 5: Define Hyperparameters for XGBoost</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-tuning-hyperparameters-for-catboost">Example: Tuning Hyperparameters for CatBoost</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-6-initialize-and-configure-the-model">Step 6: Initialize and Configure the <code class="docutils literal notranslate"><span class="pre">Model</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-7-perform-grid-search-parameter-tuning">Step 7: Perform Grid Search Parameter Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-8-fit-the-model">Step 8: Fit the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-9-return-metrics-optional">Step 9: Return Metrics (Optional)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-10-calibrate-the-model-if-needed">Step 10: Calibrate the Model (if needed)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#classification-report-optional">Classification Report (Optional)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#recursive-feature-elimination-rfe">Recursive Feature Elimination (RFE)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#elastic-net-for-feature-selection-with-rfe">Elastic Net for Feature Selection with RFE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#imbalanced-learning">Imbalanced Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#generating-an-imbalanced-dataset">Generating an Imbalanced Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#define-hyperparameters-for-xgboost">Define Hyperparameters for XGBoost</a></li>
<li class="toctree-l3"><a class="reference internal" href="#define-the-model-object">Define The Model object</a></li>
<li class="toctree-l3"><a class="reference internal" href="#addressing-class-imbalance-in-machine-learning">Addressing Class Imbalance in Machine Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#techniques-to-address-class-imbalance">Techniques to Address Class Imbalance</a></li>
<li class="toctree-l4"><a class="reference internal" href="#purpose-of-using-these-techniques">Purpose of Using These Techniques</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#synthetic-minority-oversampling-technique-smote">Synthetic Minority Oversampling Technique (SMOTE)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#initalize-and-configure-the-model">Initalize and Configure The Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#perform-grid-search-parameter-tuning-and-retrieve-split-data">Perform Grid Search Parameter Tuning and Retrieve Split Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#smote-distribution-of-y-values-after-resampling">SMOTE: Distribution of y values after resampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="#fit-the-model">Fit The Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#return-metrics-optional">Return Metrics (Optional)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#shap-shapley-additive-explanations">SHAP (SHapley Additive exPlanations)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-transform-the-test-data-using-the-feature-selection-pipeline">Step 1: Transform the test data using the feature selection pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-retrieve-the-trained-xgboost-classifier-from-the-pipeline">Step 2: Retrieve the trained XGBoost classifier from the pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-extract-feature-names-from-the-training-data-and-initialize-the-shap-explainer-for-the-xgboost-classifier">Step 3: Extract feature names from the training data, and initialize the SHAP explainer for the XGBoost classifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-4-compute-shap-values-for-the-transformed-test-dataset-and-generate-a-summary-plot-of-shap-values">Step 4: Compute SHAP values for the transformed test dataset and generate a summary plot of SHAP values</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-5-generate-a-summary-plot-of-shap-values">Step 5: Generate a summary plot of SHAP values</a></li>
<li class="toctree-l3"><a class="reference internal" href="#feature-importance-and-impact">Feature Importance and Impact</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#regression">Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#california-housing-with-xgboost">California Housing with XGBoost</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">Step 1: Import Necessary Libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-load-the-dataset">Step 2: Load the Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-create-an-instance-of-the-xgbregressor">Step 3: Create an Instance of the XGBRegressor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-4-define-hyperparameters-for-xgboost">Step 4: Define Hyperparameters for XGBoost</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-5-initialize-and-configure-the-model">Step 5: Initialize and Configure the <code class="docutils literal notranslate"><span class="pre">Model</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-6-perform-grid-search-parameter-tuning-and-retrieve-split-data">Step 6: Perform Grid Search Parameter Tuning and Retrieve Split Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-7-fit-the-model">Step 7: Fit the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-8-return-metrics-optional">Step 8: Return Metrics (Optional)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#bootstrap-metrics">Bootstrap Metrics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#check_input_type"><code class="docutils literal notranslate"><span class="pre">check_input_type()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#sampling_method"><code class="docutils literal notranslate"><span class="pre">sampling_method()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#evaluate_bootstrap_metrics"><code class="docutils literal notranslate"><span class="pre">evaluate_bootstrap_metrics()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#return_bootstrap_metrics"><code class="docutils literal notranslate"><span class="pre">return_bootstrap_metrics()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#bootstrap-metrics-example">Bootstrap Metrics Example</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Caveats</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="caveats.html">Zero Variance Columns</a></li>
<li class="toctree-l1"><a class="reference internal" href="caveats.html#dependent-variable">Dependent Variable</a></li>
<li class="toctree-l1"><a class="reference internal" href="caveats.html#imputation-before-scaling">Imputation Before Scaling</a></li>
<li class="toctree-l1"><a class="reference internal" href="caveats.html#column-stratification-with-cross-validation">Column Stratification with Cross-Validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="caveats.html#model-calibration">Model Calibration</a></li>
<li class="toctree-l1"><a class="reference internal" href="caveats.html#using-imputation-and-scaling-in-pipeline-steps-for-model-preprocessing">Using Imputation and Scaling in Pipeline Steps for Model Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="caveats.html#caveats-in-imbalanced-learning">Caveats in Imbalanced Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="caveats.html#elasticnet-regularization">ElasticNet Regularization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About Model Tuner</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="about.html">GitHub Repository</a></li>
<li class="toctree-l1"><a class="reference internal" href="about.html#acknowledgements">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="about.html#citing-model-tuner">Citing Model Tuner</a></li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Model Tuner</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">iPython Notebooks</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="no-click"><a class="reference internal image-reference" href="_images/ModelTunerTarget.png"><img alt="Model Tuner Logo" class="align-left" src="_images/ModelTunerTarget.png" style="width: 250px;" />
</a>
</div><div style="height: 150px;"></div><p></p>
<section id="ipython-notebooks">
<h1>iPython Notebooks<a class="headerlink" href="#ipython-notebooks" title="Link to this heading"></a></h1>
<section id="binary-classification-examples">
<h2>Binary Classification Examples<a class="headerlink" href="#binary-classification-examples" title="Link to this heading"></a></h2>
<blockquote>
<div><p><strong>Google Colab Notebooks</strong></p>
<ul>
<li><a href="https://colab.research.google.com/drive/1bP0DzSYgV0ncHlkJq9uV3pUXn8PaR31z#scrollTo=OTWiK2ZwdeMK" target="_blank">Binary Classification + KFold Example: Titanic Dataset - Categorical Data</a></li>
<li><a href="https://colab.research.google.com/drive/12XywbGBiwlZIbi0C3JKu9NOQPPRgVwcp?usp=sharing#scrollTo=rm5TA__pC3M-" target="_blank">Binary Classification: AIDS Clinical Trials - Numerical Data</a></li>
<li><a href="https://colab.research.google.com/drive/16gWnRAJvpUjTIes5y1gFRdX1soASdV6m#scrollTo=3NYa_tQWy6HR" target="_blank">Binary Classification: Imbalanced Learning</a></li>
</ul>
<p><strong>HTML Files</strong></p>
<ul>
<li><a href="./example_htmls/Model_Tuner_Column_Transformer.html" target="_blank">Binary Classification + KFold Example: Titanic Dataset - Categorical Data</a></li>
<li><a href="./example_htmls/Model_Tuner_Binary_Classification_AIDS_Clinical_Trials.html" target="_blank">Binary Classification: AIDS Clinical Trials HTML File</a></li>
<li><a href="./example_htmls/Model_Tuner_Binary_Classification_Imbalanced_Learning.html" target="_blank">Binary Classification: Imbalanced Learning</a></li>
</ul>
</div></blockquote>
</section>
<section id="regression-example">
<h2>Regression Example<a class="headerlink" href="#regression-example" title="Link to this heading"></a></h2>
<blockquote>
<div><p><strong>Google Colab Notebook</strong></p>
<ul>
<li><a href="https://colab.research.google.com/drive/151kdlsW-WyJ0pwwt_iWpjXDuqj1Ktam_?authuser=1#scrollTo=UhfZKVoq3sAN" target="_blank">Redfin Real Estate - Los Angeles Data Colab Notebook</a></li>
</ul>
<p><strong>HTML File</strong></p>
<ul>
<li><a href="./example_htmls/Model_Tuner_Regression_Redfin_Real_Estate.html" target="_blank">Redfin Real Estate - Los Angeles Data HTML File</a></li>
</ul>
</div></blockquote>
</section>
</section>
<section id="key-methods-and-functionalities">
<h1>Key Methods and Functionalities<a class="headerlink" href="#key-methods-and-functionalities" title="Link to this heading"></a></h1>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">__init__(...)</span></code></dt><dd><p>Initializes the model tuner with configurations, including estimator, cross-validation settings, scoring metrics, pipeline steps, feature selection, imbalance sampler, Bayesian search, and model calibration options.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">reset_estimator()</span></code></dt><dd><p>Resets the estimator and pipeline configuration.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">process_imbalance_sampler(X_train,</span> <span class="pre">y_train)</span></code></dt><dd><p>Processes the imbalance sampler, applying it to resample the training data.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">calibrateModel(X,</span> <span class="pre">y,</span> <span class="pre">score=None)</span></code></dt><dd><p>Calibrates the model with cross-validation support and configurable calibration methods, improving probability estimates.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">get_train_data(X,</span> <span class="pre">y),</span> <span class="pre">get_valid_data(X,</span> <span class="pre">y),</span> <span class="pre">get_test_data(X,</span> <span class="pre">y)</span></code></dt><dd><p>Retrieves train, validation, and test data based on specified indices.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">calibrate_report(X,</span> <span class="pre">y,</span> <span class="pre">score=None)</span></code></dt><dd><p>Generates a calibration report, including a confusion matrix and classification report.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">fit(X,</span> <span class="pre">y,</span> <span class="pre">validation_data=None,</span> <span class="pre">score=None)</span></code></dt><dd><p>Fits the model to training data and, if applicable, tunes threshold and performs early stopping. Allows feature selection and processing steps as part of the pipeline.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">return_metrics(X_test,</span> <span class="pre">y_test,</span> <span class="pre">optimal_threshold=False)</span></code></dt><dd><p>Returns evaluation metrics with confusion matrix and classification report, optionally using optimized classification thresholds.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">predict(X,</span> <span class="pre">y=None,</span> <span class="pre">optimal_threshold=False),</span> <span class="pre">predict_proba(X,</span> <span class="pre">y=None)</span></code></dt><dd><p>Makes predictions and predicts probabilities, allowing threshold tuning.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">grid_search_param_tuning(X,</span> <span class="pre">y,</span> <span class="pre">f1_beta_tune=False,</span> <span class="pre">betas=[1,</span> <span class="pre">2])</span></code></dt><dd><p>Performs grid or Bayesian search parameter tuning, optionally tuning F-beta score thresholds for classification.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">print_selected_best_features(X)</span></code></dt><dd><p>Prints and returns the selected top K best features based on the feature selection step.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">tune_threshold_Fbeta(score,</span> <span class="pre">y_valid,</span> <span class="pre">betas,</span> <span class="pre">y_valid_proba,</span> <span class="pre">kfold=False)</span></code></dt><dd><p>Tunes classification threshold for optimal F-beta score, balancing precision and recall across various thresholds.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">train_val_test_split(X,</span> <span class="pre">y,</span> <span class="pre">stratify_y,</span> <span class="pre">train_size,</span> <span class="pre">validation_size,</span> <span class="pre">test_size,</span> <span class="pre">random_state,</span> <span class="pre">stratify_cols)</span></code></dt><dd><p>Splits data into train, validation, and test sets, supporting stratification by specific columns or the target variable.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">get_best_score_params(X,</span> <span class="pre">y)</span></code></dt><dd><p>Retrieves the best hyperparameters for the model based on cross-validation scores for specified metrics.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">conf_mat_class_kfold(X,</span> <span class="pre">y,</span> <span class="pre">test_model,</span> <span class="pre">score=None)</span></code></dt><dd><p>Generates and averages confusion matrices across k-folds, producing a combined classification report.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">regression_report_kfold(X,</span> <span class="pre">y,</span> <span class="pre">test_model,</span> <span class="pre">score=None)</span></code></dt><dd><p>Generates averaged regression metrics across k-folds.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">regression_report(y_true,</span> <span class="pre">y_pred,</span> <span class="pre">print_results=True)</span></code></dt><dd><p>Generates a regression report with metrics like Mean Absolute Error, R-squared, and Root Mean Squared Error.</p>
</dd>
</dl>
</section>
<section id="helper-functions">
<h1>Helper Functions<a class="headerlink" href="#helper-functions" title="Link to this heading"></a></h1>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">kfold_split(classifier,</span> <span class="pre">X,</span> <span class="pre">y,</span> <span class="pre">stratify=False,</span> <span class="pre">scoring=[&quot;roc_auc&quot;],</span> <span class="pre">n_splits=10,</span> <span class="pre">random_state=3)</span></code></dt><dd><p>Splits data using k-fold or stratified k-fold cross-validation.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">get_cross_validate(classifier,</span> <span class="pre">X,</span> <span class="pre">y,</span> <span class="pre">kf,</span> <span class="pre">scoring=[&quot;roc_auc&quot;])</span></code></dt><dd><p>Performs cross-validation and returns training scores and estimator instances.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">_confusion_matrix_print(conf_matrix,</span> <span class="pre">labels)</span></code></dt><dd><p>Prints the formatted confusion matrix for binary classification.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">print_pipeline(pipeline)</span></code></dt><dd><p>Displays an ASCII representation of the pipeline steps for visual clarity.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">report_model_metrics(model,</span> <span class="pre">X_valid=None,</span> <span class="pre">y_valid=None,</span> <span class="pre">threshold=0.5)</span></code></dt><dd><p>Generates a DataFrame of key model performance metrics, including Precision, Sensitivity, Specificity, and AUC-ROC.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>This class is designed to be flexible and can be extended to include additional functionalities or custom metrics.</p></li>
<li><p>It is essential to properly configure the parameters during initialization to suit the specific requirements of your machine learning task.</p></li>
<li><p>Ensure that all dependencies are installed and properly imported before using the <code class="docutils literal notranslate"><span class="pre">Model</span></code> class from the <code class="docutils literal notranslate"><span class="pre">model_tuner</span></code> library.</p></li>
</ul>
</div>
</section>
<section id="input-parameters">
<h1>Input Parameters<a class="headerlink" href="#input-parameters" title="Link to this heading"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="Model">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">calibrate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kfold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">imbalance_sampler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stratify_y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stratify_cols</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['roc_auc']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_splits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">display</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">randomized_grid</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pipeline_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boost_early</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_selection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">calibration_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sigmoid'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_scorer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bayesian</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Model" title="Link to this definition"></a></dt>
<dd><p>A class for building, tuning, and evaluating machine learning models, supporting both classification and regression tasks, as well as multi-label classification.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – A unique name for the model, helpful for tracking outputs and logs.</p></li>
<li><p><strong>estimator_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – Prefix for the estimator in the pipeline, used for setting parameters in tuning (e.g., estimator_name + <code class="docutils literal notranslate"><span class="pre">__param_name</span></code>).</p></li>
<li><p><strong>estimator</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><em>object</em></a>) – The machine learning model to be trained and tuned.</p></li>
<li><p><strong>model_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – Specifies the type of model, must be either <code class="docutils literal notranslate"><span class="pre">classification</span></code> or <code class="docutils literal notranslate"><span class="pre">regression</span></code>.</p></li>
<li><p><strong>calibrate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to calibrate the model’s probability estimates. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>kfold</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to perform k-fold cross-validation. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>imbalance_sampler</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><em>object</em></a><em>, </em><em>optional</em>) – An imbalanced data sampler from the imblearn library, e.g., <code class="docutils literal notranslate"><span class="pre">RandomUnderSampler</span></code> or <code class="docutils literal notranslate"><span class="pre">RandomOverSampler</span></code>.</p></li>
<li><p><strong>train_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Proportion of the data to be used for training. Default is <code class="docutils literal notranslate"><span class="pre">0.6</span></code>.</p></li>
<li><p><strong>validation_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Proportion of the data to be used for validation. Default is <code class="docutils literal notranslate"><span class="pre">0.2</span></code>.</p></li>
<li><p><strong>test_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Proportion of the data to be used for testing. Default is <code class="docutils literal notranslate"><span class="pre">0.2</span></code>.</p></li>
<li><p><strong>stratify_y</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to stratify by the target variable during data splitting. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>stratify_cols</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em>, or </em><em>pandas.DataFrame</em><em>, </em><em>optional</em>) – Columns to use for stratification during data splitting.
Can be a single column name (as a string), a list of column names (as strings),
or a DataFrame containing the columns for stratification. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>grid</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Hyperparameter grid for model tuning, supporting both regular and Bayesian search.</p></li>
<li><p><strong>scoring</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – List of scoring metrics for evaluation, e.g., <code class="docutils literal notranslate"><span class="pre">[&quot;roc_auc&quot;,</span> <span class="pre">&quot;accuracy&quot;]</span></code>.</p></li>
<li><p><strong>n_splits</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of splits for k-fold cross-validation. Default is <code class="docutils literal notranslate"><span class="pre">10</span></code>.</p></li>
<li><p><strong>random_state</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Seed for random number generation to ensure reproducibility. Default is <code class="docutils literal notranslate"><span class="pre">3</span></code>.</p></li>
<li><p><strong>n_jobs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of parallel jobs to run for model fitting. Default is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><strong>display</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to print messages during the tuning and training process. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>randomized_grid</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to use randomized grid search. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>n_iter</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of iterations for randomized grid search. Default is <code class="docutils literal notranslate"><span class="pre">100</span></code>.</p></li>
<li><p><strong>pipeline_steps</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em>, </em><em>optional</em>) – List of steps for the pipeline, e.g., preprocessing and feature selection steps. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
<li><p><strong>boost_early</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to enable early stopping for boosting algorithms like XGBoost. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>feature_selection</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to enable feature selection. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>class_labels</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em>, </em><em>optional</em>) – List of labels for multi-class classification. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>multi_label</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether the task is a multi-label classification problem. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>calibration_method</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>optional</em>) – Method for calibration; options include <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> and <code class="docutils literal notranslate"><span class="pre">isotonic</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code>.</p></li>
<li><p><strong>custom_scorer</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a><em>, </em><em>optional</em>) – Dictionary of custom scoring functions, allowing additional metrics to be evaluated. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
<li><p><strong>bayesian</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to perform Bayesian hyperparameter tuning using <code class="docutils literal notranslate"><span class="pre">BayesSearchCV</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ImportError" title="(in Python v3.13)"><strong>ImportError</strong></a> – If the <code class="docutils literal notranslate"><span class="pre">bootstrapper</span></code> module is not found or not installed.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – Raised for various issues, such as:
- Invalid <code class="docutils literal notranslate"><span class="pre">model_type</span></code> value. The <code class="docutils literal notranslate"><span class="pre">model_type</span></code> must be explicitly specified as either <code class="docutils literal notranslate"><span class="pre">classification</span></code> or <code class="docutils literal notranslate"><span class="pre">regression</span></code>.
- Invalid hyperparameter configurations or mismatched <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> shapes.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.13)"><strong>AttributeError</strong></a> – Raised if an expected pipeline step is missing, or if <code class="docutils literal notranslate"><span class="pre">self.estimator</span></code> is improperly initialized.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><strong>TypeError</strong></a> – Raised when an incorrect parameter type is provided, such as passing <code class="docutils literal notranslate"><span class="pre">None</span></code> instead of a valid object.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#IndexError" title="(in Python v3.13)"><strong>IndexError</strong></a> – Raised for indexing issues, particularly in confusion matrix formatting functions.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#KeyError" title="(in Python v3.13)"><strong>KeyError</strong></a> – Raised when accessing dictionary keys that are not available, such as missing scores in <code class="docutils literal notranslate"><span class="pre">self.best_params_per_score</span></code>.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – Raised for unexpected issues during model fitting or transformations that do not fit into the other exception categories.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="pipeline-management">
<h1>Pipeline Management<a class="headerlink" href="#pipeline-management" title="Link to this heading"></a></h1>
<p>The pipeline in the model tuner class is designed to automatically organize steps into three categories: <strong>preprocessing</strong>, <strong>feature selection</strong>, and <strong>imbalanced sampling</strong>. The steps are ordered in the following sequence:</p>
<ol class="arabic simple">
<li><p><strong>Preprocessing</strong>:</p>
<ul class="simple">
<li><p>Imputation</p></li>
<li><p>Scaling</p></li>
<li><p>Other preprocessing steps</p></li>
</ul>
</li>
<li><p><strong>Imbalanced Sampling</strong></p></li>
<li><p><strong>Feature Selection</strong></p></li>
<li><p><strong>Classifier</strong></p></li>
</ol>
<p>The <code class="docutils literal notranslate"><span class="pre">pipeline_assembly</span></code> method automatically sorts the steps into this order.</p>
<section id="specifying-pipeline-steps">
<h2>Specifying Pipeline Steps<a class="headerlink" href="#specifying-pipeline-steps" title="Link to this heading"></a></h2>
<p>Pipeline steps can be specified in multiple ways. For example, if naming a pipeline step then specify like so:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pipeline_steps</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;imputer&#39;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">()]</span>
</pre></div>
</div>
<p>Naming each step is optional and the steps can also be specified like so:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pipeline_steps</span> <span class="o">=</span> <span class="p">[</span><span class="n">SimpleImputer</span><span class="p">(),</span> <span class="n">StandardScalar</span><span class="p">(),</span> <span class="n">rfe</span><span class="p">()]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>If no name is assigned, the step will be renamed automatically to follow the convention <code class="docutils literal notranslate"><span class="pre">step_0</span></code>, <code class="docutils literal notranslate"><span class="pre">step_1</span></code>, etc.</p></li>
<li><p>Column transformers can also be included in the pipeline and are automatically categorized under the <strong>preprocessing</strong> section.</p></li>
</ul>
</section>
<section id="helper-methods-for-pipeline-extraction">
<h2>Helper Methods for Pipeline Extraction<a class="headerlink" href="#helper-methods-for-pipeline-extraction" title="Link to this heading"></a></h2>
<p>To support advanced use cases, the model tuner provides helper methods to extract parts of the pipeline for later use. For example, when generating SHAP plots, users might only need the preprocessing section of the pipeline.</p>
<p>Here are some of the available methods:</p>
<dl class="py function">
<dt class="sig sig-object py" id="get_preprocessing_and_feature_selection_pipeline">
<span class="sig-name descname"><span class="pre">get_preprocessing_and_feature_selection_pipeline</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#get_preprocessing_and_feature_selection_pipeline" title="Link to this definition"></a></dt>
<dd><p>Extracts both the preprocessing and feature selection parts of the pipeline.</p>
<p><strong>Example</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_preprocessing_and_feature_selection_pipeline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">transformer</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">transformer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">steps</span>
        <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;preprocess_&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;feature_selection_&quot;</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">PipelineClass</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="get_feature_selection_pipeline">
<span class="sig-name descname"><span class="pre">get_feature_selection_pipeline</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#get_feature_selection_pipeline" title="Link to this definition"></a></dt>
<dd><p>Extracts only the feature selection part of the pipeline.</p>
<p><strong>Example</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_feature_selection_pipeline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">transformer</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">transformer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">steps</span>
        <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;feature_selection_&quot;</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">PipelineClass</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="get_preprocessing_pipeline">
<span class="sig-name descname"><span class="pre">get_preprocessing_pipeline</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#get_preprocessing_pipeline" title="Link to this definition"></a></dt>
<dd><p>Extracts only the preprocessing part of the pipeline.</p>
<p><strong>Example</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_preprocessing_pipeline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">preprocessing_steps</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">transformer</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">transformer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">steps</span>
        <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;preprocess_&quot;</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">PipelineClass</span><span class="p">(</span><span class="n">preprocessing_steps</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading"></a></h2>
<p>By organizing pipeline steps automatically and providing helper methods for extraction, the model tuner class offers flexibility and ease of use for building and managing complex pipelines. Users can focus on specifying the steps, and the tuner handles naming, sorting, and category assignments seamlessly.</p>
</section>
</section>
<section id="binary-classification">
<h1>Binary Classification<a class="headerlink" href="#binary-classification" title="Link to this heading"></a></h1>
<p>Binary classification is a type of supervised learning where a model is trained
to distinguish between two distinct classes or categories. In essence, the model
learns to classify input data into one of two possible outcomes, typically
labeled as <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>, or negative and positive. This is commonly used in
scenarios such as spam detection, disease diagnosis, or fraud detection.</p>
<p>In our library, binary classification is handled seamlessly through the <code class="docutils literal notranslate"><span class="pre">Model</span></code>
class. Users can specify a binary classifier as the estimator, and the library
takes care of essential tasks like data preprocessing, model calibration, and
cross-validation. The library also provides robust support for evaluating the
model’s performance using a variety of metrics, such as <a class="reference internal" href="caveats.html#limitations-of-accuracy"><span class="std std-ref">accuracy, precision,
recall, and ROC-AUC</span></a>, ensuring that the model’s ability to distinguish between the
two classes is thoroughly assessed. Additionally, the library supports advanced
techniques like imbalanced data handling and model calibration to fine-tune
decision thresholds, making it easier to deploy effective binary classifiers in
real-world applications.</p>
<section id="aids-clinical-trials-group-study">
<h2>AIDS Clinical Trials Group Study<a class="headerlink" href="#aids-clinical-trials-group-study" title="Link to this heading"></a></h2>
<p>The UCI Machine Learning Repository is a well-known resource for accessing a wide
range of datasets used for machine learning research and practice. One such dataset
is the AIDS Clinical Trials Group Study dataset, which can be used to build and
evaluate predictive models.</p>
<p>You can easily fetch this dataset using the ucimlrepo package. If you haven’t
installed it yet, you can do so by running the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>ucimlrepo
</pre></div>
</div>
<p>Once installed, you can quickly load the AIDS Clinical Trials Group Study dataset
with a simple command:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ucimlrepo</span> <span class="kn">import</span> <span class="n">fetch_ucirepo</span>
</pre></div>
</div>
<section id="step-1-import-necessary-libraries">
<h3>Step 1: Import Necessary Libraries<a class="headerlink" href="#step-1-import-necessary-libraries" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<span class="kn">from</span> <span class="nn">model_tuner</span> <span class="kn">import</span> <span class="n">Model</span>
</pre></div>
</div>
</section>
<section id="step-2-load-the-dataset-define-x-y">
<h3>Step 2: Load the dataset, define X, y<a class="headerlink" href="#step-2-load-the-dataset-define-x-y" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># fetch dataset</span>
<span class="n">aids_clinical_trials_group_study_175</span> <span class="o">=</span> <span class="n">fetch_ucirepo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">890</span><span class="p">)</span>

<span class="c1"># data (as pandas dataframes)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">aids_clinical_trials_group_study_175</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">features</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">aids_clinical_trials_group_study_175</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">targets</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="c1"># convert a DataFrame to Series when single column</span>
</pre></div>
</div>
</section>
<section id="step-3-check-for-zero-variance-columns-and-drop-accordingly">
<h3>Step 3: Check for zero-variance columns and drop accordingly<a class="headerlink" href="#step-3-check-for-zero-variance-columns-and-drop-accordingly" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check for zero-variance columns and drop them</span>
<span class="n">zero_variance_columns</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">var</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">zero_variance_columns</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
   <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">zero_variance_columns</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-4-create-an-instance-of-the-xgbclassifier">
<h3>Step 4: Create an Instance of the XGBClassifier<a class="headerlink" href="#step-4-create-an-instance-of-the-xgbclassifier" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating an instance of the XGBClassifier</span>
<span class="n">xgb_name</span> <span class="o">=</span> <span class="s2">&quot;xgb&quot;</span>
<span class="n">xgb</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span>
   <span class="n">objective</span><span class="o">=</span><span class="s2">&quot;binary:logistic&quot;</span><span class="p">,</span>
   <span class="n">random_state</span><span class="o">=</span><span class="mi">222</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-5-define-hyperparameters-for-xgboost">
<span id="xgb-hyperparams"></span><h3>Step 5: Define Hyperparameters for XGBoost<a class="headerlink" href="#step-5-define-hyperparameters-for-xgboost" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xgbearly</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">tuned_parameters_xgb</span> <span class="o">=</span> <span class="p">{</span>
   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__max_depth&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">500</span><span class="p">],</span>
   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__learning_rate&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-4</span><span class="p">],</span>
   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__n_estimators&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1000</span><span class="p">],</span>
   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__early_stopping_rounds&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">],</span>
   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__verbose&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__eval_metric&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;logloss&quot;</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">xgb_definition</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s2">&quot;clc&quot;</span><span class="p">:</span> <span class="n">xgb</span><span class="p">,</span>
   <span class="s2">&quot;estimator_name&quot;</span><span class="p">:</span> <span class="n">xgb_name</span><span class="p">,</span>
   <span class="s2">&quot;tuned_parameters&quot;</span><span class="p">:</span> <span class="n">tuned_parameters_xgb</span><span class="p">,</span>
   <span class="s2">&quot;randomized_grid&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
   <span class="s2">&quot;n_iter&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
   <span class="s2">&quot;early&quot;</span><span class="p">:</span> <span class="n">xgbearly</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">verbose</span></code> parameter in XGBoost allows you to control the level of output during training:</p>
<ul class="simple">
<li><p>Set to <code class="docutils literal notranslate"><span class="pre">0</span></code> or <code class="docutils literal notranslate"><span class="pre">False</span></code>: Suppresses all training output (silent mode).</p></li>
<li><p>Set to <code class="docutils literal notranslate"><span class="pre">1</span></code> or <code class="docutils literal notranslate"><span class="pre">True</span></code>: Displays progress and evaluation metrics during training.</p></li>
</ul>
<p>This can be particularly useful for monitoring model performance when early stopping is enabled.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>When defining hyperparameters for boosting algorithms, frameworks like
XGBoost allow straightforward configuration, such as specifying <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>
for the number of boosting rounds. However, CatBoost introduces potential
pitfalls when defining this parameter.</p>
<p>According to the <a class="reference external" href="https://catboost.ai/docs/en/references/training-parameters/">CatBoost documentation</a>:</p>
<blockquote>
<div><p>“For the Python package several parameters have aliases. For example, the –iterations parameter has the following synonyms: num_boost_round, n_estimators, num_trees. Simultaneous usage of different names of one parameter raises an error.”</p>
</div></blockquote>
<p>To avoid this issue in CatBoost, ensure you define only one of these parameters (e.g., <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>) and avoid including others such as <code class="docutils literal notranslate"><span class="pre">iterations</span></code> or <code class="docutils literal notranslate"><span class="pre">num_boost_round</span></code>.</p>
</div>
</section>
<section id="example-tuning-hyperparameters-for-catboost">
<h3>Example: Tuning Hyperparameters for CatBoost<a class="headerlink" href="#example-tuning-hyperparameters-for-catboost" title="Link to this heading"></a></h3>
<p>When defining hyperparameters for grid search, specify only one alias in your configuration. Below is an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cat_name</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span>
<span class="n">tuned_hyperparameters_cat</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cat_name</span><span class="si">}</span><span class="s2">__n_estimators&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1500</span><span class="p">],</span>  <span class="c1"># Use only &quot;n_estimators&quot;</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cat_name</span><span class="si">}</span><span class="s2">__learning_rate&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cat_name</span><span class="si">}</span><span class="s2">__depth&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cat_name</span><span class="si">}</span><span class="s2">__loss_function&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Logloss&quot;</span><span class="p">],</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This ensures compatibility with CatBoost’s requirements and avoids errors during hyperparameter tuning.</p>
</section>
<section id="step-6-initialize-and-configure-the-model">
<h3>Step 6: Initialize and Configure the <code class="docutils literal notranslate"><span class="pre">Model</span></code><a class="headerlink" href="#step-6-initialize-and-configure-the-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;xgb&quot;</span>
<span class="n">clc</span> <span class="o">=</span> <span class="n">xgb_definition</span><span class="p">[</span><span class="s2">&quot;clc&quot;</span><span class="p">]</span>
<span class="n">estimator_name</span> <span class="o">=</span> <span class="n">xgb_definition</span><span class="p">[</span><span class="s2">&quot;estimator_name&quot;</span><span class="p">]</span>

<span class="n">tuned_parameters</span> <span class="o">=</span> <span class="n">xgb_definition</span><span class="p">[</span><span class="s2">&quot;tuned_parameters&quot;</span><span class="p">]</span>
<span class="n">n_iter</span> <span class="o">=</span> <span class="n">xgb_definition</span><span class="p">[</span><span class="s2">&quot;n_iter&quot;</span><span class="p">]</span>
<span class="n">rand_grid</span> <span class="o">=</span> <span class="n">xgb_definition</span><span class="p">[</span><span class="s2">&quot;randomized_grid&quot;</span><span class="p">]</span>
<span class="n">early_stop</span> <span class="o">=</span> <span class="n">xgb_definition</span><span class="p">[</span><span class="s2">&quot;early&quot;</span><span class="p">]</span>
<span class="n">kfold</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">calibrate</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># Initialize model_tuner</span>
<span class="n">model_xgb</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
   <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;AIDS_Clinical_</span><span class="si">{</span><span class="n">model_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
   <span class="n">estimator_name</span><span class="o">=</span><span class="n">estimator_name</span><span class="p">,</span>
   <span class="n">calibrate</span><span class="o">=</span><span class="n">calibrate</span><span class="p">,</span>
   <span class="n">estimator</span><span class="o">=</span><span class="n">clc</span><span class="p">,</span>
   <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;classification&quot;</span><span class="p">,</span>
   <span class="n">kfold</span><span class="o">=</span><span class="n">kfold</span><span class="p">,</span>
   <span class="n">stratify_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
   <span class="n">stratify_cols</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;race&quot;</span><span class="p">],</span>
   <span class="n">grid</span><span class="o">=</span><span class="n">tuned_parameters</span><span class="p">,</span>
   <span class="n">randomized_grid</span><span class="o">=</span><span class="n">rand_grid</span><span class="p">,</span>
   <span class="n">boost_early</span><span class="o">=</span><span class="n">early_stop</span><span class="p">,</span>
   <span class="n">scoring</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;roc_auc&quot;</span><span class="p">],</span>
   <span class="n">random_state</span><span class="o">=</span><span class="mi">222</span><span class="p">,</span>
   <span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-7-perform-grid-search-parameter-tuning">
<h3>Step 7: Perform Grid Search Parameter Tuning<a class="headerlink" href="#step-7-perform-grid-search-parameter-tuning" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform grid search parameter tuning</span>
<span class="n">model_xgb</span><span class="o">.</span><span class="n">grid_search_param_tuning</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f1_beta_tune</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Pipeline<span class="w"> </span>Steps:

┌─────────────────┐
│<span class="w"> </span>Step<span class="w"> </span><span class="m">1</span>:<span class="w"> </span>xgb<span class="w">     </span>│
│<span class="w"> </span>XGBClassifier<span class="w">   </span>│
└─────────────────┘

<span class="m">100</span>%<span class="p">|</span>██████████<span class="p">|</span><span class="w"> </span><span class="m">5</span>/5<span class="w"> </span><span class="o">[</span><span class="m">00</span>:19&lt;<span class="m">00</span>:00,<span class="w">  </span><span class="m">3</span>.98s/it<span class="o">]</span>
Fitting<span class="w"> </span>model<span class="w"> </span>with<span class="w"> </span>best<span class="w"> </span>params<span class="w"> </span>and<span class="w"> </span>tuning<span class="w"> </span><span class="k">for</span><span class="w"> </span>best<span class="w"> </span>threshold<span class="w"> </span>...
<span class="m">100</span>%<span class="p">|</span>██████████<span class="p">|</span><span class="w"> </span><span class="m">2</span>/2<span class="w"> </span><span class="o">[</span><span class="m">00</span>:00&lt;<span class="m">00</span>:00,<span class="w">  </span><span class="m">3</span>.42it/s<span class="o">]</span>Best<span class="w"> </span>score/param<span class="w"> </span><span class="nb">set</span><span class="w"> </span>found<span class="w"> </span>on<span class="w"> </span>validation<span class="w"> </span>set:
<span class="o">{</span><span class="s1">&#39;params&#39;</span>:<span class="w"> </span><span class="o">{</span><span class="s1">&#39;xgb__early_stopping_rounds&#39;</span>:<span class="w"> </span><span class="m">100</span>,
<span class="w">            </span><span class="s1">&#39;xgb__eval_metric&#39;</span>:<span class="w"> </span><span class="s1">&#39;logloss&#39;</span>,
<span class="w">            </span><span class="s1">&#39;xgb__learning_rate&#39;</span>:<span class="w"> </span><span class="m">0</span>.0001,
<span class="w">            </span><span class="s1">&#39;xgb__max_depth&#39;</span>:<span class="w"> </span><span class="m">3</span>,
<span class="w">            </span><span class="s1">&#39;xgb__n_estimators&#39;</span>:<span class="w"> </span><span class="m">999</span><span class="o">}</span>,
<span class="s1">&#39;score&#39;</span>:<span class="w"> </span><span class="m">0</span>.9280033238366572<span class="o">}</span>
Best<span class="w"> </span>roc_auc:<span class="w"> </span><span class="m">0</span>.928
</pre></div>
</div>
</section>
<section id="step-8-fit-the-model">
<h3>Step 8: Fit the Model<a class="headerlink" href="#step-8-fit-the-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## Get the training and validation data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">model_tuner</span><span class="o">.</span><span class="n">get_train_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">model_tuner</span><span class="o">.</span><span class="n">get_valid_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_tuner</span><span class="o">.</span><span class="n">get_test_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">model_xgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="p">[</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-9-return-metrics-optional">
<h3>Step 9: Return Metrics (Optional)<a class="headerlink" href="#step-9-return-metrics-optional" title="Link to this heading"></a></h3>
<p>You can use this function to evaluate the model by printing the output.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Validation<span class="w"> </span>Metrics
Confusion<span class="w"> </span>matrix<span class="w"> </span>on<span class="w"> </span><span class="nb">set</span><span class="w"> </span>provided:
--------------------------------------------------------------------------------
<span class="w">         </span>Predicted:
<span class="w">             </span>Pos<span class="w">   </span>Neg
--------------------------------------------------------------------------------
Actual:<span class="w"> </span>Pos<span class="w">  </span><span class="m">95</span><span class="w"> </span><span class="o">(</span>tp<span class="o">)</span><span class="w">    </span><span class="m">9</span><span class="w"> </span><span class="o">(</span>fn<span class="o">)</span>
<span class="w">        </span>Neg<span class="w">  </span><span class="m">79</span><span class="w"> </span><span class="o">(</span>fp<span class="o">)</span><span class="w">  </span><span class="m">245</span><span class="w"> </span><span class="o">(</span>tn<span class="o">)</span>
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
<span class="o">{</span><span class="s1">&#39;AUC ROC&#39;</span>:<span class="w"> </span><span class="m">0</span>.9280033238366572,
<span class="s1">&#39;Average Precision&#39;</span>:<span class="w"> </span><span class="m">0</span>.7992275185850191,
<span class="s1">&#39;Brier Score&#39;</span>:<span class="w"> </span><span class="m">0</span>.16713189436073958,
<span class="s1">&#39;Precision/PPV&#39;</span>:<span class="w"> </span><span class="m">0</span>.5459770114942529,
<span class="s1">&#39;Sensitivity&#39;</span>:<span class="w"> </span><span class="m">0</span>.9134615384615384,
<span class="s1">&#39;Specificity&#39;</span>:<span class="w"> </span><span class="m">0</span>.7561728395061729<span class="o">}</span>
--------------------------------------------------------------------------------

<span class="w">              </span>precision<span class="w">    </span>recall<span class="w">  </span>f1-score<span class="w">   </span>support

<span class="w">           </span><span class="m">0</span><span class="w">       </span><span class="m">0</span>.96<span class="w">      </span><span class="m">0</span>.76<span class="w">      </span><span class="m">0</span>.85<span class="w">       </span><span class="m">324</span>
<span class="w">           </span><span class="m">1</span><span class="w">       </span><span class="m">0</span>.55<span class="w">      </span><span class="m">0</span>.91<span class="w">      </span><span class="m">0</span>.68<span class="w">       </span><span class="m">104</span>

<span class="w">    </span>accuracy<span class="w">                           </span><span class="m">0</span>.79<span class="w">       </span><span class="m">428</span>
<span class="w">   </span>macro<span class="w"> </span>avg<span class="w">       </span><span class="m">0</span>.76<span class="w">      </span><span class="m">0</span>.83<span class="w">      </span><span class="m">0</span>.77<span class="w">       </span><span class="m">428</span>
weighted<span class="w"> </span>avg<span class="w">       </span><span class="m">0</span>.86<span class="w">      </span><span class="m">0</span>.79<span class="w">      </span><span class="m">0</span>.81<span class="w">       </span><span class="m">428</span>

--------------------------------------------------------------------------------

Test<span class="w"> </span>Metrics
Confusion<span class="w"> </span>matrix<span class="w"> </span>on<span class="w"> </span><span class="nb">set</span><span class="w"> </span>provided:
--------------------------------------------------------------------------------
<span class="w">         </span>Predicted:
<span class="w">             </span>Pos<span class="w">   </span>Neg
--------------------------------------------------------------------------------
Actual:<span class="w"> </span>Pos<span class="w">  </span><span class="m">95</span><span class="w"> </span><span class="o">(</span>tp<span class="o">)</span><span class="w">    </span><span class="m">9</span><span class="w"> </span><span class="o">(</span>fn<span class="o">)</span>
<span class="w">        </span>Neg<span class="w">  </span><span class="m">78</span><span class="w"> </span><span class="o">(</span>fp<span class="o">)</span><span class="w">  </span><span class="m">246</span><span class="w"> </span><span class="o">(</span>tn<span class="o">)</span>
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
<span class="o">{</span><span class="s1">&#39;AUC ROC&#39;</span>:<span class="w"> </span><span class="m">0</span>.934576804368471,
<span class="s1">&#39;Average Precision&#39;</span>:<span class="w"> </span><span class="m">0</span>.8023014087345259,
<span class="s1">&#39;Brier Score&#39;</span>:<span class="w"> </span><span class="m">0</span>.16628708993634742,
<span class="s1">&#39;Precision/PPV&#39;</span>:<span class="w"> </span><span class="m">0</span>.5491329479768786,
<span class="s1">&#39;Sensitivity&#39;</span>:<span class="w"> </span><span class="m">0</span>.9134615384615384,
<span class="s1">&#39;Specificity&#39;</span>:<span class="w"> </span><span class="m">0</span>.7592592592592593<span class="o">}</span>
--------------------------------------------------------------------------------

<span class="w">              </span>precision<span class="w">    </span>recall<span class="w">  </span>f1-score<span class="w">   </span>support

<span class="w">           </span><span class="m">0</span><span class="w">       </span><span class="m">0</span>.96<span class="w">      </span><span class="m">0</span>.76<span class="w">      </span><span class="m">0</span>.85<span class="w">       </span><span class="m">324</span>
<span class="w">           </span><span class="m">1</span><span class="w">       </span><span class="m">0</span>.55<span class="w">      </span><span class="m">0</span>.91<span class="w">      </span><span class="m">0</span>.69<span class="w">       </span><span class="m">104</span>

<span class="w">    </span>accuracy<span class="w">                           </span><span class="m">0</span>.80<span class="w">       </span><span class="m">428</span>
<span class="w">   </span>macro<span class="w"> </span>avg<span class="w">       </span><span class="m">0</span>.76<span class="w">      </span><span class="m">0</span>.84<span class="w">      </span><span class="m">0</span>.77<span class="w">       </span><span class="m">428</span>
weighted<span class="w"> </span>avg<span class="w">       </span><span class="m">0</span>.86<span class="w">      </span><span class="m">0</span>.80<span class="w">      </span><span class="m">0</span>.81<span class="w">       </span><span class="m">428</span>

--------------------------------------------------------------------------------
</pre></div>
</div>
</section>
<section id="step-10-calibrate-the-model-if-needed">
<h3>Step 10: Calibrate the Model (if needed)<a class="headerlink" href="#step-10-calibrate-the-model-if-needed" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.calibration</span> <span class="kn">import</span> <span class="n">calibration_curve</span>

<span class="c1">## Get the predicted probabilities for the validation data from uncalibrated model</span>
<span class="n">y_prob_uncalibrated</span> <span class="o">=</span> <span class="n">model_xgb</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1">## Compute the calibration curve for the uncalibrated model</span>
<span class="n">prob_true_uncalibrated</span><span class="p">,</span> <span class="n">prob_pred_uncalibrated</span> <span class="o">=</span> <span class="n">calibration_curve</span><span class="p">(</span>
   <span class="n">y_test</span><span class="p">,</span>
   <span class="n">y_prob_uncalibrated</span><span class="p">,</span>
   <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1">## Calibrate the model</span>
<span class="k">if</span> <span class="n">model_xgb</span><span class="o">.</span><span class="n">calibrate</span><span class="p">:</span>
   <span class="n">model_xgb</span><span class="o">.</span><span class="n">calibrateModel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">score</span><span class="o">=</span><span class="s2">&quot;roc_auc&quot;</span><span class="p">)</span>

<span class="c1">## Predict on the validation set</span>
<span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">model_xgb</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Change<span class="w"> </span>back<span class="w"> </span>to<span class="w"> </span>CPU
Confusion<span class="w"> </span>matrix<span class="w"> </span>on<span class="w"> </span>validation<span class="w"> </span><span class="nb">set</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>roc_auc
--------------------------------------------------------------------------------
<span class="w">         </span>Predicted:
<span class="w">             </span>Pos<span class="w">   </span>Neg
--------------------------------------------------------------------------------
Actual:<span class="w"> </span>Pos<span class="w">  </span><span class="m">70</span><span class="w"> </span><span class="o">(</span>tp<span class="o">)</span><span class="w">   </span><span class="m">34</span><span class="w"> </span><span class="o">(</span>fn<span class="o">)</span>
<span class="w">        </span>Neg<span class="w">   </span><span class="m">9</span><span class="w"> </span><span class="o">(</span>fp<span class="o">)</span><span class="w">  </span><span class="m">315</span><span class="w"> </span><span class="o">(</span>tn<span class="o">)</span>
--------------------------------------------------------------------------------

<span class="w">              </span>precision<span class="w">     </span>recall<span class="w">  </span>f1-score<span class="w">    </span>support

<span class="w">           </span><span class="m">0</span><span class="w">       </span><span class="m">0</span>.90<span class="w">       </span><span class="m">0</span>.97<span class="w">      </span><span class="m">0</span>.94<span class="w">        </span><span class="m">324</span>
<span class="w">           </span><span class="m">1</span><span class="w">       </span><span class="m">0</span>.89<span class="w">       </span><span class="m">0</span>.67<span class="w">      </span><span class="m">0</span>.77<span class="w">        </span><span class="m">104</span>

<span class="w">    </span>accuracy<span class="w">                            </span><span class="m">0</span>.90<span class="w">        </span><span class="m">428</span>
<span class="w">   </span>macro<span class="w"> </span>avg<span class="w">       </span><span class="m">0</span>.89<span class="w">       </span><span class="m">0</span>.82<span class="w">      </span><span class="m">0</span>.85<span class="w">        </span><span class="m">428</span>
weighted<span class="w"> </span>avg<span class="w">       </span><span class="m">0</span>.90<span class="w">       </span><span class="m">0</span>.90<span class="w">      </span><span class="m">0</span>.89<span class="w">        </span><span class="m">428</span>

--------------------------------------------------------------------------------
roc_auc<span class="w"> </span>after<span class="w"> </span>calibration:<span class="w"> </span><span class="m">0</span>.9280033238366572
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## Get the predicted probabilities for the validation data from calibrated model</span>
<span class="n">y_prob_calibrated</span> <span class="o">=</span> <span class="n">model_xgb</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1">## Compute the calibration curve for the calibrated model</span>
<span class="n">prob_true_calibrated</span><span class="p">,</span> <span class="n">prob_pred_calibrated</span> <span class="o">=</span> <span class="n">calibration_curve</span><span class="p">(</span>
   <span class="n">y_test</span><span class="p">,</span>
   <span class="n">y_prob_calibrated</span><span class="p">,</span>
   <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1">## Plot the calibration curves</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
   <span class="n">prob_pred_uncalibrated</span><span class="p">,</span>
   <span class="n">prob_true_uncalibrated</span><span class="p">,</span>
   <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
   <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Uncalibrated XGBoost&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
   <span class="n">prob_pred_calibrated</span><span class="p">,</span>
   <span class="n">prob_true_calibrated</span><span class="p">,</span>
   <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
   <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Calibrated XGBoost&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
   <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
   <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
   <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
   <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Perfectly calibrated&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted probability&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;True probability in each bin&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Calibration plot (reliability curve)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="no-click"><a class="reference internal image-reference" href="_images/calibration_curves.png"><img alt="Calibration Curve AIDs" class="align-center" src="_images/calibration_curves.png" style="width: 400px;" />
</a>
</div><div style="height: 50px;"></div></section>
<section id="classification-report-optional">
<h3>Classification Report (Optional)<a class="headerlink" href="#classification-report-optional" title="Link to this heading"></a></h3>
<p>A classification report is readily available at this stage, should you wish to
print and examine it. A call to <code class="docutils literal notranslate"><span class="pre">print(model_tuner.classification_report)</span></code> will
output it as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model_tuner</span><span class="o">.</span><span class="n">classification_report</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">             </span>precision<span class="w">    </span>recall<span class="w">  </span>f1-score<span class="w">   </span>support

<span class="w">           </span><span class="m">0</span><span class="w">      </span><span class="m">0</span>.90<span class="w">      </span><span class="m">0</span>.97<span class="w">      </span><span class="m">0</span>.94<span class="w">       </span><span class="m">324</span>
<span class="w">           </span><span class="m">1</span><span class="w">      </span><span class="m">0</span>.89<span class="w">      </span><span class="m">0</span>.67<span class="w">      </span><span class="m">0</span>.77<span class="w">       </span><span class="m">104</span>

<span class="w">    </span>accuracy<span class="w">                          </span><span class="m">0</span>.90<span class="w">       </span><span class="m">428</span>
<span class="w">   </span>macro<span class="w"> </span>avg<span class="w">      </span><span class="m">0</span>.89<span class="w">      </span><span class="m">0</span>.82<span class="w">      </span><span class="m">0</span>.85<span class="w">       </span><span class="m">428</span>
weighted<span class="w"> </span>avg<span class="w">      </span><span class="m">0</span>.90<span class="w">      </span><span class="m">0</span>.90<span class="w">      </span><span class="m">0</span>.89<span class="w">       </span><span class="m">428</span>
</pre></div>
</div>
</section>
</section>
<section id="recursive-feature-elimination-rfe">
<h2>Recursive Feature Elimination (RFE)<a class="headerlink" href="#recursive-feature-elimination-rfe" title="Link to this heading"></a></h2>
<p>Now that we’ve trained the models, we can also refine them by identifying which
features contribute most to their performance. One effective method for this is
Recursive Feature Elimination (RFE). This technique allows us to systematically
remove the least important features, retraining the model at each step to evaluate
how performance is affected. By focusing only on the most impactful variables,
RFE helps streamline the dataset, reduce noise, and improve both the accuracy and
interpretability of the final model.</p>
<p>It works by recursively training a model, ranking the importance of features
based on the model’s outputas (such as coefficients in linear models or
importance scores in tree-based models), and then removing the least important
features one by one. This process continues until a specified number of features
remains or the desired performance criteria are met.</p>
<p>The primary advantage of RFE is its ability to streamline datasets, improving
model performance and interpretability by focusing on features that contribute
the most to the predictive power. However, it can be computationally expensive
since it involves repeated model training, and its effectiveness depends on the
underlying model’s ability to evaluate feature importance. RFE is commonly used
with cross-validation to ensure that the selected features generalize well across
datasets, making it a robust choice for model optimization and dimensionality
reduction.</p>
<p>As an illustrative example, we will retrain the above model using RFE.</p>
<p>We will begin by appending the feature selection technique to our <a class="reference internal" href="#xgb-hyperparams"><span class="std std-ref">tuned parameters dictionary</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xgb_definition</span><span class="p">[</span><span class="s2">&quot;tuned_parameters&quot;</span><span class="p">][</span><span class="sa">f</span><span class="s2">&quot;feature_selection_rfe__n_features_to_select&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
   <span class="mi">5</span><span class="p">,</span>
   <span class="mi">10</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<section id="elastic-net-for-feature-selection-with-rfe">
<h3>Elastic Net for Feature Selection with RFE<a class="headerlink" href="#elastic-net-for-feature-selection-with-rfe" title="Link to this heading"></a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You may wish to explore <a class="reference internal" href="caveats.html#elastic-net"><span class="std std-ref">this section</span></a> for the rationale in applying this technique.</p>
</div>
<p>We will use elastic net because it strikes a balance between two widely used
regularization techniques: Lasso (<span class="math notranslate nohighlight">\(L1\)</span>) and Ridge (<span class="math notranslate nohighlight">\(L2\)</span>). Elastic net
is particularly effective in scenarios where we expect the dataset to have a mix
of strongly and weakly correlated features. Lasso alone tends to select only one
feature from a group of highly correlated ones, ignoring the others, while Ridge
includes all features but may not perform well when some are entirely irrelevant.
Elastic net addresses this limitation by combining both penalties, allowing it to handle
multicollinearity more effectively while still performing feature selection.</p>
<p>Additionally, elastic net provides flexibility by controlling the ratio between
<span class="math notranslate nohighlight">\(L1\)</span> and <span class="math notranslate nohighlight">\(L2\)</span> penalties, enabling fine-tuning to suit the specific needs of
our dataset. This makes it a robust choice for datasets with many features, some
of which may be irrelevant or redundant, as it can reduce overfitting while
retaining a manageable subset of predictors.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rfe_estimator</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">()</span>

<span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">rfe_estimator</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">model_tuner</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="n">model_xgb</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
   <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;AIDS_Clinical_</span><span class="si">{</span><span class="n">model_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
   <span class="n">estimator_name</span><span class="o">=</span><span class="n">estimator_name</span><span class="p">,</span>
   <span class="n">calibrate</span><span class="o">=</span><span class="n">calibrate</span><span class="p">,</span>
   <span class="n">estimator</span><span class="o">=</span><span class="n">clc</span><span class="p">,</span>
   <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;classification&quot;</span><span class="p">,</span>
   <span class="n">kfold</span><span class="o">=</span><span class="n">kfold</span><span class="p">,</span>
   <span class="n">pipeline_steps</span><span class="o">=</span><span class="p">[</span>
      <span class="p">(</span><span class="s2">&quot;rfe&quot;</span><span class="p">,</span> <span class="n">rfe</span><span class="p">),</span>
   <span class="p">],</span>
   <span class="n">stratify_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
   <span class="n">stratify_cols</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
   <span class="n">grid</span><span class="o">=</span><span class="n">tuned_parameters</span><span class="p">,</span>
   <span class="n">randomized_grid</span><span class="o">=</span><span class="n">rand_grid</span><span class="p">,</span>
   <span class="n">feature_selection</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
   <span class="n">boost_early</span><span class="o">=</span><span class="n">early_stop</span><span class="p">,</span>
   <span class="n">scoring</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;roc_auc&quot;</span><span class="p">],</span>
   <span class="n">random_state</span><span class="o">=</span><span class="mi">222</span><span class="p">,</span>
   <span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model_xgb</span><span class="o">.</span><span class="n">grid_search_param_tuning</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f1_beta_tune</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">model_xgb</span><span class="o">.</span><span class="n">get_train_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_xgb</span><span class="o">.</span><span class="n">get_test_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">model_xgb</span><span class="o">.</span><span class="n">get_valid_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">model_xgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
   <span class="n">X_train</span><span class="p">,</span>
   <span class="n">y_train</span><span class="p">,</span>
   <span class="n">validation_data</span><span class="o">=</span><span class="p">[</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">],</span>
<span class="p">)</span>


<span class="c1"># ------------------------- VALID AND TEST METRICS -----------------------------</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Validation Metrics&quot;</span><span class="p">)</span>
<span class="n">model_xgb</span><span class="o">.</span><span class="n">return_metrics</span><span class="p">(</span>
   <span class="n">X_valid</span><span class="p">,</span>
   <span class="n">y_valid</span><span class="p">,</span>
   <span class="n">optimal_threshold</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test Metrics&quot;</span><span class="p">)</span>
<span class="n">model_xgb</span><span class="o">.</span><span class="n">return_metrics</span><span class="p">(</span>
   <span class="n">X_test</span><span class="p">,</span>
   <span class="n">y_test</span><span class="p">,</span>
   <span class="n">optimal_threshold</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>┌─────────────────────────────────┐
│<span class="w"> </span>Step<span class="w"> </span><span class="m">1</span>:<span class="w"> </span>feature_selection_rfe<span class="w">   </span>│
│<span class="w"> </span>RFE<span class="w">                             </span>│
└─────────────────────────────────┘
<span class="w">               </span>│
<span class="w">               </span>▼
┌─────────────────────────────────┐
│<span class="w"> </span>Step<span class="w"> </span><span class="m">2</span>:<span class="w"> </span>xgb<span class="w">                     </span>│
│<span class="w"> </span>XGBClassifier<span class="w">                   </span>│
└─────────────────────────────────┘

<span class="m">100</span>%<span class="p">|</span>██████████<span class="p">|</span><span class="w"> </span><span class="m">10</span>/10<span class="w"> </span><span class="o">[</span><span class="m">00</span>:25&lt;<span class="m">00</span>:00,<span class="w">  </span><span class="m">2</span>.52s/it<span class="o">]</span>
Fitting<span class="w"> </span>model<span class="w"> </span>with<span class="w"> </span>best<span class="w"> </span>params<span class="w"> </span>and<span class="w"> </span>tuning<span class="w"> </span><span class="k">for</span><span class="w"> </span>best<span class="w"> </span>threshold<span class="w"> </span>...
<span class="m">100</span>%<span class="p">|</span>██████████<span class="p">|</span><span class="w"> </span><span class="m">2</span>/2<span class="w"> </span><span class="o">[</span><span class="m">00</span>:00&lt;<span class="m">00</span>:00,<span class="w">  </span><span class="m">3</span>.53it/s<span class="o">]</span>
Best<span class="w"> </span>score/param<span class="w"> </span><span class="nb">set</span><span class="w"> </span>found<span class="w"> </span>on<span class="w"> </span>validation<span class="w"> </span>set:
<span class="o">{</span><span class="s1">&#39;params&#39;</span>:<span class="w"> </span><span class="o">{</span><span class="s1">&#39;feature_selection_rfe__n_features_to_select&#39;</span>:<span class="w"> </span><span class="m">10</span>,
<span class="w">            </span><span class="s1">&#39;xgb__early_stopping_rounds&#39;</span>:<span class="w"> </span><span class="m">100</span>,
<span class="w">            </span><span class="s1">&#39;xgb__eval_metric&#39;</span>:<span class="w"> </span><span class="s1">&#39;logloss&#39;</span>,
<span class="w">            </span><span class="s1">&#39;xgb__learning_rate&#39;</span>:<span class="w"> </span><span class="m">0</span>.0001,
<span class="w">            </span><span class="s1">&#39;xgb__max_depth&#39;</span>:<span class="w"> </span><span class="m">10</span>,
<span class="w">            </span><span class="s1">&#39;xgb__n_estimators&#39;</span>:<span class="w"> </span><span class="m">999</span><span class="o">}</span>,
<span class="s1">&#39;score&#39;</span>:<span class="w"> </span><span class="m">0</span>.9316684472934472<span class="o">}</span>
Best<span class="w"> </span>roc_auc:<span class="w"> </span><span class="m">0</span>.932

Validation<span class="w"> </span>Metrics
Confusion<span class="w"> </span>matrix<span class="w"> </span>on<span class="w"> </span><span class="nb">set</span><span class="w"> </span>provided:
--------------------------------------------------------------------------------
<span class="w">          </span>Predicted:
<span class="w">             </span>Pos<span class="w">   </span>Neg
--------------------------------------------------------------------------------
Actual:<span class="w"> </span>Pos<span class="w">  </span><span class="m">95</span><span class="w"> </span><span class="o">(</span>tp<span class="o">)</span><span class="w">    </span><span class="m">9</span><span class="w"> </span><span class="o">(</span>fn<span class="o">)</span>
<span class="w">        </span>Neg<span class="w">  </span><span class="m">70</span><span class="w"> </span><span class="o">(</span>fp<span class="o">)</span><span class="w">  </span><span class="m">254</span><span class="w"> </span><span class="o">(</span>tn<span class="o">)</span>
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
<span class="o">{</span><span class="s1">&#39;AUC ROC&#39;</span>:<span class="w"> </span><span class="m">0</span>.9316981244064577,
<span class="s1">&#39;Average Precision&#39;</span>:<span class="w"> </span><span class="m">0</span>.8206553111036822,
<span class="s1">&#39;Brier Score&#39;</span>:<span class="w"> </span><span class="m">0</span>.16608154668556174,
<span class="s1">&#39;Precision/PPV&#39;</span>:<span class="w"> </span><span class="m">0</span>.5757575757575758,
<span class="s1">&#39;Sensitivity&#39;</span>:<span class="w"> </span><span class="m">0</span>.9134615384615384,
<span class="s1">&#39;Specificity&#39;</span>:<span class="w"> </span><span class="m">0</span>.7839506172839507<span class="o">}</span>
--------------------------------------------------------------------------------

<span class="w">             </span>precision<span class="w">    </span>recall<span class="w">  </span>f1-score<span class="w">   </span>support

<span class="w">           </span><span class="m">0</span><span class="w">      </span><span class="m">0</span>.97<span class="w">      </span><span class="m">0</span>.78<span class="w">      </span><span class="m">0</span>.87<span class="w">       </span><span class="m">324</span>
<span class="w">           </span><span class="m">1</span><span class="w">      </span><span class="m">0</span>.58<span class="w">      </span><span class="m">0</span>.91<span class="w">      </span><span class="m">0</span>.71<span class="w">       </span><span class="m">104</span>

<span class="w">    </span>accuracy<span class="w">                          </span><span class="m">0</span>.82<span class="w">       </span><span class="m">428</span>
<span class="w">   </span>macro<span class="w"> </span>avg<span class="w">      </span><span class="m">0</span>.77<span class="w">      </span><span class="m">0</span>.85<span class="w">      </span><span class="m">0</span>.79<span class="w">       </span><span class="m">428</span>
weighted<span class="w"> </span>avg<span class="w">      </span><span class="m">0</span>.87<span class="w">      </span><span class="m">0</span>.82<span class="w">      </span><span class="m">0</span>.83<span class="w">       </span><span class="m">428</span>

--------------------------------------------------------------------------------

Feature<span class="w"> </span>names<span class="w"> </span>selected:
<span class="o">[</span><span class="s1">&#39;time&#39;</span>,<span class="w"> </span><span class="s1">&#39;preanti&#39;</span>,<span class="w"> </span><span class="s1">&#39;strat&#39;</span>,<span class="w"> </span><span class="s1">&#39;symptom&#39;</span>,<span class="w"> </span><span class="s1">&#39;treat&#39;</span>,<span class="w"> </span><span class="s1">&#39;offtrt&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd40&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd420&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd80&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd820&#39;</span><span class="o">]</span>


Test<span class="w"> </span>Metrics
Confusion<span class="w"> </span>matrix<span class="w"> </span>on<span class="w"> </span><span class="nb">set</span><span class="w"> </span>provided:
--------------------------------------------------------------------------------
<span class="w">          </span>Predicted:
<span class="w">             </span>Pos<span class="w">   </span>Neg
--------------------------------------------------------------------------------
Actual:<span class="w"> </span>Pos<span class="w">  </span><span class="m">91</span><span class="w"> </span><span class="o">(</span>tp<span class="o">)</span><span class="w">   </span><span class="m">13</span><span class="w"> </span><span class="o">(</span>fn<span class="o">)</span>
<span class="w">        </span>Neg<span class="w">  </span><span class="m">70</span><span class="w"> </span><span class="o">(</span>fp<span class="o">)</span><span class="w">  </span><span class="m">254</span><span class="w"> </span><span class="o">(</span>tn<span class="o">)</span>
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
<span class="o">{</span><span class="s1">&#39;AUC ROC&#39;</span>:<span class="w"> </span><span class="m">0</span>.9278104226020893,
<span class="s1">&#39;Average Precision&#39;</span>:<span class="w"> </span><span class="m">0</span>.8133787683637559,
<span class="s1">&#39;Brier Score&#39;</span>:<span class="w"> </span><span class="m">0</span>.1658272032260468,
<span class="s1">&#39;Precision/PPV&#39;</span>:<span class="w"> </span><span class="m">0</span>.5652173913043478,
<span class="s1">&#39;Sensitivity&#39;</span>:<span class="w"> </span><span class="m">0</span>.875,
<span class="s1">&#39;Specificity&#39;</span>:<span class="w"> </span><span class="m">0</span>.7839506172839507<span class="o">}</span>
--------------------------------------------------------------------------------

<span class="w">              </span>precision<span class="w">    </span>recall<span class="w">  </span>f1-score<span class="w">   </span>support

<span class="w">           </span><span class="m">0</span><span class="w">       </span><span class="m">0</span>.95<span class="w">      </span><span class="m">0</span>.78<span class="w">      </span><span class="m">0</span>.86<span class="w">       </span><span class="m">324</span>
<span class="w">           </span><span class="m">1</span><span class="w">       </span><span class="m">0</span>.57<span class="w">      </span><span class="m">0</span>.88<span class="w">      </span><span class="m">0</span>.69<span class="w">       </span><span class="m">104</span>

<span class="w">    </span>accuracy<span class="w">                           </span><span class="m">0</span>.81<span class="w">       </span><span class="m">428</span>
<span class="w">   </span>macro<span class="w"> </span>avg<span class="w">       </span><span class="m">0</span>.76<span class="w">      </span><span class="m">0</span>.83<span class="w">      </span><span class="m">0</span>.77<span class="w">       </span><span class="m">428</span>
weighted<span class="w"> </span>avg<span class="w">       </span><span class="m">0</span>.86<span class="w">      </span><span class="m">0</span>.81<span class="w">      </span><span class="m">0</span>.82<span class="w">       </span><span class="m">428</span>

--------------------------------------------------------------------------------

Feature<span class="w"> </span>names<span class="w"> </span>selected:
<span class="o">[</span><span class="s1">&#39;time&#39;</span>,<span class="w"> </span><span class="s1">&#39;preanti&#39;</span>,<span class="w"> </span><span class="s1">&#39;strat&#39;</span>,<span class="w"> </span><span class="s1">&#39;symptom&#39;</span>,<span class="w"> </span><span class="s1">&#39;treat&#39;</span>,<span class="w"> </span><span class="s1">&#39;offtrt&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd40&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd420&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd80&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd820&#39;</span><span class="o">]</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Passing <code class="docutils literal notranslate"><span class="pre">feature_selection=True</span></code> in conjunction with accounting for <code class="docutils literal notranslate"><span class="pre">rfe</span></code> for
the <code class="docutils literal notranslate"><span class="pre">pipeline_steps</span></code> inside the <code class="docutils literal notranslate"><span class="pre">Model`</span></code> class above is necessary to print the
output of the feature names selected, thus yielding:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Feature<span class="w"> </span>names<span class="w"> </span>selected:
<span class="o">[</span><span class="s1">&#39;offtrt&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd40&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd420&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd80&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd820&#39;</span><span class="o">]</span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="imbalanced-learning">
<h2>Imbalanced Learning<a class="headerlink" href="#imbalanced-learning" title="Link to this heading"></a></h2>
<p>In machine learning, imbalanced datasets are a frequent challenge, especially in
real-world scenarios. These datasets have an unequal distribution of target classes,
with one class (e.g., fraudulent transactions, rare diseases, or other low-frequency events)
being underrepresented compared to the majority class. Models trained on imbalanced data
often struggle to generalize, as they tend to favor the majority class, leading to
poor performance on the minority class.</p>
<p>To mitigate these issues, it is crucial to:</p>
<ol class="arabic simple">
<li><p>Understand the nature of the imbalance in the dataset.</p></li>
<li><p>Apply appropriate resampling techniques (oversampling, undersampling, or hybrid methods).</p></li>
<li><p>Use metrics beyond accuracy, such as <a class="reference internal" href="caveats.html#limitations-of-accuracy"><span class="std std-ref">precision, recall, and F1-score</span></a>, to evaluate model performance fairly.</p></li>
</ol>
<section id="generating-an-imbalanced-dataset">
<h3>Generating an Imbalanced Dataset<a class="headerlink" href="#generating-an-imbalanced-dataset" title="Link to this heading"></a></h3>
<p>Demonstrated below are the steps to generate an imbalanced dataset using
<code class="docutils literal notranslate"><span class="pre">make_classification</span></code> from the <code class="docutils literal notranslate"><span class="pre">sklearn.datasets</span></code> module. The following
parameters are specified:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_samples=1000</span></code>: The dataset contains 1,000 samples.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_features=20</span></code>: Each sample has 20 features.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_informative=2</span></code>: Two features are informative for predicting the target.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_redundant=2</span></code>: Two features are linear combinations of the informative features.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weights=[0.9,</span> <span class="pre">0.1]</span></code>: The target class distribution is 90% for the majority class and 10% for the minority class, creating an imbalance.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">flip_y=0</span></code>: No label noise is added to the target variable.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">random_state=42</span></code>: Ensures reproducibility by using a fixed random seed.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span>
   <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
   <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
   <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
   <span class="n">n_redundant</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
   <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
   <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
   <span class="n">flip_y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
   <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1">## Convert to a pandas DataFrame for better visualization</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;feature_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">)])</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="s2">&quot;target&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">col</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>Below, you will see that the dataset we have generated is severely imbalanced with
900 observations allocated to the majority class (0) and 100 observations to the minority class (1).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1">## Create a bar plot</span>
<span class="n">value_counts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">value_counts</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
   <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;bar&quot;</span><span class="p">,</span>
   <span class="n">rot</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
   <span class="n">width</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1">## Add labels inside the bars</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">value_counts</span><span class="p">):</span>
   <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
      <span class="n">index</span><span class="p">,</span>
      <span class="n">count</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span>
      <span class="nb">str</span><span class="p">(</span><span class="n">count</span><span class="p">),</span>
      <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span>
      <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span>
      <span class="n">color</span><span class="o">=</span><span class="s2">&quot;yellow&quot;</span><span class="p">,</span>
   <span class="p">)</span>

<span class="c1">## Customize labels and title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Class Distribution&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span> <span class="c1">## Show the plot</span>
</pre></div>
</div>
<div class="no-click"><a class="reference internal image-reference" href="_images/imbalanced_classes.png"><img alt="Calibration Curve AIDs" class="align-center" src="_images/imbalanced_classes.png" style="width: 400px;" />
</a>
</div><div style="height: 50px;"></div></section>
<section id="define-hyperparameters-for-xgboost">
<h3>Define Hyperparameters for XGBoost<a class="headerlink" href="#define-hyperparameters-for-xgboost" title="Link to this heading"></a></h3>
<p>Below, we will use an XGBoost classifier with the following hyperparameters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>

<span class="n">xgb_name</span> <span class="o">=</span> <span class="s2">&quot;xgb&quot;</span>
<span class="n">xgb</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span>
   <span class="n">random_state</span><span class="o">=</span><span class="mi">222</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">xgbearly</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">tuned_parameters_xgb</span> <span class="o">=</span> <span class="p">{</span>
   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__max_depth&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">500</span><span class="p">],</span>
   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__learning_rate&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-4</span><span class="p">],</span>
   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__n_estimators&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1000</span><span class="p">],</span>
   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__early_stopping_rounds&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">],</span>
   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__verbose&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
   <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__eval_metric&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;logloss&quot;</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">xgb_definition</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s2">&quot;clc&quot;</span><span class="p">:</span> <span class="n">xgb</span><span class="p">,</span>
   <span class="s2">&quot;estimator_name&quot;</span><span class="p">:</span> <span class="n">xgb_name</span><span class="p">,</span>
   <span class="s2">&quot;tuned_parameters&quot;</span><span class="p">:</span> <span class="n">tuned_parameters_xgb</span><span class="p">,</span>
   <span class="s2">&quot;randomized_grid&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
   <span class="s2">&quot;n_iter&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
   <span class="s2">&quot;early&quot;</span><span class="p">:</span> <span class="n">xgbearly</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="define-the-model-object">
<h3>Define The Model object<a class="headerlink" href="#define-the-model-object" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;xgb&quot;</span>
<span class="n">clc</span> <span class="o">=</span> <span class="n">xgb_definition</span><span class="p">[</span><span class="s2">&quot;clc&quot;</span><span class="p">]</span>
<span class="n">estimator_name</span> <span class="o">=</span> <span class="n">xgb_definition</span><span class="p">[</span><span class="s2">&quot;estimator_name&quot;</span><span class="p">]</span>

<span class="n">tuned_parameters</span> <span class="o">=</span> <span class="n">xgb_definition</span><span class="p">[</span><span class="s2">&quot;tuned_parameters&quot;</span><span class="p">]</span>
<span class="n">n_iter</span> <span class="o">=</span> <span class="n">xgb_definition</span><span class="p">[</span><span class="s2">&quot;n_iter&quot;</span><span class="p">]</span>
<span class="n">rand_grid</span> <span class="o">=</span> <span class="n">xgb_definition</span><span class="p">[</span><span class="s2">&quot;randomized_grid&quot;</span><span class="p">]</span>
<span class="n">early_stop</span> <span class="o">=</span> <span class="n">xgb_definition</span><span class="p">[</span><span class="s2">&quot;early&quot;</span><span class="p">]</span>
<span class="n">kfold</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">calibrate</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</section>
<section id="addressing-class-imbalance-in-machine-learning">
<h3>Addressing Class Imbalance in Machine Learning<a class="headerlink" href="#addressing-class-imbalance-in-machine-learning" title="Link to this heading"></a></h3>
<p>Class imbalance occurs when one class significantly outweighs another in the
dataset, leading to biased models that perform well on the majority class but
poorly on the minority class. Techniques like SMOTE and others aim to address
this issue by improving the representation of the minority class, ensuring balanced
learning and better generalization.</p>
<section id="techniques-to-address-class-imbalance">
<h4>Techniques to Address Class Imbalance<a class="headerlink" href="#techniques-to-address-class-imbalance" title="Link to this heading"></a></h4>
<p><strong>Resampling Techniques</strong></p>
<ul class="simple">
<li><p><strong>SMOTE (Synthetic Minority Oversampling Technique)</strong>: SMOTE generates synthetic samples for the minority class by interpolating between existing minority class data points and their nearest neighbors. This helps create a more balanced class distribution without merely duplicating data, thus avoiding overfitting.</p></li>
<li><p><strong>Oversampling</strong>: Randomly duplicates examples from the minority class to balance the dataset. While simple, it risks overfitting to the duplicated examples.</p></li>
<li><p><strong>Undersampling</strong>: Reduces the majority class by randomly removing samples. While effective, it can lead to loss of important information.</p></li>
</ul>
</section>
<section id="purpose-of-using-these-techniques">
<h4>Purpose of Using These Techniques<a class="headerlink" href="#purpose-of-using-these-techniques" title="Link to this heading"></a></h4>
<p>The goal of using these techniques is to improve model performance on imbalanced datasets, specifically by:</p>
<ul class="simple">
<li><p>Ensuring the model captures meaningful patterns in the minority class.</p></li>
<li><p>Reducing bias toward the majority class, which often dominates predictions in imbalanced datasets.</p></li>
<li><p>Improving metrics like <a class="reference internal" href="caveats.html#limitations-of-accuracy"><span class="std std-ref">recall, F1-score, and AUC-ROC</span></a> for the minority class, which are critical in applications like fraud detection, healthcare, and rare event prediction.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While we provide comprehensive examples for SMOTE, ADASYN, and
RandomUnderSampler in the <a class="reference external" href="https://colab.research.google.com/drive/16gWnRAJvpUjTIes5y1gFRdX1soASdV6m#scrollTo=3NYa_tQWy6HR">accompanying notebook</a>,
this documentation section demonstrates the implementation of SMOTE. The other
examples follow a similar workflow and can be executed by simply passing the
respective <code class="docutils literal notranslate"><span class="pre">imbalance_sampler</span></code> input to <code class="docutils literal notranslate"><span class="pre">ADASYN()</span></code> or <code class="docutils literal notranslate"><span class="pre">RandomUnderSampler()</span></code>, as
needed. For detailed examples of all methods, please refer to the linked notebook.</p>
</div>
</section>
</section>
<section id="synthetic-minority-oversampling-technique-smote">
<h3>Synthetic Minority Oversampling Technique (SMOTE)<a class="headerlink" href="#synthetic-minority-oversampling-technique-smote" title="Link to this heading"></a></h3>
<p>SMOTE (Synthetic Minority Oversampling Technique) is a method used to address
class imbalance in datasets. It generates synthetic samples for the minority
class by interpolating between existing minority samples and their nearest neighbors,
effectively increasing the size of the minority class without duplicating data.
This helps models better learn patterns from the minority class, improving
classification performance on imbalanced datasets.</p>
<section id="initalize-and-configure-the-model">
<h4>Initalize and Configure The Model<a class="headerlink" href="#initalize-and-configure-the-model" title="Link to this heading"></a></h4>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>In the code block below, we initalize and configure the model by calling the
<code class="docutils literal notranslate"><span class="pre">Model</span></code> class, and assign it to a new variable call <code class="docutils literal notranslate"><span class="pre">xgb_smote</span></code>. Notice that
we pass the <code class="docutils literal notranslate"><span class="pre">imbalance_sampler=SMOTE()</span></code> as a necessary step of activating
this imbalanced sampler.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">model_tuner</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="n">xgb_smote</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
   <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Make_Classification_</span><span class="si">{</span><span class="n">model_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
   <span class="n">estimator_name</span><span class="o">=</span><span class="n">estimator_name</span><span class="p">,</span>
   <span class="n">calibrate</span><span class="o">=</span><span class="n">calibrate</span><span class="p">,</span>
   <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;classification&quot;</span><span class="p">,</span>
   <span class="n">estimator</span><span class="o">=</span><span class="n">clc</span><span class="p">,</span>
   <span class="n">kfold</span><span class="o">=</span><span class="n">kfold</span><span class="p">,</span>
   <span class="n">stratify_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
   <span class="n">stratify_cols</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
   <span class="n">grid</span><span class="o">=</span><span class="n">tuned_parameters</span><span class="p">,</span>
   <span class="n">randomized_grid</span><span class="o">=</span><span class="n">rand_grid</span><span class="p">,</span>
   <span class="n">boost_early</span><span class="o">=</span><span class="n">early_stop</span><span class="p">,</span>
   <span class="n">scoring</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;roc_auc&quot;</span><span class="p">],</span>
   <span class="n">random_state</span><span class="o">=</span><span class="mi">222</span><span class="p">,</span>
   <span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
   <span class="n">imbalance_sampler</span><span class="o">=</span><span class="n">SMOTE</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="perform-grid-search-parameter-tuning-and-retrieve-split-data">
<h4>Perform Grid Search Parameter Tuning and Retrieve Split Data<a class="headerlink" href="#perform-grid-search-parameter-tuning-and-retrieve-split-data" title="Link to this heading"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xgb_smote</span><span class="o">.</span><span class="n">grid_search_param_tuning</span><span class="p">(</span>
   <span class="n">X</span><span class="p">,</span>
   <span class="n">y</span><span class="p">,</span>
   <span class="n">f1_beta_tune</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">xgb_smote</span><span class="o">.</span><span class="n">get_train_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">xgb_smote</span><span class="o">.</span><span class="n">get_test_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">xgb_smote</span><span class="o">.</span><span class="n">get_valid_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Pipeline<span class="w"> </span>Steps:

┌─────────────────────┐
│<span class="w"> </span>Step<span class="w"> </span><span class="m">1</span>:<span class="w"> </span>resampler<span class="w">   </span>│
│<span class="w"> </span>SMOTE<span class="w">               </span>│
└─────────────────────┘
<span class="w">         </span>│
<span class="w">         </span>▼
┌─────────────────────┐
│<span class="w"> </span>Step<span class="w"> </span><span class="m">2</span>:<span class="w"> </span>xgb<span class="w">         </span>│
│<span class="w"> </span>XGBClassifier<span class="w">       </span>│
└─────────────────────┘

Distribution<span class="w"> </span>of<span class="w"> </span>y<span class="w"> </span>values<span class="w"> </span>after<span class="w"> </span>resampling:<span class="w"> </span>target
<span class="m">0</span><span class="w">         </span><span class="m">540</span>
<span class="m">1</span><span class="w">         </span><span class="m">540</span>
Name:<span class="w"> </span>count,<span class="w"> </span>dtype:<span class="w"> </span>int64

<span class="m">100</span>%<span class="p">|</span>██████████<span class="p">|</span><span class="w"> </span><span class="m">5</span>/5<span class="w"> </span><span class="o">[</span><span class="m">00</span>:34&lt;<span class="m">00</span>:00,<span class="w">  </span><span class="m">6</span>.87s/it<span class="o">]</span>
Fitting<span class="w"> </span>model<span class="w"> </span>with<span class="w"> </span>best<span class="w"> </span>params<span class="w"> </span>and<span class="w"> </span>tuning<span class="w"> </span><span class="k">for</span><span class="w"> </span>best<span class="w"> </span>threshold<span class="w"> </span>...
<span class="m">100</span>%<span class="p">|</span>██████████<span class="p">|</span><span class="w"> </span><span class="m">2</span>/2<span class="w"> </span><span class="o">[</span><span class="m">00</span>:00&lt;<span class="m">00</span>:00,<span class="w">  </span><span class="m">4</span>.37it/s<span class="o">]</span>Best<span class="w"> </span>score/param<span class="w"> </span><span class="nb">set</span><span class="w"> </span>found<span class="w"> </span>on<span class="w"> </span>validation<span class="w"> </span>set:
<span class="o">{</span><span class="s1">&#39;params&#39;</span>:<span class="w"> </span><span class="o">{</span><span class="s1">&#39;xgb__early_stopping_rounds&#39;</span>:<span class="w"> </span><span class="m">100</span>,
<span class="w">            </span><span class="s1">&#39;xgb__eval_metric&#39;</span>:<span class="w"> </span><span class="s1">&#39;logloss&#39;</span>,
<span class="w">            </span><span class="s1">&#39;xgb__learning_rate&#39;</span>:<span class="w"> </span><span class="m">0</span>.0001,
<span class="w">            </span><span class="s1">&#39;xgb__max_depth&#39;</span>:<span class="w"> </span><span class="m">10</span>,
<span class="w">            </span><span class="s1">&#39;xgb__n_estimators&#39;</span>:<span class="w"> </span><span class="m">999</span><span class="o">}</span>,
<span class="s1">&#39;score&#39;</span>:<span class="w"> </span><span class="m">0</span>.9990277777777777<span class="o">}</span>
Best<span class="w"> </span>roc_auc:<span class="w"> </span><span class="m">0</span>.999
</pre></div>
</div>
</section>
<section id="smote-distribution-of-y-values-after-resampling">
<h4>SMOTE: Distribution of y values after resampling<a class="headerlink" href="#smote-distribution-of-y-values-after-resampling" title="Link to this heading"></a></h4>
<p>Notice that the target has been redistributed after SMOTE to 540 observations
for the minority class and 540 observations for the majority class.</p>
</section>
<section id="fit-the-model">
<h4>Fit The Model<a class="headerlink" href="#fit-the-model" title="Link to this heading"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xgb_smote</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
   <span class="n">X_train</span><span class="p">,</span>
   <span class="n">y_train</span><span class="p">,</span>
   <span class="n">validation_data</span><span class="o">=</span><span class="p">[</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="return-metrics-optional">
<h4>Return Metrics (Optional)<a class="headerlink" href="#return-metrics-optional" title="Link to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Validation<span class="w"> </span>Metrics
Confusion<span class="w"> </span>matrix<span class="w"> </span>on<span class="w"> </span><span class="nb">set</span><span class="w"> </span>provided:
--------------------------------------------------------------------------------
<span class="w">         </span>Predicted:
<span class="w">             </span>Pos<span class="w">   </span>Neg
--------------------------------------------------------------------------------
Actual:<span class="w"> </span>Pos<span class="w">  </span><span class="m">20</span><span class="w"> </span><span class="o">(</span>tp<span class="o">)</span><span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="o">(</span>fn<span class="o">)</span>
<span class="w">        </span>Neg<span class="w">   </span><span class="m">6</span><span class="w"> </span><span class="o">(</span>fp<span class="o">)</span><span class="w">  </span><span class="m">174</span><span class="w"> </span><span class="o">(</span>tn<span class="o">)</span>
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
<span class="o">{</span><span class="s1">&#39;AUC ROC&#39;</span>:<span class="w"> </span><span class="m">0</span>.9955555555555555,
<span class="s1">&#39;Average Precision&#39;</span>:<span class="w"> </span><span class="m">0</span>.9378696741854636,
<span class="s1">&#39;Brier Score&#39;</span>:<span class="w"> </span><span class="m">0</span>.20835571676988004,
<span class="s1">&#39;Precision/PPV&#39;</span>:<span class="w"> </span><span class="m">0</span>.7692307692307693,
<span class="s1">&#39;Sensitivity&#39;</span>:<span class="w"> </span><span class="m">1</span>.0,
<span class="s1">&#39;Specificity&#39;</span>:<span class="w"> </span><span class="m">0</span>.9666666666666667<span class="o">}</span>
--------------------------------------------------------------------------------

<span class="w">            </span>precision<span class="w">   </span>recall<span class="w">  </span>f1-score<span class="w">   </span>support

<span class="w">           </span><span class="m">0</span><span class="w">     </span><span class="m">1</span>.00<span class="w">     </span><span class="m">0</span>.97<span class="w">      </span><span class="m">0</span>.98<span class="w">       </span><span class="m">180</span>
<span class="w">           </span><span class="m">1</span><span class="w">     </span><span class="m">0</span>.77<span class="w">     </span><span class="m">1</span>.00<span class="w">      </span><span class="m">0</span>.87<span class="w">        </span><span class="m">20</span>

<span class="w">    </span>accuracy<span class="w">                        </span><span class="m">0</span>.97<span class="w">       </span><span class="m">200</span>
<span class="w">   </span>macro<span class="w"> </span>avg<span class="w">     </span><span class="m">0</span>.88<span class="w">     </span><span class="m">0</span>.98<span class="w">      </span><span class="m">0</span>.93<span class="w">       </span><span class="m">200</span>
weighted<span class="w"> </span>avg<span class="w">     </span><span class="m">0</span>.98<span class="w">     </span><span class="m">0</span>.97<span class="w">      </span><span class="m">0</span>.97<span class="w">       </span><span class="m">200</span>

--------------------------------------------------------------------------------

Test<span class="w"> </span>Metrics
Confusion<span class="w"> </span>matrix<span class="w"> </span>on<span class="w"> </span><span class="nb">set</span><span class="w"> </span>provided:
--------------------------------------------------------------------------------
<span class="w">         </span>Predicted:
<span class="w">             </span>Pos<span class="w">   </span>Neg
--------------------------------------------------------------------------------
Actual:<span class="w"> </span>Pos<span class="w">  </span><span class="m">19</span><span class="w"> </span><span class="o">(</span>tp<span class="o">)</span><span class="w">    </span><span class="m">1</span><span class="w"> </span><span class="o">(</span>fn<span class="o">)</span>
<span class="w">        </span>Neg<span class="w">   </span><span class="m">3</span><span class="w"> </span><span class="o">(</span>fp<span class="o">)</span><span class="w">  </span><span class="m">177</span><span class="w"> </span><span class="o">(</span>tn<span class="o">)</span>
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
<span class="o">{</span><span class="s1">&#39;AUC ROC&#39;</span>:<span class="w"> </span><span class="m">0</span>.9945833333333333,
<span class="s1">&#39;Average Precision&#39;</span>:<span class="w"> </span><span class="m">0</span>.9334649122807017,
<span class="s1">&#39;Brier Score&#39;</span>:<span class="w"> </span><span class="m">0</span>.20820269480995568,
<span class="s1">&#39;Precision/PPV&#39;</span>:<span class="w"> </span><span class="m">0</span>.8636363636363636,
<span class="s1">&#39;Sensitivity&#39;</span>:<span class="w"> </span><span class="m">0</span>.95,
<span class="s1">&#39;Specificity&#39;</span>:<span class="w"> </span><span class="m">0</span>.9833333333333333<span class="o">}</span>
--------------------------------------------------------------------------------

<span class="w">            </span>precision<span class="w">    </span>recall<span class="w">  </span>f1-score<span class="w">   </span>support

<span class="w">           </span><span class="m">0</span><span class="w">     </span><span class="m">0</span>.99<span class="w">      </span><span class="m">0</span>.98<span class="w">      </span><span class="m">0</span>.99<span class="w">       </span><span class="m">180</span>
<span class="w">           </span><span class="m">1</span><span class="w">     </span><span class="m">0</span>.86<span class="w">      </span><span class="m">0</span>.95<span class="w">      </span><span class="m">0</span>.90<span class="w">        </span><span class="m">20</span>

<span class="w">    </span>accuracy<span class="w">                         </span><span class="m">0</span>.98<span class="w">       </span><span class="m">200</span>
<span class="w">   </span>macro<span class="w"> </span>avg<span class="w">     </span><span class="m">0</span>.93<span class="w">      </span><span class="m">0</span>.97<span class="w">      </span><span class="m">0</span>.95<span class="w">       </span><span class="m">200</span>
weighted<span class="w"> </span>avg<span class="w">     </span><span class="m">0</span>.98<span class="w">      </span><span class="m">0</span>.98<span class="w">      </span><span class="m">0</span>.98<span class="w">       </span><span class="m">200</span>

--------------------------------------------------------------------------------
</pre></div>
</div>
</section>
</section>
</section>
<section id="shap-shapley-additive-explanations">
<h2>SHAP (SHapley Additive exPlanations)<a class="headerlink" href="#shap-shapley-additive-explanations" title="Link to this heading"></a></h2>
<p>This example demonstrates how to compute and visualize SHAP (SHapley Additive exPlanations)
values for a machine learning model with a pipeline that includes feature selection.
SHAP values provide insights into how individual features contribute to the predictions of a model.</p>
<p><strong>Steps</strong></p>
<ol class="arabic simple">
<li><p>The dataset is transformed through the model’s feature selection pipeline to ensure only the selected features are used for SHAP analysis.</p></li>
<li><p>The final model (e.g., <code class="docutils literal notranslate"><span class="pre">XGBoost</span></code> classifier) is retrieved from the custom Model object. This is required because SHAP operates on the underlying model, not the pipeline.</p></li>
<li><p>SHAP’s <code class="docutils literal notranslate"><span class="pre">TreeExplainer</span></code> is used to explain the predictions of the XGBoost classifier.</p></li>
<li><p>SHAP values are calculated for the transformed dataset to quantify the contribution of each feature to the predictions.</p></li>
<li><p>A summary plot is generated to visualize the impact of each feature across all data points.</p></li>
</ol>
<section id="step-1-transform-the-test-data-using-the-feature-selection-pipeline">
<h3>Step 1: Transform the test data using the feature selection pipeline<a class="headerlink" href="#step-1-transform-the-test-data-using-the-feature-selection-pipeline" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## The pipeline applies preprocessing (e.g., imputation, scaling) and feature</span>
<span class="c1">## selection (RFE) to X_test</span>
<span class="n">X_test_transformed</span> <span class="o">=</span> <span class="n">model_xgb</span><span class="o">.</span><span class="n">get_feature_selection_pipeline</span><span class="p">()</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-2-retrieve-the-trained-xgboost-classifier-from-the-pipeline">
<h3>Step 2: Retrieve the trained XGBoost classifier from the pipeline<a class="headerlink" href="#step-2-retrieve-the-trained-xgboost-classifier-from-the-pipeline" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## The last estimator in the pipeline is the XGBoost model</span>
<span class="n">xgb_classifier</span> <span class="o">=</span> <span class="n">model_xgb</span><span class="o">.</span><span class="n">estimator</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="step-3-extract-feature-names-from-the-training-data-and-initialize-the-shap-explainer-for-the-xgboost-classifier">
<h3>Step 3: Extract feature names from the training data, and initialize the SHAP explainer for the XGBoost classifier<a class="headerlink" href="#step-3-extract-feature-names-from-the-training-data-and-initialize-the-shap-explainer-for-the-xgboost-classifier" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## Import SHAP for model explainability</span>
<span class="kn">import</span> <span class="nn">shap</span>

<span class="c1">## Feature names are required for interpretability in SHAP plots</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>

<span class="c1">## Initialize the SHAP explainer with the model</span>
<span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">xgb_classifier</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-4-compute-shap-values-for-the-transformed-test-dataset-and-generate-a-summary-plot-of-shap-values">
<h3>Step 4: Compute SHAP values for the transformed test dataset and generate a summary plot of SHAP values<a class="headerlink" href="#step-4-compute-shap-values-for-the-transformed-test-dataset-and-generate-a-summary-plot-of-shap-values" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## Compute SHAP values for the transformed dataset</span>
<span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="o">.</span><span class="n">shap_values</span><span class="p">(</span><span class="n">X_test_transformed</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-5-generate-a-summary-plot-of-shap-values">
<h3>Step 5: Generate a summary plot of SHAP values<a class="headerlink" href="#step-5-generate-a-summary-plot-of-shap-values" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## Plot SHAP values</span>
<span class="c1">## Summary plot of SHAP values for all features across all data points</span>
<span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">X_test_transformed</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,)</span>
</pre></div>
</div>
<div class="no-click"><a class="reference internal image-reference" href="_images/shap_summary_plot.png"><img alt="Calibration Curve AIDs" class="align-center" src="_images/shap_summary_plot.png" style="width: 600px;" />
</a>
</div><div style="height: 50px;"></div></section>
<section id="feature-importance-and-impact">
<h3>Feature Importance and Impact<a class="headerlink" href="#feature-importance-and-impact" title="Link to this heading"></a></h3>
<p>This SHAP summary plot provides a detailed visualization of how each feature
contributes to the model’s predictions, offering insight into feature importance
and their directional effects. The X-axis represents SHAP values, which quantify
the magnitude and direction of a feature’s influence. Positive SHAP values
indicate that the feature increases the predicted output, while negative values
suggest a decrease. Along the Y-axis, features are ranked by their overall importance,
with the most influential features, such as <code class="docutils literal notranslate"><span class="pre">time</span></code>, positioned at the top.</p>
<p>Each point on the plot corresponds to an individual observation, where the color
gradient reflects the feature value. Blue points represent lower feature values,
while pink points indicate higher values, allowing us to observe how varying
feature values affect the prediction. For example, the time feature shows a wide
range of SHAP values, with higher values (pink) strongly increasing the prediction
and lower values (blue) reducing it, demonstrating its critical role in driving
the model’s output.</p>
<p>In contrast, features like <code class="docutils literal notranslate"><span class="pre">hemo</span></code> and <code class="docutils literal notranslate"><span class="pre">age</span></code> exhibit SHAP values closer to zero,
signifying a lower overall impact on predictions. Features such as <code class="docutils literal notranslate"><span class="pre">homo</span></code>, <code class="docutils literal notranslate"><span class="pre">karnof</span></code>,
and <code class="docutils literal notranslate"><span class="pre">trt</span></code> show more variability in their influence, indicating that their effect is
context-dependent and can significantly shift predictions in certain cases. This
plot provides a holistic view of feature behavior, enabling a deeper understanding
of the model’s decision-making process.</p>
</section>
</section>
</section>
<section id="regression">
<span id="id1"></span><h1>Regression<a class="headerlink" href="#regression" title="Link to this heading"></a></h1>
<p>Here is an example of using the <code class="docutils literal notranslate"><span class="pre">Model</span></code> class for regression using <code class="docutils literal notranslate"><span class="pre">XGBoost</span></code> on the California Housing dataset.</p>
<section id="california-housing-with-xgboost">
<h2>California Housing with XGBoost<a class="headerlink" href="#california-housing-with-xgboost" title="Link to this heading"></a></h2>
<section id="id2">
<h3>Step 1: Import Necessary Libraries<a class="headerlink" href="#id2" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
<span class="kn">from</span> <span class="nn">model_tuner</span> <span class="kn">import</span> <span class="n">Model</span>
</pre></div>
</div>
</section>
<section id="step-2-load-the-dataset">
<h3>Step 2: Load the Dataset<a class="headerlink" href="#step-2-load-the-dataset" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the California Housing dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-3-create-an-instance-of-the-xgbregressor">
<h3>Step 3: Create an Instance of the XGBRegressor<a class="headerlink" href="#step-3-create-an-instance-of-the-xgbregressor" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xgb_name</span> <span class="o">=</span> <span class="s2">&quot;xgb&quot;</span>
<span class="n">xgb</span> <span class="o">=</span> <span class="n">XGBRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">222</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-4-define-hyperparameters-for-xgboost">
<h3>Step 4: Define Hyperparameters for XGBoost<a class="headerlink" href="#step-4-define-hyperparameters-for-xgboost" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tuned_parameters_xgb</span> <span class="o">=</span> <span class="p">[</span>
   <span class="p">{</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__learning_rate&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">],</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__n_estimators&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">],</span>  <span class="c1"># Number of trees.</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__max_depth&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">][:</span><span class="mi">1</span><span class="p">],</span>    <span class="c1"># Maximum depth of the trees</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__subsample&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">][:</span><span class="mi">1</span><span class="p">],</span>   <span class="c1"># Subsample ratio of the</span>
                                                   <span class="c1"># training instances</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__colsample_bytree&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">][:</span><span class="mi">1</span><span class="p">],</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__eval_metric&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;logloss&quot;</span><span class="p">],</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__early_stopping_rounds&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">],</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__tree_method&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;hist&quot;</span><span class="p">],</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">xgb_name</span><span class="si">}</span><span class="s2">__verbose&quot;</span><span class="p">:</span> <span class="p">[</span><span class="kc">False</span><span class="p">],</span>
   <span class="p">}</span>
<span class="p">]</span>

<span class="n">xgb_definition</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s2">&quot;clc&quot;</span><span class="p">:</span> <span class="n">xgb</span><span class="p">,</span>
   <span class="s2">&quot;estimator_name&quot;</span><span class="p">:</span> <span class="n">xgb_name</span><span class="p">,</span>
   <span class="s2">&quot;tuned_parameters&quot;</span><span class="p">:</span> <span class="n">tuned_parameters_xgb</span><span class="p">,</span>
   <span class="s2">&quot;randomized_grid&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
   <span class="s2">&quot;early&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">model_definition</span> <span class="o">=</span> <span class="p">{</span><span class="n">xgb_name</span><span class="p">:</span> <span class="n">xgb_definition</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="step-5-initialize-and-configure-the-model">
<h3>Step 5: Initialize and Configure the <code class="docutils literal notranslate"><span class="pre">Model</span></code><a class="headerlink" href="#step-5-initialize-and-configure-the-model" title="Link to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">XGBRegressor</span></code> inherently handles missing values (<code class="docutils literal notranslate"><span class="pre">NaN</span></code>) without requiring explicit
imputation strategies. During training, <code class="docutils literal notranslate"><span class="pre">XGBoost</span></code> treats missing values as a
separate category and learns how to route them within its decision trees.
Therefore, passing a <code class="docutils literal notranslate"><span class="pre">SimpleImputer</span></code> or using an imputation strategy is unnecessary
when using <code class="docutils literal notranslate"><span class="pre">XGBRegressor</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">kfold</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">calibrate</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Define model object</span>
<span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;xgb&quot;</span>
<span class="n">clc</span> <span class="o">=</span> <span class="n">model_definition</span><span class="p">[</span><span class="n">model_type</span><span class="p">][</span><span class="s2">&quot;clc&quot;</span><span class="p">]</span>
<span class="n">estimator_name</span> <span class="o">=</span> <span class="n">model_definition</span><span class="p">[</span><span class="n">model_type</span><span class="p">][</span><span class="s2">&quot;estimator_name&quot;</span><span class="p">]</span>

<span class="c1"># Set the parameters by cross-validation</span>
<span class="n">tuned_parameters</span> <span class="o">=</span> <span class="n">model_definition</span><span class="p">[</span><span class="n">model_type</span><span class="p">][</span><span class="s2">&quot;tuned_parameters&quot;</span><span class="p">]</span>
<span class="n">rand_grid</span> <span class="o">=</span> <span class="n">model_definition</span><span class="p">[</span><span class="n">model_type</span><span class="p">][</span><span class="s2">&quot;randomized_grid&quot;</span><span class="p">]</span>
<span class="n">early_stop</span> <span class="o">=</span> <span class="n">model_definition</span><span class="p">[</span><span class="n">model_type</span><span class="p">][</span><span class="s2">&quot;early&quot;</span><span class="p">]</span>

<span class="n">model_xgb</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
   <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;xgb_</span><span class="si">{</span><span class="n">model_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
   <span class="n">estimator_name</span><span class="o">=</span><span class="n">estimator_name</span><span class="p">,</span>
   <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;regression&quot;</span><span class="p">,</span>
   <span class="n">calibrate</span><span class="o">=</span><span class="n">calibrate</span><span class="p">,</span>
   <span class="n">estimator</span><span class="o">=</span><span class="n">clc</span><span class="p">,</span>
   <span class="n">kfold</span><span class="o">=</span><span class="n">kfold</span><span class="p">,</span>
   <span class="n">stratify_y</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
   <span class="n">grid</span><span class="o">=</span><span class="n">tuned_parameters</span><span class="p">,</span>
   <span class="n">randomized_grid</span><span class="o">=</span><span class="n">rand_grid</span><span class="p">,</span>
   <span class="n">boost_early</span><span class="o">=</span><span class="n">early_stop</span><span class="p">,</span>
   <span class="n">scoring</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;r2&quot;</span><span class="p">],</span>
   <span class="n">random_state</span><span class="o">=</span><span class="mi">222</span><span class="p">,</span>
   <span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-6-perform-grid-search-parameter-tuning-and-retrieve-split-data">
<h3>Step 6: Perform Grid Search Parameter Tuning and Retrieve Split Data<a class="headerlink" href="#step-6-perform-grid-search-parameter-tuning-and-retrieve-split-data" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model_xgb</span><span class="o">.</span><span class="n">grid_search_param_tuning</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">model_xgb</span><span class="o">.</span><span class="n">get_train_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_xgb</span><span class="o">.</span><span class="n">get_test_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">model_xgb</span><span class="o">.</span><span class="n">get_valid_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Pipeline<span class="w"> </span>Steps:

┌────────────────┐
│<span class="w"> </span>Step<span class="w"> </span><span class="m">1</span>:<span class="w"> </span>xgb<span class="w">    </span>│
│<span class="w"> </span>XGBRegressor<span class="w">   </span>│
└────────────────┘

<span class="m">100</span>%<span class="p">|</span>██████████<span class="p">|</span><span class="w"> </span><span class="m">9</span>/9<span class="w"> </span><span class="o">[</span><span class="m">00</span>:22&lt;<span class="m">00</span>:00,<span class="w">  </span><span class="m">2</span>.45s/it<span class="o">]</span>Best<span class="w"> </span>score/param<span class="w"> </span><span class="nb">set</span><span class="w"> </span>found<span class="w"> </span>on<span class="w"> </span>validation<span class="w"> </span>set:
<span class="o">{</span><span class="s1">&#39;params&#39;</span>:<span class="w"> </span><span class="o">{</span><span class="s1">&#39;xgb__colsample_bytree&#39;</span>:<span class="w"> </span><span class="m">0</span>.8,
<span class="w">            </span><span class="s1">&#39;xgb__early_stopping_rounds&#39;</span>:<span class="w"> </span><span class="m">10</span>,
<span class="w">            </span><span class="s1">&#39;xgb__eval_metric&#39;</span>:<span class="w"> </span><span class="s1">&#39;logloss&#39;</span>,
<span class="w">            </span><span class="s1">&#39;xgb__learning_rate&#39;</span>:<span class="w"> </span><span class="m">0</span>.1,
<span class="w">            </span><span class="s1">&#39;xgb__max_depth&#39;</span>:<span class="w"> </span><span class="m">3</span>,
<span class="w">            </span><span class="s1">&#39;xgb__n_estimators&#39;</span>:<span class="w"> </span><span class="m">67</span>,
<span class="w">            </span><span class="s1">&#39;xgb__subsample&#39;</span>:<span class="w"> </span><span class="m">0</span>.8,
<span class="w">            </span><span class="s1">&#39;xgb__tree_method&#39;</span>:<span class="w"> </span><span class="s1">&#39;hist&#39;</span><span class="o">}</span>,
<span class="s1">&#39;score&#39;</span>:<span class="w"> </span><span class="m">0</span>.7651490279157868<span class="o">}</span>
Best<span class="w"> </span>r2:<span class="w"> </span><span class="m">0</span>.765
</pre></div>
</div>
</section>
<section id="step-7-fit-the-model">
<h3>Step 7: Fit the Model<a class="headerlink" href="#step-7-fit-the-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model_xgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
   <span class="n">X_train</span><span class="p">,</span>
   <span class="n">y_train</span><span class="p">,</span>
   <span class="n">validation_data</span><span class="o">=</span><span class="p">[</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-8-return-metrics-optional">
<h3>Step 8: Return Metrics (Optional)<a class="headerlink" href="#step-8-return-metrics-optional" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Validation</span> <span class="n">Metrics</span>
<span class="o">********************************************************************************</span>
<span class="p">{</span><span class="s1">&#39;Explained Variance&#39;</span><span class="p">:</span> <span class="mf">0.7647451659057567</span><span class="p">,</span>
<span class="s1">&#39;Mean Absolute Error&#39;</span><span class="p">:</span> <span class="mf">0.3830825326824073</span><span class="p">,</span>
<span class="s1">&#39;Mean Squared Error&#39;</span><span class="p">:</span> <span class="mf">0.3066172248224347</span><span class="p">,</span>
<span class="s1">&#39;Median Absolute Error&#39;</span><span class="p">:</span> <span class="mf">0.2672762813568116</span><span class="p">,</span>
<span class="s1">&#39;R2&#39;</span><span class="p">:</span> <span class="mf">0.7647433075624044</span><span class="p">,</span>
<span class="s1">&#39;RMSE&#39;</span><span class="p">:</span> <span class="mf">0.5537302816556403</span><span class="p">}</span>
<span class="o">********************************************************************************</span>
<span class="n">Test</span> <span class="n">Metrics</span>
<span class="o">********************************************************************************</span>
<span class="p">{</span><span class="s1">&#39;Explained Variance&#39;</span><span class="p">:</span> <span class="mf">0.7888942913974833</span><span class="p">,</span>
<span class="s1">&#39;Mean Absolute Error&#39;</span><span class="p">:</span> <span class="mf">0.3743548199982513</span><span class="p">,</span>
<span class="s1">&#39;Mean Squared Error&#39;</span><span class="p">:</span> <span class="mf">0.28411432705731066</span><span class="p">,</span>
<span class="s1">&#39;Median Absolute Error&#39;</span><span class="p">:</span> <span class="mf">0.26315186452865597</span><span class="p">,</span>
<span class="s1">&#39;R2&#39;</span><span class="p">:</span> <span class="mf">0.7888925135381788</span><span class="p">,</span>
<span class="s1">&#39;RMSE&#39;</span><span class="p">:</span> <span class="mf">0.533023758436067</span><span class="p">}</span>
<span class="o">********************************************************************************</span>
<span class="p">{</span><span class="s1">&#39;Explained Variance&#39;</span><span class="p">:</span> <span class="mf">0.7888942913974833</span><span class="p">,</span>
<span class="s1">&#39;R2&#39;</span><span class="p">:</span> <span class="mf">0.7888925135381788</span><span class="p">,</span>
<span class="s1">&#39;Mean Absolute Error&#39;</span><span class="p">:</span> <span class="mf">0.3743548199982513</span><span class="p">,</span>
<span class="s1">&#39;Median Absolute Error&#39;</span><span class="p">:</span> <span class="mf">0.26315186452865597</span><span class="p">,</span>
<span class="s1">&#39;Mean Squared Error&#39;</span><span class="p">:</span> <span class="mf">0.28411432705731066</span><span class="p">,</span>
<span class="s1">&#39;RMSE&#39;</span><span class="p">:</span> <span class="mf">0.533023758436067</span><span class="p">}</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="bootstrap-metrics">
<h1>Bootstrap Metrics<a class="headerlink" href="#bootstrap-metrics" title="Link to this heading"></a></h1>
<p>The <code class="docutils literal notranslate"><span class="pre">bootstrapper.py</span></code> module provides utility functions for input type checking, data resampling, and evaluating bootstrap metrics.</p>
<dl class="py function">
<dt class="sig sig-object py" id="check_input_type">
<span class="sig-name descname"><span class="pre">check_input_type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#check_input_type" title="Link to this definition"></a></dt>
<dd><p>Validates and normalizes the input type for data processing. Converts NumPy arrays, Pandas Series, and DataFrames into a standard Pandas DataFrame with a reset index.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>array-like</em>) – Input data (NumPy array, Pandas Series, or DataFrame).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Normalized input as a Pandas DataFrame.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pandas.DataFrame</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If the input type is not supported.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sampling_method">
<span class="sig-name descname"><span class="pre">sampling_method</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stratify</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">balance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_proportions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sampling_method" title="Link to this definition"></a></dt>
<dd><p>Resamples a dataset based on specified options for balancing, stratification, or custom class proportions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y</strong> (<em>pandas.Series</em>) – Target variable to resample.</p></li>
<li><p><strong>n_samples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of samples to draw.</p></li>
<li><p><strong>stratify</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to stratify based on the provided target variable.</p></li>
<li><p><strong>balance</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to balance class distributions equally.</p></li>
<li><p><strong>class_proportions</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a><em>, </em><em>optional</em>) – Custom proportions for each class. Must sum to 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Resampled target variable.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pandas.DataFrame</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If class proportions do not sum to 1.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="evaluate_bootstrap_metrics">
<span class="sig-name descname"><span class="pre">evaluate_bootstrap_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred_prob</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_resamples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['roc_auc',</span> <span class="pre">'f1_weighted',</span> <span class="pre">'average_precision']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">42</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stratify</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">balance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_proportions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#evaluate_bootstrap_metrics" title="Link to this definition"></a></dt>
<dd><p>Evaluates classification or regression metrics on bootstrap samples using a pre-trained model or pre-computed predictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><em>object</em></a><em>, </em><em>optional</em>) – Pre-trained model with a <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> method. Required if <code class="docutils literal notranslate"><span class="pre">y_pred_prob</span></code> is not provided.</p></li>
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>optional</em>) – Input features. Not required if <code class="docutils literal notranslate"><span class="pre">y_pred_prob</span></code> is provided.</p></li>
<li><p><strong>y</strong> (<em>array-like</em>) – Ground truth labels.</p></li>
<li><p><strong>y_pred_prob</strong> (<em>array-like</em><em>, </em><em>optional</em>) – Pre-computed predicted probabilities.</p></li>
<li><p><strong>n_samples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of samples per bootstrap iteration. Default is 500.</p></li>
<li><p><strong>num_resamples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of bootstrap iterations. Default is 1000.</p></li>
<li><p><strong>metrics</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – List of metrics to calculate (e.g., <code class="docutils literal notranslate"><span class="pre">&quot;roc_auc&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;f1_weighted&quot;</span></code>).</p></li>
<li><p><strong>random_state</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Random seed for reproducibility. Default is 42.</p></li>
<li><p><strong>threshold</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Classification threshold for probability predictions. Default is 0.5.</p></li>
<li><p><strong>model_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – Specifies the task type, either <code class="docutils literal notranslate"><span class="pre">&quot;classification&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;regression&quot;</span></code>.</p></li>
<li><p><strong>stratify</strong> (<em>pandas.Series</em><em>, </em><em>optional</em>) – Variable for stratified sampling.</p></li>
<li><p><strong>balance</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to balance class distributions.</p></li>
<li><p><strong>class_proportions</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a><em>, </em><em>optional</em>) – Custom class proportions for sampling.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>DataFrame with mean and confidence intervals for each metric.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pandas.DataFrame</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If invalid parameters or metrics are provided.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – If sample size is insufficient for metric calculation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">model_tuner_utils.py</span></code> module includes utility functions for evaluating bootstrap metrics in the context of model tuning.</p>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="return_bootstrap_metrics">
<span class="sig-name descname"><span class="pre">return_bootstrap_metrics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X_test</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_test</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_resamples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">balance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#return_bootstrap_metrics" title="Link to this definition"></a></dt>
<dd><p>Evaluates bootstrap metrics for a trained model using the test dataset. This function supports both classification and regression tasks by leveraging <cite>evaluate_bootstrap_metrics</cite> to compute confidence intervals for the specified metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_test</strong> (<em>pandas.DataFrame</em>) – Test dataset features.</p></li>
<li><p><strong>y_test</strong> (<em>pandas.Series</em><em> or </em><em>pandas.DataFrame</em>) – Test dataset labels.</p></li>
<li><p><strong>metrics</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – List of metric names to calculate (e.g., <code class="docutils literal notranslate"><span class="pre">&quot;roc_auc&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;f1_weighted&quot;</span></code>).</p></li>
<li><p><strong>threshold</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Threshold for converting predicted probabilities into class predictions. Default is 0.5.</p></li>
<li><p><strong>num_resamples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of bootstrap iterations. Default is 500.</p></li>
<li><p><strong>n_samples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of samples per bootstrap iteration. Default is 500.</p></li>
<li><p><strong>balance</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to balance the class distribution during resampling. Default is False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>DataFrame containing mean and confidence intervals for the specified metrics.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pandas.DataFrame</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If <code class="docutils literal notranslate"><span class="pre">X_test</span></code> or <code class="docutils literal notranslate"><span class="pre">y_test</span></code> are not provided as Pandas DataFrames or if unsupported input types are specified.</p>
</dd>
</dl>
</dd></dl>

<section id="bootstrap-metrics-example">
<h2>Bootstrap Metrics Example<a class="headerlink" href="#bootstrap-metrics-example" title="Link to this heading"></a></h2>
<p>Continuing from the model output object (<code class="docutils literal notranslate"><span class="pre">model_xgb</span></code>) from the <a class="reference internal" href="#regression"><span class="std std-ref">regression example</span></a> above, we leverage the <code class="docutils literal notranslate"><span class="pre">return_bootstrap_metrics</span></code> method from <code class="docutils literal notranslate"><span class="pre">model_tuner_utils.py</span></code> to print bootstrap performance metrics (<span class="math notranslate nohighlight">\(R^2\)</span> and <span class="math notranslate nohighlight">\(\text{explained variance}\)</span>) at 95% confidence levels as shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bootstrap Metrics&quot;</span><span class="p">)</span>

<span class="n">model_xgb</span><span class="o">.</span><span class="n">return_bootstrap_metrics</span><span class="p">(</span>
   <span class="n">X_test</span><span class="o">=</span><span class="n">X_test</span><span class="p">,</span>
   <span class="n">y_test</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
   <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;r2&quot;</span><span class="p">,</span> <span class="s2">&quot;explained_variance&quot;</span><span class="p">],</span>
   <span class="n">n_samples</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
   <span class="n">num_resamples</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Bootstrap<span class="w"> </span>Metrics
<span class="m">100</span>%<span class="p">|</span>██████████<span class="p">|</span><span class="w"> </span><span class="m">300</span>/300<span class="w"> </span><span class="o">[</span><span class="m">00</span>:00&lt;<span class="m">00</span>:00,<span class="w"> </span><span class="m">358</span>.05it/s<span class="o">]</span>
Metric<span class="w">                     </span>Mean<span class="w">  </span><span class="m">95</span>%<span class="w"> </span>CI<span class="w"> </span>Lower<span class="w">  </span><span class="m">95</span>%<span class="w"> </span>CI<span class="w"> </span>Upper
<span class="m">0</span><span class="w">                 </span>r2<span class="w">   </span><span class="m">0</span>.781523<span class="w">      </span><span class="m">0</span>.770853<span class="w">      </span><span class="m">0</span>.792193
<span class="m">1</span><span class="w"> </span>explained_variance<span class="w">   </span><span class="m">0</span>.788341<span class="w">      </span><span class="m">0</span>.777898<span class="w">      </span><span class="m">0</span>.798785
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="getting_started.html" class="btn btn-neutral float-left" title="Welcome to Model Tuner’s Documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="caveats.html" class="btn btn-neutral float-right" title="Zero Variance Columns" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, UCLA CTSI ML Team: Leonid Shpaner, Arthur Funnell, Panayiotis Petousis.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>