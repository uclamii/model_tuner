<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>iPython Notebooks &mdash; Model Tuner 0.0.11a0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=f712845e" />

  
    <link rel="canonical" href="https://uclamii.github.io/model_tuner/usage_guide.html" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=ff605d92"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="_static/copybutton.js?v=f281be69"></script>
        <script src="_static/custom.js?v=59429b38"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="GitHub Repository" href="about.html" />
    <link rel="prev" title="Welcome to Model Tuner’s Documentation!" href="getting_started.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Model Tuner
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Welcome to Model Tuner’s Documentation!</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Usage Guide</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">iPython Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="#key-methods-and-functionalities">Key Methods and Functionalities</a></li>
<li class="toctree-l1"><a class="reference internal" href="#helper-functions">Helper Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="#input-parameters">Input Parameters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Model"><code class="docutils literal notranslate"><span class="pre">Model()</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#caveats">Caveats</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#zero-variance-columns">Zero Variance Columns</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#effects-on-model-training">Effects on Model Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dependent-variable">Dependent Variable</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#target-variable-shape-and-its-effects">Target Variable Shape and Its Effects</a></li>
<li class="toctree-l3"><a class="reference internal" href="#solution">Solution</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#model-calibration">Model Calibration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#goal-of-calibration">Goal of Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#calibration-curve">Calibration Curve</a></li>
<li class="toctree-l2"><a class="reference internal" href="#brier-score">Brier Score</a></li>
<li class="toctree-l2"><a class="reference internal" href="#platt-scaling">Platt Scaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#isotonic-regression">Isotonic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#example-calibration-in-logistic-regression">Example: Calibration in Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#binary-classification">Binary Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#aids-clinical-trials-group-study">AIDS Clinical Trials Group Study</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-import-necessary-libraries">Step 1: Import Necessary Libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-load-the-dataset-define-x-y">Step 2: Load the dataset, define X, y</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-check-for-zero-variance-columns-and-drop-accordingly">Step 3: Check for zero-variance columns and drop accordingly</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-4-create-an-instance-of-the-xgbclassifier">Step 4: Create an Instance of the XGBClassifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-5-define-hyperparameters-for-xgboost">Step 5: Define Hyperparameters for XGBoost</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-6-initialize-and-configure-the-model">Step 6: Initialize and Configure the <code class="docutils literal notranslate"><span class="pre">Model</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-7-perform-grid-search-parameter-tuning">Step 7: Perform Grid Search Parameter Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-8-fit-the-model">Step 8: Fit the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-9-return-metrics-optional">Step 9: Return Metrics (Optional)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-10-calibrate-the-model-if-needed">Step 10: Calibrate the Model (if needed)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#classification-report-optional">Classification Report (Optional)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#regression">Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#california-housing-with-xgboost">California Housing with XGBoost</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Step 1: Import Necessary Libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-load-the-dataset">Step 2: Load the Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-create-an-instance-of-the-xgbclassifier">Step 3: Create an Instance of the XGBClassifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-4-define-hyperparameters-for-xgboost">Step 4: Define Hyperparameters for XGBoost</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-5-initialize-and-configure-the-model">Step 5: Initialize and Configure the <code class="docutils literal notranslate"><span class="pre">Model</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-6-fit-the-model">Step 6: Fit the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-7-return-metrics-optional">Step 7: Return Metrics (Optional)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About Model Tuner</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="about.html">GitHub Repository</a></li>
<li class="toctree-l1"><a class="reference internal" href="about.html#acknowledgements">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="about.html#citing-model-tuner">Citing Model Tuner</a></li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Model Tuner</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">iPython Notebooks</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="no-click"><a class="reference internal image-reference" href="_images/ModelTunerTarget.png"><img alt="Model Tuner Logo" class="align-left" src="_images/ModelTunerTarget.png" style="width: 250px;" /></a>
</div><div style="height: 150px;"></div><p></p>
<section id="ipython-notebooks">
<h1>iPython Notebooks<a class="headerlink" href="#ipython-notebooks" title="Link to this heading"></a></h1>
<ul class="simple">
<li><p><a class="reference external" href="https://colab.research.google.com/drive/1D9nl8rLdwxPEpiZplsU0I0lFSAec7NzP?authuser=1#scrollTo=tumIjsNpSAKC&amp;uniqifier=1">Binary Classification Example</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/drive/1ujLL2mRtIWwGamnpWKIo2f271_Q103t-?usp=sharing#scrollTo=uMxyy0yvd2xQ">Column Transformer Example</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/drive/151kdlsW-WyJ0pwwt_iWpjXDuqj1Ktam_?authuser=1#scrollTo=UhfZKVoq3sAN">Regression Example</a></p></li>
</ul>
</section>
<section id="key-methods-and-functionalities">
<h1>Key Methods and Functionalities<a class="headerlink" href="#key-methods-and-functionalities" title="Link to this heading"></a></h1>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">__init__(...)</span></code></dt><dd><p>Initializes the model_tuner with configurations such as estimator, cross-validation settings, scoring metrics, etc.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">reset_estimator()</span></code></dt><dd><p>Resets the estimator.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">process_imbalance_sampler(X_train,</span> <span class="pre">y_train)</span></code></dt><dd><p>Processes imbalance sampler.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">calibrateModel(X,</span> <span class="pre">y,</span> <span class="pre">score=None,</span> <span class="pre">stratify=None)</span></code></dt><dd><p>Calibrates the model.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">get_train_data(X,</span> <span class="pre">y),</span> <span class="pre">get_valid_data(X,</span> <span class="pre">y),</span> <span class="pre">get_test_data(X,</span> <span class="pre">y)</span></code></dt><dd><p>Retrieves train, validation, and test data.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">calibrate_report(X,</span> <span class="pre">y,</span> <span class="pre">score=None)</span></code></dt><dd><p>Generates a calibration report.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">fit(X,</span> <span class="pre">y,</span> <span class="pre">validation_data=None,</span> <span class="pre">score=None)</span></code></dt><dd><p>Fits the model to the data.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">return_metrics(X_test,</span> <span class="pre">y_test)</span></code></dt><dd><p>Returns evaluation metrics.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">predict(X,</span> <span class="pre">y=None,</span> <span class="pre">optimal_threshold=False),</span> <span class="pre">predict_proba(X,</span> <span class="pre">y=None)</span></code></dt><dd><p>Makes predictions and predicts probabilities.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">grid_search_param_tuning(X,</span> <span class="pre">y,</span> <span class="pre">f1_beta_tune=False,</span> <span class="pre">betas=[1,</span> <span class="pre">2])</span></code></dt><dd><p>Performs grid search parameter tuning.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">print_k_best_features(X)</span></code></dt><dd><p>Prints the top K best features.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">tune_threshold_Fbeta(score,</span> <span class="pre">X_train,</span> <span class="pre">y_train,</span> <span class="pre">X_valid,</span> <span class="pre">y_valid,</span> <span class="pre">betas,</span> <span class="pre">kfold=False)</span></code></dt><dd><p>Tunes the threshold for F-beta score.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">train_val_test_split(X,</span> <span class="pre">y,</span> <span class="pre">stratify_y,</span> <span class="pre">train_size,</span> <span class="pre">validation_size,</span> <span class="pre">test_size,</span> <span class="pre">random_state,</span> <span class="pre">stratify_cols,</span> <span class="pre">calibrate)</span></code></dt><dd><p>Splits the data into train, validation, and test sets.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">get_best_score_params(X,</span> <span class="pre">y)</span></code></dt><dd><p>Retrieves the best score parameters.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">conf_mat_class_kfold(X,</span> <span class="pre">y,</span> <span class="pre">test_model,</span> <span class="pre">score=None)</span></code></dt><dd><p>Generates confusion matrix for k-fold cross-validation.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">regression_report_kfold(X,</span> <span class="pre">y,</span> <span class="pre">test_model,</span> <span class="pre">score=None)</span></code></dt><dd><p>Generates regression report for k-fold cross-validation.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">regression_report(y_true,</span> <span class="pre">y_pred,</span> <span class="pre">print_results=True)</span></code></dt><dd><p>Generates a regression report.</p>
</dd>
</dl>
</section>
<section id="helper-functions">
<h1>Helper Functions<a class="headerlink" href="#helper-functions" title="Link to this heading"></a></h1>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">kfold_split(classifier,</span> <span class="pre">X,</span> <span class="pre">y,</span> <span class="pre">stratify=False,</span> <span class="pre">scoring=[&quot;roc_auc&quot;],</span> <span class="pre">n_splits=10,</span> <span class="pre">random_state=3)</span></code></dt><dd><p>Splits data using k-fold cross-validation.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">get_cross_validate(classifier,</span> <span class="pre">X,</span> <span class="pre">y,</span> <span class="pre">kf,</span> <span class="pre">stratify=False,</span> <span class="pre">scoring=[&quot;roc_auc&quot;])</span></code></dt><dd><p>Performs cross-validation.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">_confusion_matrix_print(conf_matrix,</span> <span class="pre">labels)</span></code></dt><dd><p>Prints the confusion matrix.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>This class is designed to be flexible and can be extended to include additional functionalities or custom metrics.</p></li>
<li><p>It is essential to properly configure the parameters during initialization to suit the specific requirements of your machine learning task.</p></li>
<li><p>Ensure that all dependencies are installed and properly imported before using the <code class="docutils literal notranslate"><span class="pre">Model</span></code> class from the <code class="docutils literal notranslate"><span class="pre">model_tuner</span></code> library.</p></li>
</ul>
</div>
</section>
<section id="input-parameters">
<h1>Input Parameters<a class="headerlink" href="#input-parameters" title="Link to this heading"></a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="Model">
<span class="sig-name descname"><span class="pre">Model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">calibrate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kfold</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">imbalance_sampler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stratify_y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stratify_cols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_strat_feat</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_splits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">display</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_names</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">randomized_grid</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trained</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pipeline</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaler_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">impute_strategy</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">impute</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pipeline_steps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">xgboost_early</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selectKBest</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_label</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">calibration_method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">custom_scorer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Model" title="Link to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a>) – A name for the model, useful for identifying the model in outputs and logs.</p></li>
<li><p><strong>estimator_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a>) – The prefix for the estimator used in the pipeline. This is used in parameter tuning (e.g., estimator_name + <code class="docutils literal notranslate"><span class="pre">__param_name</span></code>).</p></li>
<li><p><strong>estimator</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.12)"><em>object</em></a>) – The machine learning model to be tuned and trained.</p></li>
<li><p><strong>calibrate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to calibrate the classifier. Default is False.</p></li>
<li><p><strong>kfold</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to use k-fold cross-validation. Default is False.</p></li>
<li><p><strong>imbalance_sampler</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.12)"><em>object</em></a><em>, </em><em>optional</em>) – An imbalanced data sampler from the imblearn library, e.g., <code class="docutils literal notranslate"><span class="pre">RandomUnderSampler</span></code> or <code class="docutils literal notranslate"><span class="pre">RandomOverSampler</span></code>.</p></li>
<li><p><strong>train_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – Proportion of the data to use for training. Default is 0.6.</p></li>
<li><p><strong>validation_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – Proportion of the data to use for validation. Default is 0.2.</p></li>
<li><p><strong>test_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><em>float</em></a><em>, </em><em>optional</em>) – Proportion of the data to use for testing. Default is 0.2.</p></li>
<li><p><strong>stratify_y</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to stratify by the target variable during train/validation/test split. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>stratify_cols</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)"><em>list</em></a><em>, </em><em>optional</em>) – List of columns to stratify by during train/validation/test split. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>drop_strat_feat</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)"><em>list</em></a><em>, </em><em>optional</em>) – List of columns to drop after stratification. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>grid</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)"><em>dict</em></a>) – Hyperparameter grid for tuning.</p></li>
<li><p><strong>scoring</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a>) – Scoring metrics for evaluation.</p></li>
<li><p><strong>n_splits</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of splits for k-fold cross-validation. Default is <code class="docutils literal notranslate"><span class="pre">10</span></code>.</p></li>
<li><p><strong>random_state</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Random state for reproducibility. Default is <code class="docutils literal notranslate"><span class="pre">3</span></code>.</p></li>
<li><p><strong>n_jobs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of jobs to run in parallel for model fitting. Default is <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p><strong>display</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to display output messages during the tuning process. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>feature_names</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)"><em>list</em></a><em>, </em><em>optional</em>) – List of feature names. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>randomized_grid</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to use randomized grid search. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>n_iter</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) – Number of iterations for randomized grid search. Default is <code class="docutils literal notranslate"><span class="pre">100</span></code>.</p></li>
<li><p><strong>trained</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether the model has been trained. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>pipeline</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to use a pipeline. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>scaler_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – Type of scaler to use. Options are <code class="docutils literal notranslate"><span class="pre">min_max_scaler</span></code>, <code class="docutils literal notranslate"><span class="pre">standard_scaler</span></code>, <code class="docutils literal notranslate"><span class="pre">max_abs_scaler</span></code>, or <code class="docutils literal notranslate"><span class="pre">None</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">min_max_scaler</span></code>.</p></li>
<li><p><strong>impute_strategy</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – Strategy for imputation. Options are <code class="docutils literal notranslate"><span class="pre">mean</span></code>, <code class="docutils literal notranslate"><span class="pre">median</span></code>, <code class="docutils literal notranslate"><span class="pre">most_frequent</span></code>, or <code class="docutils literal notranslate"><span class="pre">constant</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">mean</span></code>.</p></li>
<li><p><strong>impute</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to impute missing values. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>pipeline_steps</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)"><em>list</em></a><em>, </em><em>optional</em>) – List of pipeline steps. Default is <code class="docutils literal notranslate"><span class="pre">[(min_max_scaler,</span> <span class="pre">MinMaxScaler())]</span></code>.</p></li>
<li><p><strong>xgboost_early</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to use early stopping for <code class="docutils literal notranslate"><span class="pre">XGBoost</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>selectKBest</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to select K best features. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>model_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – Type of model, either <code class="docutils literal notranslate"><span class="pre">classification</span></code> or <code class="docutils literal notranslate"><span class="pre">regression</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">classification</span></code>.</p></li>
<li><p><strong>class_labels</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)"><em>list</em></a><em>, </em><em>optional</em>) – List of class labels for multi-class classification. Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>multi_label</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether the problem is a multi-label classification problem. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>calibration_method</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em>, </em><em>optional</em>) – Method for calibration, options are <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> or <code class="docutils literal notranslate"><span class="pre">isotonic</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code>.</p></li>
<li><p><strong>custom_scorer</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)"><em>dict</em></a><em>, </em><em>optional</em>) – Custom scorers for evaluation. Default is <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ImportError" title="(in Python v3.12)"><strong>ImportError</strong></a> – If the <code class="docutils literal notranslate"><span class="pre">bootstrapper</span></code> module is not found or not installed.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)"><strong>ValueError</strong></a> – In various cases, such as when an invalid parameter is passed to Scikit-learn functions like <code class="docutils literal notranslate"><span class="pre">cross_validate</span></code>, <code class="docutils literal notranslate"><span class="pre">fit</span></code>, or <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code>, or if the shapes of <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> do not match during operations.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.12)"><strong>AttributeError</strong></a> – If an expected step in the pipeline (e.g., “imputer”, “Resampler”) is missing from <code class="docutils literal notranslate"><span class="pre">self.estimator.named_steps</span></code>, or if <code class="docutils literal notranslate"><span class="pre">self.PipelineClass</span></code> or <code class="docutils literal notranslate"><span class="pre">self.estimator</span></code> is not properly initialized.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)"><strong>TypeError</strong></a> – If an incorrect type is passed to a function or method, such as passing <code class="docutils literal notranslate"><span class="pre">None</span></code> where a numerical value or a non-NoneType object is expected.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#IndexError" title="(in Python v3.12)"><strong>IndexError</strong></a> – If the dimensions of the confusion matrix are incorrect or unexpected in <code class="docutils literal notranslate"><span class="pre">_confusion_matrix_print_ML</span></code> or <code class="docutils literal notranslate"><span class="pre">_confusion_matrix_print</span></code>.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#KeyError" title="(in Python v3.12)"><strong>KeyError</strong></a> – If a key is not found in a dictionary, such as when accessing <code class="docutils literal notranslate"><span class="pre">self.best_params_per_score</span></code> with a score that is not in the dictionary, or when accessing configuration keys in the <code class="docutils literal notranslate"><span class="pre">summarize_auto_keras_params</span></code> method.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.12)"><strong>RuntimeError</strong></a> – If there is an unexpected issue during model fitting or transformation that does not fit into the other categories of exceptions.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="caveats">
<h1>Caveats<a class="headerlink" href="#caveats" title="Link to this heading"></a></h1>
<section id="zero-variance-columns">
<h2>Zero Variance Columns<a class="headerlink" href="#zero-variance-columns" title="Link to this heading"></a></h2>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Ensure that your feature set <cite>X</cite> is free of zero-variance columns before using this method.
Zero-variance columns can lead to issues such as <code class="docutils literal notranslate"><span class="pre">UserWarning:</span> <span class="pre">Features[feat_num]</span> <span class="pre">are</span> <span class="pre">constant</span></code>
and <code class="docutils literal notranslate"><span class="pre">RuntimeWarning:</span> <span class="pre">invalid</span> <span class="pre">value</span> <span class="pre">encountered</span> <span class="pre">in</span> <span class="pre">divide</span> <span class="pre">f</span> <span class="pre">=</span> <span class="pre">msb/msw</span></code> during the model training process.</p>
<p>To check for and remove zero-variance columns, you can use the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check for zero-variance columns and drop them</span>
<span class="n">zero_variance_columns</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">var</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">zero_variance_columns</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">zero_variance_columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Zero-variance columns in the feature set <span class="math notranslate nohighlight">\(X\)</span> refer to columns where all values are identical.
Mathematically, if <span class="math notranslate nohighlight">\(X_j\)</span> is a column in <span class="math notranslate nohighlight">\(X\)</span>, the variance of this column is calculated as:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(X_j) = \frac{1}{n} \sum_{i=1}^{n} (X_{ij} - \bar{X}_j)^2 = 0\]</div>
<p>where <span class="math notranslate nohighlight">\(X_{ij}\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th observation of feature <span class="math notranslate nohighlight">\(j\)</span>, and <span class="math notranslate nohighlight">\(\bar{X}_j\)</span> is the mean of the <span class="math notranslate nohighlight">\(j\)</span>-th feature.
Since all <span class="math notranslate nohighlight">\(X_{ij}\)</span> are equal, <span class="math notranslate nohighlight">\(\text{Var}(X_j)\)</span> is zero.</p>
<section id="effects-on-model-training">
<h3>Effects on Model Training<a class="headerlink" href="#effects-on-model-training" title="Link to this heading"></a></h3>
<ol class="arabic">
<li><p><strong>UserWarning:</strong></p>
<p>During model training, algorithms often check for variability in features to determine their usefulness in predicting the target variable. A zero-variance column provides no information, leading to the following warning:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>UserWarning: Features[feat_num] are constant
</pre></div>
</div>
<p>This indicates that the feature <span class="math notranslate nohighlight">\(X_j\)</span> has no variability and, therefore, cannot contribute to the model’s predictive power.</p>
</li>
<li><p><strong>RuntimeWarning:</strong></p>
<p>When calculating metrics like the F-statistic used in Analysis of Variance (ANOVA) or feature importance metrics, the following ratio is computed:</p>
<div class="math notranslate nohighlight">
\[F = \frac{\text{MSB}}{\text{MSW}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{MSB}\)</span> (Mean Square Between) and <span class="math notranslate nohighlight">\(\text{MSW}\)</span> (Mean Square Within) are defined as:</p>
<div class="math notranslate nohighlight">
\[\text{MSB} = \frac{1}{k-1} \sum_{j=1}^{k} n_j (\bar{X}_j - \bar{X})^2\]</div>
<div class="math notranslate nohighlight">
\[\text{MSW} = \frac{1}{n-k} \sum_{j=1}^{k} \sum_{i=1}^{n_j} (X_{ij} - \bar{X}_j)^2\]</div>
<p>If <span class="math notranslate nohighlight">\(X_j\)</span> is a zero-variance column, then <span class="math notranslate nohighlight">\(\text{MSW} = 0\)</span> because all <span class="math notranslate nohighlight">\(X_{ij}\)</span> are equal to <span class="math notranslate nohighlight">\(\bar{X}_j\)</span>. This leads to a division by zero in the calculation of <span class="math notranslate nohighlight">\(F\)</span>:</p>
<div class="math notranslate nohighlight">
\[F = \frac{\text{MSB}}{0} \rightarrow \text{undefined}\]</div>
<p>which triggers a runtime warning:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>RuntimeWarning: invalid value encountered in divide f = msb/msw
</pre></div>
</div>
<p>indicating that the calculation involves dividing by zero, resulting in undefined or infinite values.</p>
</li>
</ol>
<p>To avoid these issues, ensure that zero-variance columns are removed from <span class="math notranslate nohighlight">\(X\)</span> before proceeding with model training.</p>
</section>
</section>
<section id="dependent-variable">
<h2>Dependent Variable<a class="headerlink" href="#dependent-variable" title="Link to this heading"></a></h2>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Additionally, ensure that <cite>y</cite> (the target variable) is passed as a Series and not as a DataFrame.
Passing <cite>y</cite> as a DataFrame can cause issues such as <code class="docutils literal notranslate"><span class="pre">DataConversionWarning:</span> <span class="pre">A</span> <span class="pre">column-vector</span> <span class="pre">y</span> <span class="pre">was</span> <span class="pre">passed</span>
<span class="pre">when</span> <span class="pre">a</span> <span class="pre">1d</span> <span class="pre">array</span> <span class="pre">was</span> <span class="pre">expected.</span> <span class="pre">Please</span> <span class="pre">change</span> <span class="pre">the</span> <span class="pre">shape</span> <span class="pre">of</span> <span class="pre">y</span> <span class="pre">to</span> <span class="pre">(n_samples,)</span></code>.</p>
<p>If <cite>y</cite> is a DataFrame, you can convert it to a Series using the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert y to a Series if it&#39;s a DataFrame</span>
<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</pre></div>
</div>
<p>This conversion ensures that the target variable <cite>y</cite> has the correct shape, preventing the aforementioned warning.</p>
</div>
<section id="target-variable-shape-and-its-effects">
<h3>Target Variable Shape and Its Effects<a class="headerlink" href="#target-variable-shape-and-its-effects" title="Link to this heading"></a></h3>
<p>The target variable <span class="math notranslate nohighlight">\(y\)</span> should be passed as a 1-dimensional array (Series) and not as a 2-dimensional array (DataFrame).
If <span class="math notranslate nohighlight">\(y\)</span> is passed as a DataFrame, the model training process might raise the following warning:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>DataConversionWarning: A column-vector y was passed when a 1d array was expected.
Please change the shape of y to (n_samples,).
</pre></div>
</div>
<p><strong>Explanation:</strong></p>
<p>Machine learning models generally expect the target variable <span class="math notranslate nohighlight">\(y\)</span> to be in the shape of a 1-dimensional array,
denoted as <span class="math notranslate nohighlight">\(y = \{y_1, y_2, \dots, y_n\}\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of samples.
Mathematically, <span class="math notranslate nohighlight">\(y\)</span> is represented as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}\end{split}\]</div>
<p>When <span class="math notranslate nohighlight">\(y\)</span> is passed as a DataFrame, it is treated as a 2-dimensional array, which has the form:</p>
<div class="math notranslate nohighlight">
\[y = \begin{pmatrix} y_1, y_2, \dots , y_n \end{pmatrix}\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[\begin{split}y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}\end{split}\]</div>
<p>where each sample is represented as a column vector. This discrepancy in dimensionality can cause the model to misinterpret the data,
leading to the <code class="docutils literal notranslate"><span class="pre">DataConversionWarning</span></code>.</p>
</section>
<section id="solution">
<h3>Solution<a class="headerlink" href="#solution" title="Link to this heading"></a></h3>
<p>To ensure <span class="math notranslate nohighlight">\(y\)</span> is interpreted correctly as a 1-dimensional array, it should be passed as a Series.
If <span class="math notranslate nohighlight">\(y\)</span> is currently a DataFrame, you can convert it to a Series using the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert y to a Series if it&#39;s a DataFrame</span>
<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</pre></div>
</div>
<p>The method <code class="code docutils literal notranslate"><span class="pre">squeeze()</span></code> effectively removes any unnecessary dimensions, converting a 2-dimensional DataFrame
with a single column into a 1-dimensional Series. This ensures that <span class="math notranslate nohighlight">\(y\)</span> has the correct shape, preventing
the aforementioned warning and ensuring the model processes the target variable correctly.</p>
</section>
</section>
</section>
<section id="model-calibration">
<h1>Model Calibration<a class="headerlink" href="#model-calibration" title="Link to this heading"></a></h1>
<p>Model calibration refers to the process of adjusting the predicted probabilities of a model so that they more accurately reflect the true likelihood of outcomes. This is crucial in machine learning, particularly for classification problems where the model outputs probabilities rather than just class labels.</p>
<section id="goal-of-calibration">
<h2>Goal of Calibration<a class="headerlink" href="#goal-of-calibration" title="Link to this heading"></a></h2>
<p>The goal of calibration is to ensure that the predicted probability <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> is equal to the true probability that <span class="math notranslate nohighlight">\(y = 1\)</span> given <span class="math notranslate nohighlight">\(x\)</span>. Mathematically, this can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\hat{p}(x) = P(y = 1 \mid \hat{p}(x) = p)\]</div>
<p>This equation states that for all instances where the model predicts a probability <span class="math notranslate nohighlight">\(p\)</span>, the true fraction of positive cases should also be <span class="math notranslate nohighlight">\(p\)</span>.</p>
</section>
<section id="calibration-curve">
<h2>Calibration Curve<a class="headerlink" href="#calibration-curve" title="Link to this heading"></a></h2>
<p>To assess calibration, we often use a <em>calibration curve</em>. This involves:</p>
<ol class="arabic simple">
<li><p><strong>Binning</strong> the predicted probabilities <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> into intervals (e.g., [0.0, 0.1), [0.1, 0.2), …, [0.9, 1.0]).</p></li>
<li><p><strong>Calculating the mean predicted probability</strong> <span class="math notranslate nohighlight">\(\hat{p}_i\)</span> for each bin <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><strong>Calculating the empirical frequency</strong> <span class="math notranslate nohighlight">\(f_i\)</span> (the fraction of positives) in each bin.</p></li>
</ol>
<p>For a perfectly calibrated model:</p>
<div class="math notranslate nohighlight">
\[\hat{p}_i = f_i \quad \text{for all bins } i\]</div>
</section>
<section id="brier-score">
<h2>Brier Score<a class="headerlink" href="#brier-score" title="Link to this heading"></a></h2>
<p>The <strong>Brier score</strong> is one way to measure the calibration of a model. It’s calculated as:</p>
<div class="math notranslate nohighlight">
\[\text{Brier Score} = \frac{1}{N} \sum_{i=1}^{N} (\hat{p}(x_i) - y_i)^2\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of instances.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{p}(x_i)\)</span> is the predicted probability for instance <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> is the actual label for instance <span class="math notranslate nohighlight">\(i\)</span> (0 or 1).</p></li>
</ul>
<p>The Brier score penalizes predictions that are far from the true outcome. A lower Brier score indicates better calibration and accuracy.</p>
</section>
<section id="platt-scaling">
<h2>Platt Scaling<a class="headerlink" href="#platt-scaling" title="Link to this heading"></a></h2>
<p>One common method to calibrate a model is <strong>Platt Scaling</strong>. This involves fitting a logistic regression model to the predictions of the original model. The logistic regression model adjusts the raw predictions <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> to output calibrated probabilities.</p>
<p>Mathematically, Platt scaling is expressed as:</p>
<div class="math notranslate nohighlight">
\[\hat{p}_{\text{calibrated}}(x) = \frac{1}{1 + \exp(-(A \hat{p}(x) + B))}\]</div>
<p>Where <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are parameters learned from the data. These parameters adjust the original probability estimates to better align with the true probabilities.</p>
</section>
<section id="isotonic-regression">
<h2>Isotonic Regression<a class="headerlink" href="#isotonic-regression" title="Link to this heading"></a></h2>
<p>Another method is <strong>Isotonic Regression</strong>, a non-parametric approach that fits a piecewise constant function. Unlike Platt Scaling, which assumes a logistic function, Isotonic Regression only assumes that the function is monotonically increasing. The goal is to find a set of probabilities <span class="math notranslate nohighlight">\(p_i\)</span> that are as close as possible to the true probabilities while maintaining a monotonic relationship.</p>
<p>The isotonic regression problem can be formulated as:</p>
<div class="math notranslate nohighlight">
\[\min_{p_1 \leq p_2 \leq \dots \leq p_n} \sum_{i=1}^{n} (p_i - y_i)^2\]</div>
<p>Where <span class="math notranslate nohighlight">\(p_i\)</span> are the adjusted probabilities, and the constraint ensures that the probabilities are non-decreasing.</p>
</section>
<section id="example-calibration-in-logistic-regression">
<h2>Example: Calibration in Logistic Regression<a class="headerlink" href="#example-calibration-in-logistic-regression" title="Link to this heading"></a></h2>
<p>In a standard logistic regression model, the predicted probability is given by:</p>
<div class="math notranslate nohighlight">
\[\hat{p}(x) = \sigma(w^\top x) = \frac{1}{1 + \exp(-w^\top x)}\]</div>
<p>Where <span class="math notranslate nohighlight">\(w\)</span> is the vector of weights, and <span class="math notranslate nohighlight">\(x\)</span> is the input feature vector.</p>
<p>If this model is well-calibrated, <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> should closely match the true conditional probability <span class="math notranslate nohighlight">\(P(y = 1 \mid x)\)</span>. If not, techniques like Platt Scaling or Isotonic Regression can be applied to adjust <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> to be more accurate.</p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Model calibration</strong> is about aligning predicted probabilities with actual outcomes.</p></li>
<li><p><strong>Mathematically</strong>, calibration ensures <span class="math notranslate nohighlight">\(\hat{p}(x) = P(y = 1 \mid \hat{p}(x) = p)\)</span>.</p></li>
<li><p><strong>Platt Scaling</strong> and <strong>Isotonic Regression</strong> are two common methods to achieve calibration.</p></li>
<li><p><strong>Brier Score</strong> is a metric that captures both the calibration and accuracy of probabilistic predictions.</p></li>
</ul>
<p>Calibration is essential when the probabilities output by a model need to be trusted, such as in risk assessment, medical diagnosis, and other critical applications.</p>
</section>
</section>
<section id="binary-classification">
<h1>Binary Classification<a class="headerlink" href="#binary-classification" title="Link to this heading"></a></h1>
<p>Binary classification is a type of supervised learning where a model is trained
to distinguish between two distinct classes or categories. In essence, the model
learns to classify input data into one of two possible outcomes, typically
labeled as <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>, or negative and positive. This is commonly used in
scenarios such as spam detection, disease diagnosis, or fraud detection.</p>
<p>In our library, binary classification is handled seamlessly through the <code class="docutils literal notranslate"><span class="pre">Model</span></code>
class. Users can specify a binary classifier as the estimator, and the library
takes care of essential tasks like data preprocessing, model calibration, and
cross-validation. The library also provides robust support for evaluating the
model’s performance using a variety of metrics, such as accuracy, precision,
recall, and ROC-AUC, ensuring that the model’s ability to distinguish between the
two classes is thoroughly assessed. Additionally, the library supports advanced
techniques like imbalanced data handling and model calibration to fine-tune
decision thresholds, making it easier to deploy effective binary classifiers in
real-world applications.</p>
<section id="aids-clinical-trials-group-study">
<h2>AIDS Clinical Trials Group Study<a class="headerlink" href="#aids-clinical-trials-group-study" title="Link to this heading"></a></h2>
<p>The UCI Machine Learning Repository is a well-known resource for accessing a wide
range of datasets used for machine learning research and practice. One such dataset
is the AIDS Clinical Trials Group Study dataset, which can be used to build and
evaluate predictive models.</p>
<p>You can easily fetch this dataset using the ucimlrepo package. If you haven’t
installed it yet, you can do so by running the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>ucimlrepo
</pre></div>
</div>
<p>Once installed, you can quickly load the AIDS Clinical Trials Group Study dataset
with a simple command:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ucimlrepo</span> <span class="kn">import</span> <span class="n">fetch_ucirepo</span>
</pre></div>
</div>
<section id="step-1-import-necessary-libraries">
<h3>Step 1: Import Necessary Libraries<a class="headerlink" href="#step-1-import-necessary-libraries" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
</pre></div>
</div>
</section>
<section id="step-2-load-the-dataset-define-x-y">
<h3>Step 2: Load the dataset, define X, y<a class="headerlink" href="#step-2-load-the-dataset-define-x-y" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># fetch dataset</span>
<span class="n">aids_clinical_trials_group_study_175</span> <span class="o">=</span> <span class="n">fetch_ucirepo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">890</span><span class="p">)</span>

<span class="c1"># data (as pandas dataframes)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">aids_clinical_trials_group_study_175</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">features</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">aids_clinical_trials_group_study_175</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">targets</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="c1"># convert a DataFrame to Series when single column</span>
</pre></div>
</div>
</section>
<section id="step-3-check-for-zero-variance-columns-and-drop-accordingly">
<h3>Step 3: Check for zero-variance columns and drop accordingly<a class="headerlink" href="#step-3-check-for-zero-variance-columns-and-drop-accordingly" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check for zero-variance columns and drop them</span>
<span class="n">zero_variance_columns</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">var</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">zero_variance_columns</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
   <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">zero_variance_columns</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-4-create-an-instance-of-the-xgbclassifier">
<h3>Step 4: Create an Instance of the XGBClassifier<a class="headerlink" href="#step-4-create-an-instance-of-the-xgbclassifier" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating an instance of the XGBClassifier</span>
<span class="n">xgb_model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span>
   <span class="n">random_state</span><span class="o">=</span><span class="mi">222</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-5-define-hyperparameters-for-xgboost">
<h3>Step 5: Define Hyperparameters for XGBoost<a class="headerlink" href="#step-5-define-hyperparameters-for-xgboost" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Estimator name prefix for use in GridSearchCV or similar tools</span>
<span class="n">estimator_name_xgb</span> <span class="o">=</span> <span class="s2">&quot;xgb&quot;</span>

<span class="c1"># Define the hyperparameters for XGBoost</span>
<span class="n">xgb_learning_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]</span>  <span class="c1"># Learning rate or eta</span>
<span class="n">xgb_n_estimators</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">]</span>  <span class="c1"># Number of trees. Equivalent to n_estimators in GB</span>
<span class="n">xgb_max_depths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>  <span class="c1"># Maximum depth of the trees</span>
<span class="n">xgb_subsamples</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>  <span class="c1"># Subsample ratio of the training instances</span>
<span class="n">xgb_colsample_bytree</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>

<span class="n">xgb_eval_metric</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;logloss&quot;</span><span class="p">]</span>  <span class="c1"># Check out &quot;pr_auc&quot;</span>
<span class="n">xgb_early_stopping_rounds</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">]</span>
<span class="n">xgb_verbose</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span><span class="p">]</span>  <span class="c1"># Subsample ratio of columns when constructing each tree</span>

<span class="c1"># Combining the hyperparameters in a dictionary</span>
<span class="n">xgb_parameters</span> <span class="o">=</span> <span class="p">[</span>
   <span class="p">{</span>
      <span class="s2">&quot;xgb__learning_rate&quot;</span><span class="p">:</span> <span class="n">xgb_learning_rates</span><span class="p">,</span>
      <span class="s2">&quot;xgb__n_estimators&quot;</span><span class="p">:</span> <span class="n">xgb_n_estimators</span><span class="p">,</span>
      <span class="s2">&quot;xgb__max_depth&quot;</span><span class="p">:</span> <span class="n">xgb_max_depths</span><span class="p">,</span>
      <span class="s2">&quot;xgb__subsample&quot;</span><span class="p">:</span> <span class="n">xgb_subsamples</span><span class="p">,</span>
      <span class="s2">&quot;xgb__colsample_bytree&quot;</span><span class="p">:</span> <span class="n">xgb_colsample_bytree</span><span class="p">,</span>
      <span class="s2">&quot;xgb__eval_metric&quot;</span><span class="p">:</span> <span class="n">xgb_eval_metric</span><span class="p">,</span>
      <span class="s2">&quot;xgb__early_stopping_rounds&quot;</span><span class="p">:</span> <span class="n">xgb_early_stopping_rounds</span><span class="p">,</span>
      <span class="s2">&quot;xgb__verbose&quot;</span><span class="p">:</span> <span class="n">xgb_verbose</span><span class="p">,</span>
      <span class="s2">&quot;selectKBest__k&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
   <span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
</section>
<section id="step-6-initialize-and-configure-the-model">
<h3>Step 6: Initialize and Configure the <code class="docutils literal notranslate"><span class="pre">Model</span></code><a class="headerlink" href="#step-6-initialize-and-configure-the-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize model_tuner</span>
<span class="n">model_tuner</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
   <span class="n">name</span><span class="o">=</span><span class="s2">&quot;XGBoost_AIDS&quot;</span><span class="p">,</span>
   <span class="n">estimator_name</span><span class="o">=</span><span class="n">estimator_name_xgb</span><span class="p">,</span>
   <span class="n">calibrate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
   <span class="n">estimator</span><span class="o">=</span><span class="n">xgb_model</span><span class="p">,</span>
   <span class="n">xgboost_early</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
   <span class="n">kfold</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
   <span class="n">impute</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
   <span class="n">scaler_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Turn off scaling for XGBoost</span>
   <span class="n">selectKBest</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
   <span class="n">stratify_y</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
   <span class="n">grid</span><span class="o">=</span><span class="n">xgb_parameters</span><span class="p">,</span>
   <span class="n">randomized_grid</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
   <span class="n">scoring</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;roc_auc&quot;</span><span class="p">],</span>
   <span class="n">random_state</span><span class="o">=</span><span class="mi">222</span><span class="p">,</span>
   <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-7-perform-grid-search-parameter-tuning">
<h3>Step 7: Perform Grid Search Parameter Tuning<a class="headerlink" href="#step-7-perform-grid-search-parameter-tuning" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform grid search parameter tuning</span>
<span class="n">model_tuner</span><span class="o">.</span><span class="n">grid_search_param_tuning</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="m">100</span>%<span class="p">|</span>██████████<span class="p">|</span><span class="w"> </span><span class="m">324</span>/324<span class="w"> </span><span class="o">[</span><span class="m">01</span>:36&lt;<span class="m">00</span>:00,<span class="w">  </span><span class="m">3</span>.37it/s<span class="o">]</span>
Best<span class="w"> </span>score/param<span class="w"> </span><span class="nb">set</span><span class="w"> </span>found<span class="w"> </span>on<span class="w"> </span>validation<span class="w"> </span>set:
<span class="o">{</span><span class="s1">&#39;params&#39;</span>:<span class="w"> </span><span class="o">{</span><span class="s1">&#39;selectKBest__k&#39;</span>:<span class="w"> </span><span class="m">4</span>,
<span class="w">            </span><span class="s1">&#39;xgb__colsample_bytree&#39;</span>:<span class="w"> </span><span class="m">1</span>.0,
<span class="w">            </span><span class="s1">&#39;xgb__early_stopping_rounds&#39;</span>:<span class="w"> </span><span class="m">10</span>,
<span class="w">            </span><span class="s1">&#39;xgb__eval_metric&#39;</span>:<span class="w"> </span><span class="s1">&#39;logloss&#39;</span>,
<span class="w">            </span><span class="s1">&#39;xgb__learning_rate&#39;</span>:<span class="w"> </span><span class="m">0</span>.01,
<span class="w">            </span><span class="s1">&#39;xgb__max_depth&#39;</span>:<span class="w"> </span><span class="m">3</span>,
<span class="w">            </span><span class="s1">&#39;xgb__n_estimators&#39;</span>:<span class="w"> </span><span class="m">199</span>,
<span class="w">            </span><span class="s1">&#39;xgb__subsample&#39;</span>:<span class="w"> </span><span class="m">0</span>.8<span class="o">}</span>,
<span class="s1">&#39;score&#39;</span>:<span class="w"> </span><span class="m">0</span>.9364314448541736<span class="o">}</span>
Best<span class="w"> </span>roc_auc:<span class="w"> </span><span class="m">0</span>.936
</pre></div>
</div>
</section>
<section id="step-8-fit-the-model">
<h3>Step 8: Fit the Model<a class="headerlink" href="#step-8-fit-the-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the training and validation data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">model_tuner</span><span class="o">.</span><span class="n">get_train_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">model_tuner</span><span class="o">.</span><span class="n">get_valid_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Fit the model with the validation data</span>
<span class="n">model_tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
   <span class="n">X_train</span><span class="p">,</span>
   <span class="n">y_train</span><span class="p">,</span>
   <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
   <span class="n">score</span><span class="o">=</span><span class="s2">&quot;roc_auc&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-9-return-metrics-optional">
<h3>Step 9: Return Metrics (Optional)<a class="headerlink" href="#step-9-return-metrics-optional" title="Link to this heading"></a></h3>
<p>You can use this function to evaluate the model by printing the output.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Return metrics for the validation set</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">model_tuner</span><span class="o">.</span><span class="n">return_metrics</span><span class="p">(</span>
   <span class="n">X_valid</span><span class="p">,</span>
   <span class="n">y_valid</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Confusion<span class="w"> </span>matrix<span class="w"> </span>on<span class="w"> </span><span class="nb">set</span><span class="w"> </span>provided:
--------------------------------------------------------------------------------
<span class="w">         </span>Predicted:
<span class="w">            </span>Pos<span class="w">   </span>Neg
--------------------------------------------------------------------------------
Actual:<span class="w"> </span>Pos<span class="w"> </span><span class="m">291</span><span class="w"> </span><span class="o">(</span>tp<span class="o">)</span><span class="w">   </span><span class="m">23</span><span class="w"> </span><span class="o">(</span>fn<span class="o">)</span>
<span class="w">      </span>Neg<span class="w">  </span><span class="m">31</span><span class="w"> </span><span class="o">(</span>fp<span class="o">)</span><span class="w">   </span><span class="m">83</span><span class="w"> </span><span class="o">(</span>tn<span class="o">)</span>
--------------------------------------------------------------------------------

<span class="w">            </span>precision<span class="w">    </span>recall<span class="w">  </span>f1-score<span class="w">   </span>support

<span class="w">         </span><span class="m">0</span><span class="w">       </span><span class="m">0</span>.90<span class="w">      </span><span class="m">0</span>.93<span class="w">      </span><span class="m">0</span>.92<span class="w">       </span><span class="m">314</span>
<span class="w">         </span><span class="m">1</span><span class="w">       </span><span class="m">0</span>.78<span class="w">      </span><span class="m">0</span>.73<span class="w">      </span><span class="m">0</span>.75<span class="w">       </span><span class="m">114</span>

<span class="w">   </span>accuracy<span class="w">                           </span><span class="m">0</span>.87<span class="w">       </span><span class="m">428</span>
<span class="w">   </span>macro<span class="w"> </span>avg<span class="w">       </span><span class="m">0</span>.84<span class="w">      </span><span class="m">0</span>.83<span class="w">      </span><span class="m">0</span>.83<span class="w">       </span><span class="m">428</span>
weighted<span class="w"> </span>avg<span class="w">       </span><span class="m">0</span>.87<span class="w">      </span><span class="m">0</span>.87<span class="w">      </span><span class="m">0</span>.87<span class="w">       </span><span class="m">428</span>

--------------------------------------------------------------------------------

Feature<span class="w"> </span>names<span class="w"> </span>selected:
<span class="o">[</span><span class="s1">&#39;time&#39;</span>,<span class="w"> </span><span class="s1">&#39;strat&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd40&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd420&#39;</span><span class="o">]</span>

<span class="o">{</span><span class="s1">&#39;Classification Report&#39;</span>:<span class="w"> </span><span class="o">{</span><span class="s1">&#39;0&#39;</span>:<span class="w"> </span><span class="o">{</span><span class="s1">&#39;precision&#39;</span>:<span class="w"> </span><span class="m">0</span>.9037267080745341,
<span class="w">   </span><span class="s1">&#39;recall&#39;</span>:<span class="w"> </span><span class="m">0</span>.9267515923566879,
<span class="w">   </span><span class="s1">&#39;f1-score&#39;</span>:<span class="w"> </span><span class="m">0</span>.9150943396226415,
<span class="w">   </span><span class="s1">&#39;support&#39;</span>:<span class="w"> </span><span class="m">314</span>.0<span class="o">}</span>,
<span class="s1">&#39;1&#39;</span>:<span class="w"> </span><span class="o">{</span><span class="s1">&#39;precision&#39;</span>:<span class="w"> </span><span class="m">0</span>.7830188679245284,
<span class="w">   </span><span class="s1">&#39;recall&#39;</span>:<span class="w"> </span><span class="m">0</span>.7280701754385965,
<span class="w">   </span><span class="s1">&#39;f1-score&#39;</span>:<span class="w"> </span><span class="m">0</span>.7545454545454546,
<span class="w">   </span><span class="s1">&#39;support&#39;</span>:<span class="w"> </span><span class="m">114</span>.0<span class="o">}</span>,
<span class="s1">&#39;accuracy&#39;</span>:<span class="w"> </span><span class="m">0</span>.8738317757009346,
<span class="s1">&#39;macro avg&#39;</span>:<span class="w"> </span><span class="o">{</span><span class="s1">&#39;precision&#39;</span>:<span class="w"> </span><span class="m">0</span>.8433727879995312,
<span class="w">   </span><span class="s1">&#39;recall&#39;</span>:<span class="w"> </span><span class="m">0</span>.8274108838976422,
<span class="w">   </span><span class="s1">&#39;f1-score&#39;</span>:<span class="w"> </span><span class="m">0</span>.8348198970840481,
<span class="w">   </span><span class="s1">&#39;support&#39;</span>:<span class="w"> </span><span class="m">428</span>.0<span class="o">}</span>,
<span class="s1">&#39;weighted avg&#39;</span>:<span class="w"> </span><span class="o">{</span><span class="s1">&#39;precision&#39;</span>:<span class="w"> </span><span class="m">0</span>.8715755543897196,
<span class="w">   </span><span class="s1">&#39;recall&#39;</span>:<span class="w"> </span><span class="m">0</span>.8738317757009346,
<span class="w">   </span><span class="s1">&#39;f1-score&#39;</span>:<span class="w"> </span><span class="m">0</span>.8723313188310543,
<span class="w">   </span><span class="s1">&#39;support&#39;</span>:<span class="w"> </span><span class="m">428</span>.0<span class="o">}}</span>,
<span class="s1">&#39;Confusion Matrix&#39;</span>:<span class="w"> </span>array<span class="o">([[</span><span class="m">291</span>,<span class="w">  </span><span class="m">23</span><span class="o">]</span>,
<span class="w">      </span><span class="o">[</span><span class="w"> </span><span class="m">31</span>,<span class="w">  </span><span class="m">83</span><span class="o">]])</span>,
<span class="s1">&#39;K Best Features&#39;</span>:<span class="w"> </span><span class="o">[</span><span class="s1">&#39;time&#39;</span>,<span class="w"> </span><span class="s1">&#39;strat&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd40&#39;</span>,<span class="w"> </span><span class="s1">&#39;cd420&#39;</span><span class="o">]}</span>
</pre></div>
</div>
</section>
<section id="step-10-calibrate-the-model-if-needed">
<h3>Step 10: Calibrate the Model (if needed)<a class="headerlink" href="#step-10-calibrate-the-model-if-needed" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.calibration</span> <span class="kn">import</span> <span class="n">calibration_curve</span>

<span class="c1"># Get the predicted probabilities for the validation data from the</span>
<span class="c1"># uncalibrated model</span>
<span class="n">y_prob_uncalibrated</span> <span class="o">=</span> <span class="n">model_tuner</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Compute the calibration curve for the uncalibrated model</span>
<span class="n">prob_true_uncalibrated</span><span class="p">,</span> <span class="n">prob_pred_uncalibrated</span> <span class="o">=</span> <span class="n">calibration_curve</span><span class="p">(</span>
   <span class="n">y_test</span><span class="p">,</span>
   <span class="n">y_prob_uncalibrated</span><span class="p">,</span>
   <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1"># Calibrate the model</span>
<span class="k">if</span> <span class="n">model_tuner</span><span class="o">.</span><span class="n">calibrate</span><span class="p">:</span>
   <span class="n">model_tuner</span><span class="o">.</span><span class="n">calibrateModel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">score</span><span class="o">=</span><span class="s2">&quot;roc_auc&quot;</span><span class="p">)</span>

<span class="c1"># Predict on the validation set</span>
<span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">model_tuner</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Change<span class="w"> </span>back<span class="w"> </span>to<span class="w"> </span>CPU
Confusion<span class="w"> </span>matrix<span class="w"> </span>on<span class="w"> </span>validation<span class="w"> </span><span class="nb">set</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>roc_auc
--------------------------------------------------------------------------------
<span class="w">         </span>Predicted:
<span class="w">            </span>Pos<span class="w">   </span>Neg
--------------------------------------------------------------------------------
Actual:<span class="w"> </span>Pos<span class="w"> </span><span class="m">292</span><span class="w"> </span><span class="o">(</span>tp<span class="o">)</span><span class="w">   </span><span class="m">22</span><span class="w"> </span><span class="o">(</span>fn<span class="o">)</span>
<span class="w">      </span>Neg<span class="w">  </span><span class="m">32</span><span class="w"> </span><span class="o">(</span>fp<span class="o">)</span><span class="w">   </span><span class="m">82</span><span class="w"> </span><span class="o">(</span>tn<span class="o">)</span>
--------------------------------------------------------------------------------

<span class="w">            </span>precision<span class="w">    </span>recall<span class="w">  </span>f1-score<span class="w">   </span>support

<span class="w">         </span><span class="m">0</span><span class="w">       </span><span class="m">0</span>.90<span class="w">      </span><span class="m">0</span>.93<span class="w">      </span><span class="m">0</span>.92<span class="w">       </span><span class="m">314</span>
<span class="w">         </span><span class="m">1</span><span class="w">       </span><span class="m">0</span>.79<span class="w">      </span><span class="m">0</span>.72<span class="w">      </span><span class="m">0</span>.75<span class="w">       </span><span class="m">114</span>

<span class="w">   </span>accuracy<span class="w">                           </span><span class="m">0</span>.87<span class="w">       </span><span class="m">428</span>
<span class="w">   </span>macro<span class="w"> </span>avg<span class="w">       </span><span class="m">0</span>.84<span class="w">      </span><span class="m">0</span>.82<span class="w">      </span><span class="m">0</span>.83<span class="w">       </span><span class="m">428</span>
weighted<span class="w"> </span>avg<span class="w">       </span><span class="m">0</span>.87<span class="w">      </span><span class="m">0</span>.87<span class="w">      </span><span class="m">0</span>.87<span class="w">       </span><span class="m">428</span>

--------------------------------------------------------------------------------
roc_auc<span class="w"> </span>after<span class="w"> </span>calibration:<span class="w"> </span><span class="m">0</span>.9364035087719298
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Get the predicted probabilities for the validation data from calibrated model</span>
<span class="n">y_prob_calibrated</span> <span class="o">=</span> <span class="n">model_tuner</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Compute the calibration curve for the calibrated model</span>
<span class="n">prob_true_calibrated</span><span class="p">,</span> <span class="n">prob_pred_calibrated</span> <span class="o">=</span> <span class="n">calibration_curve</span><span class="p">(</span>
   <span class="n">y_test</span><span class="p">,</span>
   <span class="n">y_prob_calibrated</span><span class="p">,</span>
   <span class="n">n_bins</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1"># Plot the calibration curves</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
   <span class="n">prob_pred_uncalibrated</span><span class="p">,</span>
   <span class="n">prob_true_uncalibrated</span><span class="p">,</span>
   <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
   <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Uncalibrated XGBoost&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
   <span class="n">prob_pred_calibrated</span><span class="p">,</span>
   <span class="n">prob_true_calibrated</span><span class="p">,</span>
   <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
   <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Calibrated XGBoost&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
   <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
   <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
   <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
   <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Perfectly calibrated&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted probability&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;True probability in each bin&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Calibration plot (reliability curve)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="no-click"><a class="reference internal image-reference" href="_images/calibration_curves.png"><img alt="Model Tuner Logo" class="align-center" src="_images/calibration_curves.png" style="width: 400px;" /></a>
</div><div style="height: 50px;"></div></section>
<section id="classification-report-optional">
<h3>Classification Report (Optional)<a class="headerlink" href="#classification-report-optional" title="Link to this heading"></a></h3>
<p>A classification report is readily available at this stage, should you wish to
print and examine it. A call to <code class="docutils literal notranslate"><span class="pre">print(model_tuner.classification_report)</span></code> will
output it as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model_tuner</span><span class="o">.</span><span class="n">classification_report</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">              </span>precision<span class="w">    </span>recall<span class="w">  </span>f1-score<span class="w">   </span>support

<span class="w">           </span><span class="m">0</span><span class="w">       </span><span class="m">0</span>.90<span class="w">      </span><span class="m">0</span>.93<span class="w">      </span><span class="m">0</span>.92<span class="w">       </span><span class="m">314</span>
<span class="w">           </span><span class="m">1</span><span class="w">       </span><span class="m">0</span>.79<span class="w">      </span><span class="m">0</span>.72<span class="w">      </span><span class="m">0</span>.75<span class="w">       </span><span class="m">114</span>

<span class="w">    </span>accuracy<span class="w">                           </span><span class="m">0</span>.87<span class="w">       </span><span class="m">428</span>
<span class="w">   </span>macro<span class="w"> </span>avg<span class="w">       </span><span class="m">0</span>.84<span class="w">      </span><span class="m">0</span>.82<span class="w">      </span><span class="m">0</span>.83<span class="w">       </span><span class="m">428</span>
weighted<span class="w"> </span>avg<span class="w">       </span><span class="m">0</span>.87<span class="w">      </span><span class="m">0</span>.87<span class="w">      </span><span class="m">0</span>.87<span class="w">       </span><span class="m">428</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="regression">
<h1>Regression<a class="headerlink" href="#regression" title="Link to this heading"></a></h1>
<p>Here is an example of using the <code class="docutils literal notranslate"><span class="pre">Model</span></code> class for regression using XGBoost on the California Housing dataset.</p>
<section id="california-housing-with-xgboost">
<h2>California Housing with XGBoost<a class="headerlink" href="#california-housing-with-xgboost" title="Link to this heading"></a></h2>
<section id="id1">
<h3>Step 1: Import Necessary Libraries<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
<span class="kn">from</span> <span class="nn">model_tuner</span> <span class="kn">import</span> <span class="n">model_tuner</span>
</pre></div>
</div>
</section>
<section id="step-2-load-the-dataset">
<h3>Step 2: Load the Dataset<a class="headerlink" href="#step-2-load-the-dataset" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the California Housing dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-3-create-an-instance-of-the-xgbclassifier">
<h3>Step 3: Create an Instance of the XGBClassifier<a class="headerlink" href="#step-3-create-an-instance-of-the-xgbclassifier" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating an instance of the XGBRegressor</span>
<span class="n">xgb_model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBRegressor</span><span class="p">(</span>
   <span class="n">random_state</span><span class="o">=</span><span class="mi">222</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-4-define-hyperparameters-for-xgboost">
<h3>Step 4: Define Hyperparameters for XGBoost<a class="headerlink" href="#step-4-define-hyperparameters-for-xgboost" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Estimator name prefix for use in GridSearchCV or similar tools</span>
<span class="n">estimator_name_xgb</span> <span class="o">=</span> <span class="s2">&quot;xgb&quot;</span>

<span class="c1"># Define the hyperparameters for XGBoost</span>
<span class="n">xgb_learning_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]</span>
<span class="n">xgb_n_estimators</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">]</span>
<span class="n">xgb_max_depths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="n">xgb_subsamples</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">xgb_colsample_bytree</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>

<span class="c1"># Combining the hyperparameters in a dictionary</span>
<span class="n">xgb_parameters</span> <span class="o">=</span> <span class="p">[</span>
   <span class="p">{</span>
      <span class="s2">&quot;xgb__learning_rate&quot;</span><span class="p">:</span> <span class="n">xgb_learning_rates</span><span class="p">,</span>
      <span class="s2">&quot;xgb__n_estimators&quot;</span><span class="p">:</span> <span class="n">xgb_n_estimators</span><span class="p">,</span>
      <span class="s2">&quot;xgb__max_depth&quot;</span><span class="p">:</span> <span class="n">xgb_max_depths</span><span class="p">,</span>
      <span class="s2">&quot;xgb__subsample&quot;</span><span class="p">:</span> <span class="n">xgb_subsamples</span><span class="p">,</span>
      <span class="s2">&quot;xgb__colsample_bytree&quot;</span><span class="p">:</span> <span class="n">xgb_colsample_bytree</span><span class="p">,</span>
      <span class="s2">&quot;selectKBest__k&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
   <span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
</section>
<section id="step-5-initialize-and-configure-the-model">
<h3>Step 5: Initialize and Configure the <code class="docutils literal notranslate"><span class="pre">Model</span></code><a class="headerlink" href="#step-5-initialize-and-configure-the-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize model_tuner</span>
<span class="n">model_tuner</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span>
   <span class="n">name</span><span class="o">=</span><span class="s2">&quot;XGBoost_California_Housing&quot;</span><span class="p">,</span>
   <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;regression&quot;</span><span class="p">,</span>
   <span class="n">estimator_name</span><span class="o">=</span><span class="n">estimator_name_xgb</span><span class="p">,</span>
   <span class="n">calibrate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
   <span class="n">estimator</span><span class="o">=</span><span class="n">xgb_model</span><span class="p">,</span>
   <span class="n">kfold</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
   <span class="n">impute</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
   <span class="n">scaler_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
   <span class="n">selectKBest</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
   <span class="n">stratify_y</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
   <span class="n">grid</span><span class="o">=</span><span class="n">xgb_parameters</span><span class="p">,</span>
   <span class="n">randomized_grid</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
   <span class="n">scoring</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;neg_mean_squared_error&quot;</span><span class="p">],</span>
   <span class="n">random_state</span><span class="o">=</span><span class="mi">222</span><span class="p">,</span>
   <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-6-fit-the-model">
<h3>Step 6: Fit the Model<a class="headerlink" href="#step-6-fit-the-model" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the training and validation data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">model_tuner</span><span class="o">.</span><span class="n">get_train_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">model_tuner</span><span class="o">.</span><span class="n">get_valid_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Fit the model with the validation data</span>
<span class="n">model_tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
   <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span>
   <span class="n">score</span><span class="o">=</span><span class="s2">&quot;neg_mean_squared_error&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-7-return-metrics-optional">
<h3>Step 7: Return Metrics (Optional)<a class="headerlink" href="#step-7-return-metrics-optional" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Return metrics for the validation set</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">model_tuner</span><span class="o">.</span><span class="n">return_metrics</span><span class="p">(</span>
   <span class="n">X_valid</span><span class="p">,</span>
   <span class="n">y_valid</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="m">100</span>%<span class="p">|</span>██████████<span class="p">|</span><span class="w"> </span><span class="m">432</span>/432<span class="w"> </span><span class="o">[</span><span class="m">04</span>:10&lt;<span class="m">00</span>:00,<span class="w">  </span><span class="m">1</span>.73it/s<span class="o">]</span>
Best<span class="w"> </span>score/param<span class="w"> </span><span class="nb">set</span><span class="w"> </span>found<span class="w"> </span>on<span class="w"> </span>validation<span class="w"> </span>set:
<span class="o">{</span><span class="s1">&#39;params&#39;</span>:<span class="w"> </span><span class="o">{</span><span class="s1">&#39;selectKBest__k&#39;</span>:<span class="w"> </span><span class="m">8</span>,
<span class="w">            </span><span class="s1">&#39;xgb__colsample_bytree&#39;</span>:<span class="w"> </span><span class="m">0</span>.8,
<span class="w">            </span><span class="s1">&#39;xgb__learning_rate&#39;</span>:<span class="w"> </span><span class="m">0</span>.05,
<span class="w">            </span><span class="s1">&#39;xgb__max_depth&#39;</span>:<span class="w"> </span><span class="m">7</span>,
<span class="w">            </span><span class="s1">&#39;xgb__n_estimators&#39;</span>:<span class="w"> </span><span class="m">300</span>,
<span class="w">            </span><span class="s1">&#39;xgb__subsample&#39;</span>:<span class="w"> </span><span class="m">0</span>.8<span class="o">}</span>,
<span class="s1">&#39;score&#39;</span>:<span class="w"> </span>-0.21038206511437127<span class="o">}</span>
Best<span class="w"> </span>neg_mean_squared_error:<span class="w"> </span>-0.210

********************************************************************************
<span class="o">{</span><span class="s1">&#39;Explained Variance&#39;</span>:<span class="w"> </span><span class="m">0</span>.8385815985957561,
<span class="s1">&#39;Mean Absolute Error&#39;</span>:<span class="w"> </span><span class="m">0</span>.3008222037008959,
<span class="s1">&#39;Mean Squared Error&#39;</span>:<span class="w"> </span><span class="m">0</span>.21038206511437127,
<span class="s1">&#39;Median Absolute Error&#39;</span>:<span class="w"> </span><span class="m">0</span>.196492121219635,
<span class="s1">&#39;R2&#39;</span>:<span class="w"> </span><span class="m">0</span>.8385811859863378,
<span class="s1">&#39;RMSE&#39;</span>:<span class="w"> </span><span class="m">0</span>.45867424727618106<span class="o">}</span>
********************************************************************************

Feature<span class="w"> </span>names<span class="w"> </span>selected:
<span class="o">[</span><span class="s1">&#39;MedInc&#39;</span>,<span class="w"> </span><span class="s1">&#39;HouseAge&#39;</span>,<span class="w"> </span><span class="s1">&#39;AveRooms&#39;</span>,<span class="w"> </span><span class="s1">&#39;AveBedrms&#39;</span>,<span class="w"> </span><span class="s1">&#39;Population&#39;</span>,
<span class="s1">&#39;AveOccup&#39;</span>,<span class="w"> </span><span class="s1">&#39;Latitude&#39;</span>,<span class="w"> </span><span class="s1">&#39;Longitude&#39;</span><span class="o">]</span>

<span class="o">{</span><span class="s1">&#39;Regression Report&#39;</span>:<span class="w"> </span><span class="o">{</span><span class="s1">&#39;Explained Variance&#39;</span>:<span class="w"> </span><span class="m">0</span>.8385815985957561,<span class="w"> </span><span class="s1">&#39;R2&#39;</span>:
<span class="m">0</span>.8385811859863378,<span class="w"> </span><span class="s1">&#39;Mean Absolute Error&#39;</span>:<span class="w"> </span><span class="m">0</span>.3008222037008959,<span class="w"> </span><span class="s1">&#39;Median</span>
<span class="s1">Absolute Error&#39;</span>:<span class="w"> </span><span class="m">0</span>.196492121219635,<span class="w"> </span><span class="s1">&#39;Mean Squared Error&#39;</span>:
<span class="m">0</span>.21038206511437127,<span class="w"> </span><span class="s1">&#39;RMSE&#39;</span>:<span class="w"> </span><span class="m">0</span>.45867424727618106<span class="o">}</span>,<span class="w"> </span><span class="s1">&#39;K Best Features&#39;</span>:
<span class="o">[</span><span class="s1">&#39;MedInc&#39;</span>,<span class="w"> </span><span class="s1">&#39;HouseAge&#39;</span>,<span class="w"> </span><span class="s1">&#39;AveRooms&#39;</span>,<span class="w"> </span><span class="s1">&#39;AveBedrms&#39;</span>,<span class="w"> </span><span class="s1">&#39;Population&#39;</span>,
<span class="s1">&#39;AveOccup&#39;</span>,<span class="w"> </span><span class="s1">&#39;Latitude&#39;</span>,<span class="w"> </span><span class="s1">&#39;Longitude&#39;</span><span class="o">]}</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="getting_started.html" class="btn btn-neutral float-left" title="Welcome to Model Tuner’s Documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="about.html" class="btn btn-neutral float-right" title="GitHub Repository" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, UCLA CTSI ML Team: Leonid Shpaner, Arthur Funnell, Panayiotis Petousis.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>