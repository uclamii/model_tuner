

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Zero Variance Columns &mdash; Model Tuner 0.0.28b0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=72ec8e44" />

  
    <link rel="canonical" href="https://uclamii.github.io/model_tuner/caveats.html" />
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=2ce77d63"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=f281be69"></script>
      <script src="_static/custom.js?v=59429b38"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="GitHub Repository" href="about.html" />
    <link rel="prev" title="iPython Notebooks" href="usage_guide.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Model Tuner
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Welcome to Model Tuner’s Documentation!</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Usage Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="usage_guide.html">iPython Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_guide.html#input-parameters">Input Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_guide.html#key-methods-and-functionalities">Key Methods and Functionalities</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_guide.html#helper-functions">Helper Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_guide.html#pipeline-management">Pipeline Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_guide.html#binary-classification">Binary Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_guide.html#multi-class-classification">Multi-Class Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_guide.html#regression">Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_guide.html#performance-evaluation-metrics">Performance Evaluation Metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_guide.html#bootstrap-metrics">Bootstrap Metrics</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Caveats</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Zero Variance Columns</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#effects-on-model-training">Effects on Model Training</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#dependent-variable">Dependent Variable</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#target-variable-shape-and-its-effects">Target Variable Shape and Its Effects</a></li>
<li class="toctree-l2"><a class="reference internal" href="#solution">Solution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#scaling-before-imputation">Scaling Before Imputation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#accurate-imputation-with-normalized-data">Accurate Imputation with Normalized Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#consistency-in-data-transformation">Consistency in Data Transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prevention-of-imputation-bias">Prevention of Imputation Bias</a></li>
<li class="toctree-l2"><a class="reference internal" href="#avoiding-distortions-in-nearest-neighbor-imputation">Avoiding Distortions in Nearest Neighbor Imputation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#visual-proof">Visual Proof</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#column-stratification-with-cross-validation">Column Stratification with Cross-Validation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#cross-validation-and-stratification">Cross-Validation and Stratification</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#model-calibration">Model Calibration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#goal-of-calibration">Goal of Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#calibration-curve">Calibration Curve</a></li>
<li class="toctree-l2"><a class="reference internal" href="#brier-score">Brier Score</a></li>
<li class="toctree-l2"><a class="reference internal" href="#platt-scaling">Platt Scaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#isotonic-regression">Isotonic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#example-calibration-in-logistic-regression">Example: Calibration in Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#using-imputation-and-scaling-in-pipeline-steps-for-model-preprocessing">Using Imputation and Scaling in Pipeline Steps for Model Preprocessing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#when-is-imputation-and-feature-scaling-in-pipeline-steps-beneficial">When Is Imputation and Feature Scaling in <code class="docutils literal notranslate"><span class="pre">pipeline_steps</span></code> Beneficial?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#models-not-benefiting-from-imputation-and-scaling-in-pipeline-steps">Models Not Benefiting From Imputation and Scaling in <code class="docutils literal notranslate"><span class="pre">pipeline_steps</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#why-doesn-t-xgboost-require-imputation-and-scaling-in-pipeline-steps">Why Doesn’t XGBoost Require Imputation and Scaling in <code class="docutils literal notranslate"><span class="pre">pipeline_steps</span></code>?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#caveats-in-imbalanced-learning">Caveats in Imbalanced Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#bias-from-class-distribution">Bias from Class Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="#threshold-dependent-predictions">Threshold-Dependent Predictions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#limitations-of-accuracy">Limitations of Accuracy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#integration-and-practical-considerations">Integration and Practical Considerations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#impact-of-resampling-techniques">Impact of Resampling Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#smote-a-mathematical-illustration">SMOTE: A Mathematical Illustration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-of-synthetic-sample-creation">Example of Synthetic Sample Creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mitigating-the-caveats">Mitigating the Caveats</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#threshold-tuning-considerations">Threshold Tuning Considerations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#the-precision-recall-tradeoff">The Precision-Recall Tradeoff</a></li>
<li class="toctree-l2"><a class="reference internal" href="#threshold-optimization">Threshold Optimization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#elasticnet-regularization">ElasticNet Regularization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#important-considerations">Important Considerations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#catboost-training-parameters">CatBoost Training Parameters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#example-tuning-hyperparameters-for-catboost">Example: Tuning hyperparameters for CatBoost</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About Model Tuner</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="about.html">GitHub Repository</a></li>
<li class="toctree-l1"><a class="reference internal" href="about.html#acknowledgements">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="about.html#citing-model-tuner">Citing Model Tuner</a></li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Model Tuner</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Zero Variance Columns</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="no-click"><a class="reference internal image-reference" href="_images/ModelTunerTarget.png"><img alt="Model Tuner Logo" class="align-left" src="_images/ModelTunerTarget.png" style="width: 250px;" />
</a>
</div><div style="height: 150px;"></div><p></p>
<section id="zero-variance-columns">
<h1>Zero Variance Columns<a class="headerlink" href="#zero-variance-columns" title="Link to this heading"></a></h1>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Ensure that your feature set <cite>X</cite> is free of zero-variance columns before using this method.
Zero-variance columns can lead to issues such as <code class="docutils literal notranslate"><span class="pre">UserWarning:</span> <span class="pre">Features[feat_num]</span> <span class="pre">are</span> <span class="pre">constant</span></code>
and <code class="docutils literal notranslate"><span class="pre">RuntimeWarning:</span> <span class="pre">invalid</span> <span class="pre">value</span> <span class="pre">encountered</span> <span class="pre">in</span> <span class="pre">divide</span> <span class="pre">f</span> <span class="pre">=</span> <span class="pre">msb/msw</span></code> during the model training process.</p>
<p>To check for and remove zero-variance columns, you can use the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check for zero-variance columns and drop them</span>
<span class="n">zero_variance_columns</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">var</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">zero_variance_columns</span><span class="o">.</span><span class="n">empty</span><span class="p">:</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">zero_variance_columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Zero-variance columns in the feature set <span class="math notranslate nohighlight">\(X\)</span> refer to columns where all values are identical.
Mathematically, if <span class="math notranslate nohighlight">\(X_j\)</span> is a column in <span class="math notranslate nohighlight">\(X\)</span>, the variance of this column is calculated as:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(X_j) = \frac{1}{n} \sum_{i=1}^{n} (X_{ij} - \bar{X}_j)^2 = 0\]</div>
<p>where <span class="math notranslate nohighlight">\(X_{ij}\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th observation of feature <span class="math notranslate nohighlight">\(j\)</span>, and <span class="math notranslate nohighlight">\(\bar{X}_j\)</span> is the mean of the <span class="math notranslate nohighlight">\(j\)</span>-th feature.
Since all <span class="math notranslate nohighlight">\(X_{ij}\)</span> are equal, <span class="math notranslate nohighlight">\(\text{Var}(X_j)\)</span> is zero.</p>
<section id="effects-on-model-training">
<h2>Effects on Model Training<a class="headerlink" href="#effects-on-model-training" title="Link to this heading"></a></h2>
<ol class="arabic">
<li><p><strong>UserWarning:</strong></p>
<p>During model training, algorithms often check for variability in features to determine their usefulness in predicting the target variable. A zero-variance column provides no information, leading to the following warning:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>UserWarning: Features[feat_num] are constant
</pre></div>
</div>
<p>This indicates that the feature <span class="math notranslate nohighlight">\(X_j\)</span> has no variability and, therefore, cannot contribute to the model’s predictive power.</p>
</li>
<li><p><strong>RuntimeWarning:</strong></p>
<p>When calculating metrics like the F-statistic used in Analysis of Variance (ANOVA) or feature importance metrics, the following ratio is computed:</p>
<div class="math notranslate nohighlight">
\[F = \frac{\text{MSB}}{\text{MSW}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{MSB}\)</span> (Mean Square Between) and <span class="math notranslate nohighlight">\(\text{MSW}\)</span> (Mean Square Within) are defined as:</p>
<div class="math notranslate nohighlight">
\[\text{MSB} = \frac{1}{k-1} \sum_{j=1}^{k} n_j (\bar{X}_j - \bar{X})^2\]</div>
<div class="math notranslate nohighlight">
\[\text{MSW} = \frac{1}{n-k} \sum_{j=1}^{k} \sum_{i=1}^{n_j} (X_{ij} - \bar{X}_j)^2\]</div>
<p>If <span class="math notranslate nohighlight">\(X_j\)</span> is a zero-variance column, then <span class="math notranslate nohighlight">\(\text{MSW} = 0\)</span> because all <span class="math notranslate nohighlight">\(X_{ij}\)</span> are equal to <span class="math notranslate nohighlight">\(\bar{X}_j\)</span>. This leads to a division by zero in the calculation of <span class="math notranslate nohighlight">\(F\)</span>:</p>
<div class="math notranslate nohighlight">
\[F = \frac{\text{MSB}}{0} \rightarrow \text{undefined}\]</div>
<p>which triggers a runtime warning:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>RuntimeWarning: invalid value encountered in divide f = msb/msw
</pre></div>
</div>
<p>indicating that the calculation involves dividing by zero, resulting in undefined or infinite values.</p>
</li>
</ol>
<p>To avoid these issues, ensure that zero-variance columns are removed from <span class="math notranslate nohighlight">\(X\)</span> before proceeding with model training.</p>
</section>
</section>
<section id="dependent-variable">
<h1>Dependent Variable<a class="headerlink" href="#dependent-variable" title="Link to this heading"></a></h1>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Additionally, ensure that <cite>y</cite> (the target variable) is passed as a Series and not as a DataFrame.
Passing <cite>y</cite> as a DataFrame can cause issues such as <code class="docutils literal notranslate"><span class="pre">DataConversionWarning:</span> <span class="pre">A</span> <span class="pre">column-vector</span> <span class="pre">y</span> <span class="pre">was</span> <span class="pre">passed</span>
<span class="pre">when</span> <span class="pre">a</span> <span class="pre">1d</span> <span class="pre">array</span> <span class="pre">was</span> <span class="pre">expected.</span> <span class="pre">Please</span> <span class="pre">change</span> <span class="pre">the</span> <span class="pre">shape</span> <span class="pre">of</span> <span class="pre">y</span> <span class="pre">to</span> <span class="pre">(n_samples,)</span></code>.</p>
<p>If <cite>y</cite> is a DataFrame, you can convert it to a Series using the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert y to a Series if it&#39;s a DataFrame</span>
<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</pre></div>
</div>
<p>This conversion ensures that the target variable <cite>y</cite> has the correct shape, preventing the aforementioned warning.</p>
</div>
<section id="target-variable-shape-and-its-effects">
<h2>Target Variable Shape and Its Effects<a class="headerlink" href="#target-variable-shape-and-its-effects" title="Link to this heading"></a></h2>
<p>The target variable <span class="math notranslate nohighlight">\(y\)</span> should be passed as a 1-dimensional array (Series) and not as a 2-dimensional array (DataFrame).
If <span class="math notranslate nohighlight">\(y\)</span> is passed as a DataFrame, the model training process might raise the following warning:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>DataConversionWarning: A column-vector y was passed when a 1d array was expected.
Please change the shape of y to (n_samples,).
</pre></div>
</div>
<p><strong>Explanation:</strong></p>
<p>Machine learning models generally expect the target variable <span class="math notranslate nohighlight">\(y\)</span> to be in the shape of a 1-dimensional array,
denoted as <span class="math notranslate nohighlight">\(y = \{y_1, y_2, \dots, y_n\}\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of samples.
Mathematically, <span class="math notranslate nohighlight">\(y\)</span> is represented as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}\end{split}\]</div>
<p>When <span class="math notranslate nohighlight">\(y\)</span> is passed as a DataFrame, it is treated as a 2-dimensional array, which has the form:</p>
<div class="math notranslate nohighlight">
\[y = \begin{pmatrix} y_1, y_2, \dots , y_n \end{pmatrix}\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[\begin{split}y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}\end{split}\]</div>
<p>where each sample is represented as a column vector. This discrepancy in dimensionality can cause the model to misinterpret the data,
leading to the <code class="docutils literal notranslate"><span class="pre">DataConversionWarning</span></code>.</p>
</section>
<section id="solution">
<h2>Solution<a class="headerlink" href="#solution" title="Link to this heading"></a></h2>
<p>To ensure <span class="math notranslate nohighlight">\(y\)</span> is interpreted correctly as a 1-dimensional array, it should be passed as a Series.
If <span class="math notranslate nohighlight">\(y\)</span> is currently a DataFrame, you can convert it to a Series using the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert y to a Series if it&#39;s a DataFrame</span>
<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</pre></div>
</div>
<p>The method <code class="code docutils literal notranslate"><span class="pre">squeeze()</span></code> effectively removes any unnecessary dimensions, converting a 2-dimensional DataFrame
with a single column into a 1-dimensional Series. This ensures that <span class="math notranslate nohighlight">\(y\)</span> has the correct shape, preventing
the aforementioned warning and ensuring the model processes the target variable correctly.</p>
</section>
</section>
<section id="scaling-before-imputation">
<h1>Scaling Before Imputation<a class="headerlink" href="#scaling-before-imputation" title="Link to this heading"></a></h1>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>It is crucial to apply scaling before imputation during the data preprocessing
pipeline to preserve the mathematical integrity of the transformations and ensure accurate imputations. The
correct sequence for the pipeline is as follows:</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pipeline_steps</span> <span class="o">=</span> <span class="p">[</span>
   <span class="p">(</span><span class="s2">&quot;Scaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
   <span class="p">(</span><span class="s2">&quot;Imputer&quot;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">()),</span>
<span class="p">]</span>
</pre></div>
</div>
<section id="accurate-imputation-with-normalized-data">
<h2>Accurate Imputation with Normalized Data<a class="headerlink" href="#accurate-imputation-with-normalized-data" title="Link to this heading"></a></h2>
<p>Imputation methods, such as mean, median, or k-Nearest Neighbors (k-NN), depend
on the scale of the data. Without scaling, features with large magnitudes can
disproportionately influence the imputed values, especially in methods like k-NN imputation.</p>
<p>For example, consider a feature <span class="math notranslate nohighlight">\(X = [1, 2, \text{NaN}, 4, 1000]\)</span> with a
missing value. The high value of 1000 significantly skews the mean,
<span class="math notranslate nohighlight">\(\mu = \frac{1 + 2 + 4 + 1000}{4} = 251.75\)</span>, leading to an imputed value of
251.75, which does not reflect the underlying data pattern.</p>
<p>If scaling is applied first, the data becomes:</p>
<div class="math notranslate nohighlight">
\[X_{\text{scaled}} = [-0.58, -0.57, \text{NaN}, -0.54, 2.26]\]</div>
<p>Now, imputing the missing value using the scaled data results in a more accurate
representation of the data’s distribution. Once the scaling is reversed, the
imputed value aligns more naturally with the original data.</p>
</section>
<section id="consistency-in-data-transformation">
<h2>Consistency in Data Transformation<a class="headerlink" href="#consistency-in-data-transformation" title="Link to this heading"></a></h2>
<p>Scaling before imputation ensures that the transformations applied are consistent
across the entire dataset, including the imputed values. For instance, consider
a feature <span class="math notranslate nohighlight">\(X = [1, 2, \text{NaN}, 4, 5]\)</span>. After applying standardization
first, the available values are transformed using:</p>
<div class="math notranslate nohighlight">
\[z = \frac{x - \mu}{\sigma}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> are calculated using only the non-missing
values. The imputed values are then calculated on this standardized scale,
maintaining consistency in both the transformation and the imputation process.</p>
<p>Had imputation been applied first, the calculated mean and standard deviation
would have been skewed by the imputed value, potentially leading to an inaccurate
scaling transformation.</p>
</section>
<section id="prevention-of-imputation-bias">
<h2>Prevention of Imputation Bias<a class="headerlink" href="#prevention-of-imputation-bias" title="Link to this heading"></a></h2>
<p>By scaling first, we mitigate the risk of imputing values that disproportionately
align with features of larger magnitudes. For instance, in datasets where features
vary widely in scale, imputing before scaling might introduce a bias in imputed
values that heavily favors dominant features. Scaling ensures that all features
are on an equal footing prior to imputation.</p>
</section>
<section id="avoiding-distortions-in-nearest-neighbor-imputation">
<h2>Avoiding Distortions in Nearest Neighbor Imputation<a class="headerlink" href="#avoiding-distortions-in-nearest-neighbor-imputation" title="Link to this heading"></a></h2>
<p>-Nearest Neighbors (k-NN) imputation relies on the calculation of distances
between data points. If features are not scaled beforehand, large-magnitude
features dominate the distance calculations, resulting in imputation values that
may not reflect the true proximity of data points. Scaling first ensures that each
feature contributes equally to the distance metric.</p>
<p>For example, consider two features:</p>
<p><span class="math notranslate nohighlight">\(\text{Feature1} = [1, 2, \text{NaN}, 4]\)</span> and
<span class="math notranslate nohighlight">\(\text{Feature2} = [100, 200, \text{NaN}, 400]\)</span>.</p>
<p>Without scaling, the distance calculation is dominated by Feature2 due to its larger magnitude. Scaling the
features first eliminates this bias, leading to more meaningful imputations.</p>
</section>
<section id="visual-proof">
<h2>Visual Proof<a class="headerlink" href="#visual-proof" title="Link to this heading"></a></h2>
<p>The accompanying figure demonstrates the difference between the two approaches:</p>
<ul class="simple">
<li><p><strong>Left Panel (Impute First, Then Scale)</strong>: Notice the irregular distribution of scaled values after imputation, with some features exhibiting unnatural spikes.</p></li>
<li><p><strong>Right Panel (Scale First, Then Impute)</strong>: Displays a more uniform and consistent distribution across features, reflecting the integrity of the scaled data.</p></li>
</ul>
<div class="no-click"><a class="reference internal image-reference" href="_images/scale_before_impute.png"><img alt="Calibration Curve AIDs" class="align-center" src="_images/scale_before_impute.png" style="width: 800px;" />
</a>
</div><div style="height: 50px;"></div><p>This highlights that scaling before imputation results in a cleaner, more consistent preprocessing pipeline.</p>
</section>
</section>
<section id="column-stratification-with-cross-validation">
<h1>Column Stratification with Cross-Validation<a class="headerlink" href="#column-stratification-with-cross-validation" title="Link to this heading"></a></h1>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Using</strong> <code class="docutils literal notranslate"><span class="pre">stratify_cols</span></code> <strong>with Cross-Validation</strong></p>
<p>It is important to note that <code class="docutils literal notranslate"><span class="pre">stratify_cols</span></code> cannot be used when performing cross-validation.
Cross-validation involves repeatedly splitting the dataset into training and validation sets to
evaluate the model’s performance across different subsets of the data.</p>
<p><strong>Explanation:</strong></p>
<p>When using cross-validation, the process automatically handles the stratification of the target variable <span class="math notranslate nohighlight">\(y\)</span>,
if specified. This ensures that each fold is representative of the overall distribution of <span class="math notranslate nohighlight">\(y\)</span>. However,
<code class="docutils literal notranslate"><span class="pre">stratify_cols</span></code> is designed to stratify based on specific columns in the feature set <span class="math notranslate nohighlight">\(X\)</span>, which can lead to
inconsistencies or even errors when applied in the context of cross-validation.</p>
<p>Since cross-validation inherently handles stratification based on the target variable, attempting to apply
additional stratification based on specific columns would conflict with the cross-validation process.
This can result in unpredictable behavior or failure of the cross-validation routine.</p>
<p>However, you can use <code class="docutils literal notranslate"><span class="pre">stratify_y</span></code> during cross-validation to ensure that each fold of the dataset is representative
of the distribution of the target variable <span class="math notranslate nohighlight">\(y\)</span>. This is a common practice to maintain consistency in the distribution
of the target variable across the different training and validation sets.</p>
</div>
<section id="cross-validation-and-stratification">
<h2>Cross-Validation and Stratification<a class="headerlink" href="#cross-validation-and-stratification" title="Link to this heading"></a></h2>
<p>Let <span class="math notranslate nohighlight">\(D = \{(X_i, y_i)\}_{i=1}^n\)</span> be the dataset with <span class="math notranslate nohighlight">\(n\)</span> samples, where <span class="math notranslate nohighlight">\(X_i\)</span> is the feature set and <span class="math notranslate nohighlight">\(y_i\)</span> is the target variable.</p>
<p>In <cite>k-fold</cite> cross-validation, the dataset <span class="math notranslate nohighlight">\(D\)</span> is split into <span class="math notranslate nohighlight">\(k\)</span> folds <span class="math notranslate nohighlight">\(\{D_1, D_2, \dots, D_k\}\)</span>.</p>
<p>When stratifying by <span class="math notranslate nohighlight">\(y\)</span> using <code class="code docutils literal notranslate"><span class="pre">stratify_y</span></code>, each fold <span class="math notranslate nohighlight">\(D_j\)</span> is constructed such that the distribution of <span class="math notranslate nohighlight">\(y\)</span> in each fold is similar to the distribution of <span class="math notranslate nohighlight">\(y\)</span> in <span class="math notranslate nohighlight">\(D\)</span>.</p>
<p>Mathematically, if <span class="math notranslate nohighlight">\(P(y=c)\)</span> is the probability of the target variable <span class="math notranslate nohighlight">\(y\)</span> taking on class <span class="math notranslate nohighlight">\(c\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[P(y=c \mid D_j) \approx P(y=c \mid D)\]</div>
<p>for all folds <span class="math notranslate nohighlight">\(D_j\)</span> and all classes <span class="math notranslate nohighlight">\(c\)</span>.</p>
<p>This ensures that the stratified folds preserve the same class proportions as the original dataset.</p>
<p>On the other hand, <code class="code docutils literal notranslate"><span class="pre">stratify_cols</span></code> stratifies based on specific columns of <span class="math notranslate nohighlight">\(X\)</span>. However, in cross-validation, the primary focus is on the target variable <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>Attempting to stratify based on <span class="math notranslate nohighlight">\(X\)</span> columns during cross-validation can disrupt the process of ensuring a representative sample of <span class="math notranslate nohighlight">\(y\)</span> in each fold. This can lead to unreliable performance estimates and, in some cases, errors.</p>
<p>Therefore, the use of <code class="code docutils literal notranslate"><span class="pre">stratify_y</span></code> is recommended during cross-validation to maintain consistency in the target variable distribution across folds, while <code class="code docutils literal notranslate"><span class="pre">stratify_cols</span></code> should be avoided.</p>
</section>
</section>
<section id="model-calibration">
<span id="id1"></span><h1>Model Calibration<a class="headerlink" href="#model-calibration" title="Link to this heading"></a></h1>
<p>Model calibration refers to the process of adjusting the predicted probabilities of a model so that they more accurately reflect the true likelihood of outcomes. This is crucial in machine learning, particularly for classification problems where the model outputs probabilities rather than just class labels.</p>
<section id="goal-of-calibration">
<h2>Goal of Calibration<a class="headerlink" href="#goal-of-calibration" title="Link to this heading"></a></h2>
<p>The goal of calibration is to ensure that the predicted probability <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> is equal to the true probability that <span class="math notranslate nohighlight">\(y = 1\)</span> given <span class="math notranslate nohighlight">\(x\)</span>. Mathematically, this can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\hat{p}(x) = P(y = 1 \mid \hat{p}(x) = p)\]</div>
<p>This equation states that for all instances where the model predicts a probability <span class="math notranslate nohighlight">\(p\)</span>, the true fraction of positive cases should also be <span class="math notranslate nohighlight">\(p\)</span>.</p>
</section>
<section id="calibration-curve">
<h2>Calibration Curve<a class="headerlink" href="#calibration-curve" title="Link to this heading"></a></h2>
<p>To assess calibration, we often use a <em>calibration curve</em>. This involves:</p>
<ol class="arabic simple">
<li><p><strong>Binning</strong> the predicted probabilities <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> into intervals (e.g., [0.0, 0.1), [0.1, 0.2), …, [0.9, 1.0]).</p></li>
<li><p><strong>Calculating the mean predicted probability</strong> <span class="math notranslate nohighlight">\(\hat{p}_i\)</span> for each bin <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><strong>Calculating the empirical frequency</strong> <span class="math notranslate nohighlight">\(f_i\)</span> (the fraction of positives) in each bin.</p></li>
</ol>
<p>For a perfectly calibrated model:</p>
<div class="math notranslate nohighlight">
\[\hat{p}_i = f_i \quad \text{for all bins } i\]</div>
</section>
<section id="brier-score">
<h2>Brier Score<a class="headerlink" href="#brier-score" title="Link to this heading"></a></h2>
<p>The <strong>Brier score</strong> is one way to measure the calibration of a model. It’s calculated as:</p>
<div class="math notranslate nohighlight">
\[\text{Brier Score} = \frac{1}{N} \sum_{i=1}^{N} (\hat{p}(x_i) - y_i)^2\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of instances.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{p}(x_i)\)</span> is the predicted probability for instance <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> is the actual label for instance <span class="math notranslate nohighlight">\(i\)</span> (0 or 1).</p></li>
</ul>
<p>The Brier score penalizes predictions that are far from the true outcome. A lower Brier score indicates better calibration and accuracy.</p>
</section>
<section id="platt-scaling">
<h2>Platt Scaling<a class="headerlink" href="#platt-scaling" title="Link to this heading"></a></h2>
<p>One common method to calibrate a model is <strong>Platt Scaling</strong>. This involves fitting a logistic regression model to the predictions of the original model. The logistic regression model adjusts the raw predictions <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> to output calibrated probabilities.</p>
<p>Mathematically, Platt scaling is expressed as:</p>
<div class="math notranslate nohighlight">
\[\hat{p}_{\text{calibrated}}(x) = \frac{1}{1 + \exp(-(A \hat{p}(x) + B))}\]</div>
<p>Where <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are parameters learned from the data. These parameters adjust the original probability estimates to better align with the true probabilities.</p>
</section>
<section id="isotonic-regression">
<h2>Isotonic Regression<a class="headerlink" href="#isotonic-regression" title="Link to this heading"></a></h2>
<p>Another method is <strong>Isotonic Regression</strong>, a non-parametric approach that fits a piecewise constant function. Unlike Platt Scaling, which assumes a logistic function, Isotonic Regression only assumes that the function is monotonically increasing. The goal is to find a set of probabilities <span class="math notranslate nohighlight">\(p_i\)</span> that are as close as possible to the true probabilities while maintaining a monotonic relationship.</p>
<p>The isotonic regression problem can be formulated as:</p>
<div class="math notranslate nohighlight">
\[\min_{p_1 \leq p_2 \leq \dots \leq p_n} \sum_{i=1}^{n} (p_i - y_i)^2\]</div>
<p>Where <span class="math notranslate nohighlight">\(p_i\)</span> are the adjusted probabilities, and the constraint ensures that the probabilities are non-decreasing.</p>
</section>
<section id="example-calibration-in-logistic-regression">
<h2>Example: Calibration in Logistic Regression<a class="headerlink" href="#example-calibration-in-logistic-regression" title="Link to this heading"></a></h2>
<p>In a standard logistic regression model, the predicted probability is given by:</p>
<div class="math notranslate nohighlight">
\[\hat{p}(x) = \sigma(w^\top x) = \frac{1}{1 + \exp(-w^\top x)}\]</div>
<p>Where <span class="math notranslate nohighlight">\(w\)</span> is the vector of weights, and <span class="math notranslate nohighlight">\(x\)</span> is the input feature vector.</p>
<p>If this model is well-calibrated, <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> should closely match the true conditional probability <span class="math notranslate nohighlight">\(P(y = 1 \mid x)\)</span>. If not, techniques like Platt Scaling or Isotonic Regression can be applied to adjust <span class="math notranslate nohighlight">\(\hat{p}(x)\)</span> to be more accurate.</p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Model calibration</strong> is about aligning predicted probabilities with actual outcomes.</p></li>
<li><p><strong>Mathematically</strong>, calibration ensures <span class="math notranslate nohighlight">\(\hat{p}(x) = P(y = 1 \mid \hat{p}(x) = p)\)</span>.</p></li>
<li><p><strong>Platt Scaling</strong> and <strong>Isotonic Regression</strong> are two common methods to achieve calibration.</p></li>
<li><p><strong>Brier Score</strong> is a metric that captures both the calibration and accuracy of probabilistic predictions.</p></li>
</ul>
<p>Calibration is essential when the probabilities output by a model need to be trusted, such as in risk assessment, medical diagnosis, and other critical applications.</p>
</section>
</section>
<section id="using-imputation-and-scaling-in-pipeline-steps-for-model-preprocessing">
<h1>Using Imputation and Scaling in Pipeline Steps for Model Preprocessing<a class="headerlink" href="#using-imputation-and-scaling-in-pipeline-steps-for-model-preprocessing" title="Link to this heading"></a></h1>
<p>The <code class="docutils literal notranslate"><span class="pre">pipeline_steps</span></code> parameter accepts a list of tuples, where each tuple specifies
a transformation step to be applied to the data. For example, the code block below
performs imputation followed by standardization on the dataset before training the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pipeline_steps</span><span class="o">=</span><span class="p">[</span>
   <span class="p">(</span><span class="s2">&quot;Imputer&quot;</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">()),</span>
   <span class="p">(</span><span class="s2">&quot;StandardScaler&quot;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
<span class="p">]</span>
</pre></div>
</div>
<section id="when-is-imputation-and-feature-scaling-in-pipeline-steps-beneficial">
<h2>When Is Imputation and Feature Scaling in <code class="docutils literal notranslate"><span class="pre">pipeline_steps</span></code> Beneficial?<a class="headerlink" href="#when-is-imputation-and-feature-scaling-in-pipeline-steps-beneficial" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Logistic Regression:</strong> Highly sensitive to feature scaling and missing data. Preprocessing steps like imputation and standardization improve model performance significantly.</p></li>
<li><p><strong>Linear Models (e.g., Ridge, Lasso):</strong> Similar to Logistic Regression, these models require feature scaling for optimal performance.</p></li>
<li><p><strong>SVMs:</strong> Sensitive to the scale of the features, requiring preprocessing like standardization.</p></li>
</ul>
</section>
<section id="models-not-benefiting-from-imputation-and-scaling-in-pipeline-steps">
<h2>Models Not Benefiting From Imputation and Scaling in <code class="docutils literal notranslate"><span class="pre">pipeline_steps</span></code><a class="headerlink" href="#models-not-benefiting-from-imputation-and-scaling-in-pipeline-steps" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Tree-Based Models (e.g., XGBoost, Random Forests, Decision Trees):</strong> These models are invariant to feature scaling and can handle missing values natively. Passing preprocessing steps like StandardScaler or Imputer may be redundant or even unnecessary.</p></li>
</ul>
</section>
<section id="why-doesn-t-xgboost-require-imputation-and-scaling-in-pipeline-steps">
<h2>Why Doesn’t XGBoost Require Imputation and Scaling in <code class="docutils literal notranslate"><span class="pre">pipeline_steps</span></code>?<a class="headerlink" href="#why-doesn-t-xgboost-require-imputation-and-scaling-in-pipeline-steps" title="Link to this heading"></a></h2>
<p>XGBoost and similar tree-based models work on feature splits rather than feature values directly. This makes them robust to unscaled data and capable of handling missing values using default mechanisms like missing parameter handling in XGBoost. Thus, adding steps like scaling or imputation often does not improve and might complicate the training process.</p>
<p>To this end, it is best to use <code class="docutils literal notranslate"><span class="pre">pipeline_steps</span></code> strategically for algorithms that rely on numerical properties (e.g., Logistic Regression). For XGBoost, focus on other optimization techniques like hyperparameter tuning and feature engineering instead.</p>
</section>
</section>
<section id="caveats-in-imbalanced-learning">
<h1>Caveats in Imbalanced Learning<a class="headerlink" href="#caveats-in-imbalanced-learning" title="Link to this heading"></a></h1>
<p>Working with imbalanced datasets introduces several challenges that must be carefully addressed
to ensure model performance is both effective and fair. Below are key caveats to consider:</p>
<section id="bias-from-class-distribution">
<h2>Bias from Class Distribution<a class="headerlink" href="#bias-from-class-distribution" title="Link to this heading"></a></h2>
<p>In imbalanced datasets, the prior probabilities of the classes are highly skewed:</p>
<div class="math notranslate nohighlight">
\[P(Y = c_{\text{minority}}) \ll P(Y = c_{\text{majority}})\]</div>
<p>This imbalance can lead models to prioritize the majority class, resulting in biased predictions
that overlook the minority class. Models may optimize for accuracy but fail to capture the true
distribution of minority class instances.</p>
</section>
<section id="threshold-dependent-predictions">
<h2>Threshold-Dependent Predictions<a class="headerlink" href="#threshold-dependent-predictions" title="Link to this heading"></a></h2>
<p>Many classifiers rely on a decision threshold <span class="math notranslate nohighlight">\(\tau\)</span> to make predictions:</p>
<div class="math notranslate nohighlight">
\[\text{Predict } c_{\text{minority}} \text{ if } \hat{P}(Y = c_{\text{minority}} \mid X) \geq \tau\]</div>
<p>With imbalanced data, the default threshold may favor the majority class, causing a high rate of
false negatives for the minority class. Adjusting the threshold to account for imbalance can
help mitigate this issue, but it requires careful tuning and validation.</p>
</section>
<section id="limitations-of-accuracy">
<span id="id2"></span><h2>Limitations of Accuracy<a class="headerlink" href="#limitations-of-accuracy" title="Link to this heading"></a></h2>
<p>Traditional accuracy is a misleading metric in imbalanced datasets. For example, a model predicting
only the majority class can achieve high accuracy despite failing to identify any minority class instances.
Instead, alternative metrics should be used:</p>
<ul id="precision">
<li><p><strong>Precision</strong> for the minority class:</p>
<blockquote>
<div><p>Measures the proportion of correctly predicted minority class instances out of all
instances predicted as the minority class.</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}\]</div>
</li>
</ul>
<ul id="recall">
<li><p><strong>Recall</strong> for the minority class:</p>
<blockquote>
<div><p>Measures the proportion of correctly predicted minority class instances out of all actual
minority class instances.</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}\]</div>
</li>
</ul>
<ul id="f1-score">
<li><p><strong>F1-Score</strong>, the harmonic mean of precision and recall:</p>
<blockquote>
<div><p>Balances precision and recall to provide a single performance measure.</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}\]</div>
</li>
<li><p><strong>ROC AUC (Receiver Operating Characteristic - Area Under the Curve)</strong>:</p>
<blockquote>
<div><p>Measures the model’s ability to distinguish between classes. It is the area under the
ROC curve, which plots the True Positive Rate (Recall) against the False Positive Rate.</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\text{True Positive Rate (TPR)} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}\]</div>
<div class="math notranslate nohighlight">
\[\text{False Positive Rate (FPR)} = \frac{\text{False Positives}}{\text{False Positives} + \text{True Negatives}}\]</div>
</li>
</ul>
<p></p>
<blockquote>
<div><p>The AUC (Area Under Curve) is computed by integrating the ROC curve:</p>
<div class="math notranslate nohighlight">
\[\text{AUC} = \int_{0}^{1} \text{TPR}(\text{FPR}) \, d(\text{FPR})\]</div>
<p>This integral represents the total area under the ROC curve, where:</p>
<ul>
<li><p>A value of 0.5 indicates random guessing.</p></li>
<li><p>A value of 1.0 indicates a perfect classifier.</p>
<blockquote>
<div><p>Practically, the AUC is estimated using numerical integration techniques such as the trapezoidal rule
over the discrete points of the ROC curve.</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<section id="integration-and-practical-considerations">
<h3>Integration and Practical Considerations<a class="headerlink" href="#integration-and-practical-considerations" title="Link to this heading"></a></h3>
<p>The ROC AUC provides an aggregate measure of model performance across all classification thresholds.</p>
<p>However:</p>
<ul class="simple">
<li><p><strong>Imbalanced Datasets</strong>: The ROC AUC may still appear high if the classifier performs well on the majority class, even if the minority class is poorly predicted.
In such cases, metrics like Precision-Recall AUC are more informative.</p></li>
<li><p><strong>Numerical Estimation</strong>: Most implementations (e.g., in scikit-learn) compute the AUC numerically, ensuring fast and accurate computation.</p></li>
</ul>
<p>These metrics provide a more balanced evaluation of model performance on imbalanced datasets. By using metrics like ROC AUC in conjunction with precision, recall, and F1-score, practitioners
can better assess a model’s effectiveness in handling imbalanced data.</p>
</section>
</section>
<section id="impact-of-resampling-techniques">
<h2>Impact of Resampling Techniques<a class="headerlink" href="#impact-of-resampling-techniques" title="Link to this heading"></a></h2>
<p>Resampling methods such as oversampling and undersampling can address class imbalance but come with trade-offs:</p>
<p><strong>Oversampling Caveats</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>Methods like SMOTE may introduce synthetic data that does not fully reflect the true distribution of the minority class.</p></li>
<li><p>Overfitting to the minority class is a risk if too much synthetic data is added.</p></li>
</ul>
</div></blockquote>
<p><strong>Undersampling Caveats</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>Removing samples from the majority class can lead to loss of important information, reducing the model’s generalizability.</p></li>
</ul>
</div></blockquote>
<section id="smote-a-mathematical-illustration">
<h3>SMOTE: A Mathematical Illustration<a class="headerlink" href="#smote-a-mathematical-illustration" title="Link to this heading"></a></h3>
<p>SMOTE (Synthetic Minority Over-sampling Technique) is a widely used algorithm for addressing
class imbalance by generating synthetic samples for the minority class. However, while powerful,
SMOTE comes with inherent caveats that practitioners should understand. Below is a mathematical
illustration highlighting these caveats.</p>
<p><strong>Synthetic Sample Generation</strong></p>
<p>SMOTE generates synthetic samples by interpolating between a minority class sample and its nearest
neighbors. Mathematically, a synthetic sample <span class="math notranslate nohighlight">\(x_{synthetic}\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}_{\text{synthetic}} = \mathbf{x}_i + \delta \cdot (\mathbf{x}_k - \mathbf{x}_i)\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span>: A minority class sample.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}_k\)</span>: One of its <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors (from the same class).</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta\)</span>: A random value drawn from a uniform distribution, <span class="math notranslate nohighlight">\(\delta \sim U(0, 1)\)</span>.</p></li>
</ul>
<p>This process ensures that synthetic samples are generated along the line segments connecting
minority class samples and their neighbors.</p>
<p><strong>Caveats in Application</strong></p>
<ol class="arabic simple">
<li><p><strong>Overlapping Classes</strong>:</p>
<ul class="simple">
<li><p>SMOTE assumes that the minority class samples are well-clustered and separable from the majority class.</p></li>
<li><p>If the minority class overlaps significantly with the majority class, synthetic samples may fall into regions dominated by the majority class, leading to misclassification.</p></li>
</ul>
</li>
<li><p><strong>Noise Sensitivity</strong>:</p>
<ul class="simple">
<li><p>SMOTE generates synthetic samples based on existing minority class samples, including noisy or mislabeled ones.</p></li>
<li><p>Synthetic samples created from noisy data can amplify the noise, degrading model performance.</p></li>
</ul>
</li>
<li><p><strong>Feature Space Assumptions</strong>:</p>
<ul class="simple">
<li><p>SMOTE relies on linear interpolation in the feature space, which assumes that the feature space is homogeneous.</p></li>
<li><p>In highly non-linear spaces, this assumption may not hold, leading to unrealistic synthetic samples.</p></li>
</ul>
</li>
<li><p><strong>Dimensionality Challenges</strong>:</p>
<ul class="simple">
<li><p>In high-dimensional spaces, nearest neighbor calculations may become less meaningful due to the curse of dimensionality.</p></li>
<li><p>Synthetic samples may not adequately represent the true distribution of the minority class.</p></li>
</ul>
</li>
<li><p><strong>Risk of Overfitting</strong>:</p>
<ul class="simple">
<li><p>If SMOTE is applied excessively, the model may overfit to the synthetic minority class samples, reducing generalizability to unseen data.</p></li>
</ul>
</li>
</ol>
</section>
<section id="example-of-synthetic-sample-creation">
<h3>Example of Synthetic Sample Creation<a class="headerlink" href="#example-of-synthetic-sample-creation" title="Link to this heading"></a></h3>
<p>To illustrate, consider a minority class sample <span class="math notranslate nohighlight">\(f{x}_i = [1, 2]\)</span> and its nearest neighbor
<span class="math notranslate nohighlight">\(f{x}_k = [3, 4]\)</span>. If <span class="math notranslate nohighlight">\(\delta = 0.5\)</span>, the synthetic sample is computed as:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}_{\text{synthetic}} = [1, 2] + 0.5 \cdot ([3, 4] - [1, 2])\]</div>
<div class="math notranslate nohighlight">
\[\mathbf{x}_{\text{synthetic}} = [2, 3]\]</div>
<p>This synthetic sample lies midway between the two points in the feature space.</p>
</section>
<section id="mitigating-the-caveats">
<h3>Mitigating the Caveats<a class="headerlink" href="#mitigating-the-caveats" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Combine SMOTE with Undersampling</strong>: Techniques like <code class="docutils literal notranslate"><span class="pre">SMOTEENN</span></code> or <code class="docutils literal notranslate"><span class="pre">SMOTETomek</span></code> remove noisy or overlapping samples after synthetic generation.</p></li>
<li><p><strong>Apply with Feature Engineering</strong>: Ensure the feature space is meaningful and represents the underlying data structure.</p></li>
<li><p><strong>Tune Oversampling Ratio</strong>: Avoid generating excessive synthetic samples to reduce overfitting.</p></li>
</ul>
</section>
</section>
</section>
<section id="threshold-tuning-considerations">
<span id="id3"></span><h1>Threshold Tuning Considerations<a class="headerlink" href="#threshold-tuning-considerations" title="Link to this heading"></a></h1>
<p><strong>Mathematical Basis</strong>:</p>
<p>In binary classification, the decision rule is represented as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{y} =
\begin{cases}
   1 &amp; \text{if } P(\text{positive class} \mid X) &gt; \tau \\
   0 &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(\text{positive class} \mid X)\)</span> is the predicted probability of the positive class given features <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\tau\)</span> is the threshold value, which determines the decision boundary.</p></li>
</ul>
<p>By default, <span class="math notranslate nohighlight">\(\tau = 0.5\)</span>, but this may not always align with the desired balance between precision and recall.</p>
<section id="the-precision-recall-tradeoff">
<h2>The Precision-Recall Tradeoff<a class="headerlink" href="#the-precision-recall-tradeoff" title="Link to this heading"></a></h2>
<p>When tuning the threshold <span class="math notranslate nohighlight">\(\tau\)</span>, it is important to recognize its impact on precision and recall:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#precision"><span class="std std-ref">Precision</span></a> increases as <span class="math notranslate nohighlight">\(\tau\)</span> increases, since the model becomes more conservative in predicting the positive class, reducing false positives.</p></li>
<li><p><a class="reference internal" href="#recall"><span class="std std-ref">Recall</span></a> decreases as <span class="math notranslate nohighlight">\(\tau\)</span> increases, as the model’s stricter criteria result in more false negatives.</p></li>
</ul>
<p>This tradeoff is especially critical in domains where false positives or false negatives have significantly different costs, such as:</p>
<ul class="simple">
<li><p>Medical diagnostics: Emphasize recall to minimize false negatives.</p></li>
<li><p>Spam detection: Emphasize precision to reduce false positives.</p></li>
</ul>
</section>
<section id="threshold-optimization">
<h2>Threshold Optimization<a class="headerlink" href="#threshold-optimization" title="Link to this heading"></a></h2>
<p>Adjusting the threshold based solely on a single metric (e.g., maximizing precision) may lead to suboptimal performance in other metrics. For example:</p>
<ul class="simple">
<li><p>Increasing <span class="math notranslate nohighlight">\(\tau\)</span> to improve precision might drastically reduce recall.</p></li>
<li><p>Decreasing <span class="math notranslate nohighlight">\(\tau\)</span> to maximize recall might result in an unacceptably high false positive rate.</p></li>
</ul>
<p>A balanced metric like the F-beta score can address this tradeoff:</p>
<div class="math notranslate nohighlight">
\[F_\beta = (1 + \beta^2) \cdot \frac{\text{Precision} \cdot \text{Recall}}{(\beta^2 \cdot \text{Precision}) + \text{Recall}}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\beta\)</span> adjusts the weight given to recall relative to precision:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta &gt; 1\)</span>: Recall is prioritized.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta &lt; 1\)</span>: Precision is prioritized.</p></li>
</ul>
</section>
</section>
<section id="elasticnet-regularization">
<span id="elastic-net"></span><h1>ElasticNet Regularization<a class="headerlink" href="#elasticnet-regularization" title="Link to this heading"></a></h1>
<p>Elastic net minimizes the following cost function:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\beta) = \frac{1}{2n} \sum_{i=1}^{n} \left( y_i - \mathbf{x}_i^\top \beta \right)^2 + \lambda \left( \alpha \|\beta\|_1 + \frac{1 - \alpha}{2} \|\beta\|_2^2 \right)\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\|\beta\|_1 = \sum_{j=1}^p |\beta_j|\)</span> represents the <span class="math notranslate nohighlight">\(L1\)</span> norm, promoting sparsity.</p></li>
<li><p><span class="math notranslate nohighlight">\(\|\beta\|_2^2 = \sum_{j=1}^p \beta_j^2\)</span> represents the <span class="math notranslate nohighlight">\(L2\)</span> norm, promoting shrinkage.</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> controls the regularization strength.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha \in [0, 1]\)</span> determines the balance between the <span class="math notranslate nohighlight">\(L1\)</span> and <span class="math notranslate nohighlight">\(L2\)</span> penalties.</p></li>
</ul>
<section id="important-considerations">
<h2>Important Considerations<a class="headerlink" href="#important-considerations" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Balance of Sparsity and Shrinkage</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha = 1\)</span>: Reduces to Lasso (<span class="math notranslate nohighlight">\(L1\)</span> only).</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha = 0\)</span>: Reduces to Ridge (<span class="math notranslate nohighlight">\(L2\)</span> only).</p></li>
<li><p>Intermediate values allow elastic net to select features while managing multicollinearity.</p></li>
</ul>
</li>
<li><p><strong>Regularization Strength</strong>:</p>
<ul class="simple">
<li><p>Larger <span class="math notranslate nohighlight">\(\lambda\)</span> increases bias but reduces variance, favoring simpler models.</p></li>
<li><p>Smaller <span class="math notranslate nohighlight">\(\lambda\)</span> reduces bias but may increase variance, allowing more complex models.</p></li>
</ul>
</li>
<li><p><strong>Feature Correlation</strong>:</p>
<ul class="simple">
<li><p>Elastic net handles correlated features better than Lasso, spreading coefficients across groups of related predictors.</p></li>
</ul>
</li>
<li><p><strong>Hyperparameter Tuning</strong>:</p>
<ul class="simple">
<li><p>Both <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span> should be optimized via cross-validation to achieve the best performance.</p></li>
</ul>
</li>
</ol>
<p>Elastic net is well-suited for datasets with mixed feature relevance, reducing overfitting while retaining important predictors.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<ul class="simple">
<li><p>When combining elastic net with RFE, it is important to note that the recursive process may interact with the regularization in elastic net.</p></li>
<li><p>Elastic net’s built-in feature selection can prioritize sparsity, but RFE explicitly removes features step-by-step. This may lead to redundancy in feature selection efforts or alter the balance between <span class="math notranslate nohighlight">\(L1\)</span> and <span class="math notranslate nohighlight">\(L2\)</span> penalties as features are eliminated.</p></li>
<li><p>Careful calibration of <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span> is essential when using RFE alongside elastic net to prevent over-penalization or premature exclusion of relevant features.</p></li>
</ul>
</div>
</section>
</section>
<section id="catboost-training-parameters">
<span id="id4"></span><h1>CatBoost Training Parameters<a class="headerlink" href="#catboost-training-parameters" title="Link to this heading"></a></h1>
<p>According to the <a class="reference external" href="https://catboost.ai/docs/en/references/training-parameters/">CatBoost documentation</a>:</p>
<blockquote>
<div><p>“For the Python package several parameters have aliases. For example, the –iterations parameter has the following synonyms: num_boost_round, n_estimators, num_trees. Simultaneous usage of different names of one parameter raises an error.”</p>
</div></blockquote>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Attempting to pass more than one of these synonymous hyperparameters will result in the following error:</p>
<p><code class="docutils literal notranslate"><span class="pre">CatBoostError:</span> <span class="pre">only</span> <span class="pre">one</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">parameters</span> <span class="pre">iterations,</span> <span class="pre">n_estimators,</span> <span class="pre">num_boost_round,</span> <span class="pre">num_trees</span> <span class="pre">should</span> <span class="pre">be</span> <span class="pre">initialized.</span></code></p>
<p>To prevent this issue, ensure you define only <strong>one</strong> of these parameters (e.g., <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>) in your configuration, and avoid including any other aliases such as <code class="docutils literal notranslate"><span class="pre">iterations</span></code> or <code class="docutils literal notranslate"><span class="pre">num_boost_round</span></code>.</p>
</div>
<section id="example-tuning-hyperparameters-for-catboost">
<h2>Example: Tuning hyperparameters for CatBoost<a class="headerlink" href="#example-tuning-hyperparameters-for-catboost" title="Link to this heading"></a></h2>
<p>When defining hyperparameters for grid search, specify only one alias in your configuration. Below is an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cat</span> <span class="o">=</span> <span class="n">CatBoostClassifier</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="c1">## num estimator/iteration/boostrounds</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">depth</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">loss_function</span><span class="o">=</span><span class="s2">&quot;Logloss&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">tuned_hyperparameters_cat</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cat_name</span><span class="si">}</span><span class="s2">__n_estimators&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1500</span><span class="p">],</span>

    <span class="c1">## Additional hyperparameters</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cat_name</span><span class="si">}</span><span class="s2">__learning_rate&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cat_name</span><span class="si">}</span><span class="s2">__depth&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cat_name</span><span class="si">}</span><span class="s2">__loss_function&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Logloss&quot;</span><span class="p">],</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This ensures compatibility with CatBoost’s requirements and avoids errors during hyperparameter tuning.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="usage_guide.html" class="btn btn-neutral float-left" title="iPython Notebooks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="about.html" class="btn btn-neutral float-right" title="GitHub Repository" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, UCLA CTSI ML Team: Leonid Shpaner, Arthur Funnell, Panayiotis Petousis.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>